[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Computer Simulation in R (v1.0.2)",
    "section": "",
    "text": "Welcome\nThis book introduces computer simulation as a core methodological tool in modern statistics for analysing complex systems and modelling uncertainty when analytical solutions are unavailable or impractical. Students will develop an understanding of the capabilities and limitations of simulation, and how simulation complements theoretical and data-driven statistical approaches.\nThe book adopts a computational and algorithmic approach to simulation. Students will learn to design, implement, and verify simulation procedures in an appropriate computing environment, with emphasis on Monte Carlo methods, simulation of univariate and multivariate random variables, dependence structures, and simulation-based statistical inference. Simulation is treated as a structured experimental process, from problem formulation and model design to implementation and evaluation.\nStudents will develop skills in the analysis, visualisation, and interpretation of simulation output, including assessing variability, uncertainty, and convergence.\nThe book is hands-on, with computational activities using R and/or Python. Students will complete a substantial project in which they apply simulation techniques to a discipline-specific problem, demonstrating the ability to develop, analyse, and report a simulation study in a clear, rigorous, and professional manner.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#updates",
    "href": "index.html#updates",
    "title": "Computer Simulation in R (v1.0.2)",
    "section": "Updates",
    "text": "Updates\nAs of 27/02/2026, the ebook includes the following content:\n\nWeek 1: Introduction to Computer Simulation (Lecture and Workshop solutions)\nWeek 2: Statistical Distribution (Lecture and Workshop (Part 1) solutions)\nWeek 3: Joint Distributions and Transformation of Random Variables (Lecture and Workshop Activities)",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "01_introduction.html",
    "href": "01_introduction.html",
    "title": "Introduction",
    "section": "",
    "text": "Computer simulation has become one of the central methodologies of modern scientific inquiry. Alongside theoretical analysis and physical experimentation, simulation provides a third way of understanding complex systems. Where mathematical analysis seeks exact solutions and experimentation observes real-world behaviour, simulation constructs a computational model of reality and explores its behaviour through repeated virtual experiments.\n\n\nStanisław Ulam \n\nJohn Von Neumann \n\nENIAC (Electronic Numerical Integrator and Computer)\n\n\n\nThe idea of solving problems through simulated repetition dates back to the 1940s, when Stanisław Ulam considered the probability of winning a solitaire game. Rather than attempting to derive a complicated analytical formula, he proposed repeatedly playing the game and estimating the probability empirically. Together with John von Neumann, this reasoning developed into what is now known as the Monte Carlo method: using randomness and repetition to approximate solutions to problems that are otherwise analytically intractable. With the emergence of electronic computing, this approach transformed from a clever thought experiment into a powerful scientific tool.\n\nAt its core, simulation is the use of a computer to imitate the operation of a real-world stochastic system in order to study its behaviour.\n\nThe system of interest may be physical, biological, economic, or entirely abstract. We may simulate the lifetime of a machine component, the arrival of customers in a queue, rainfall patterns over time, the spread of an infectious disease, or the evolution of a financial portfolio. In each case, the goal is not merely to generate numbers, but to gain insight into variability, uncertainty, and long-run behaviour.\n\nA simulation study always rests on three conceptual pillars: a model, randomness, and repetition.\n\nThe model provides a mathematical or logical representation of the system. Randomness is introduced to reflect uncertainty in inputs or mechanisms. Repetition allows patterns to emerge from noise, enabling us to estimate quantities such as probabilities, expectations, and distributions. Without repetition, randomness obscures structure; with repetition, structure becomes visible.\nUnlike analytical methods, simulation rarely produces exact answers. Instead, it produces approximations whose accuracy improves as the number of replications increases. This fundamental idea underlies much of computational statistics and is formalised by results such as the Law of Large Numbers and the Central Limit Theorem, which justify the convergence of simulated averages to theoretical quantities.\n\nSimulation is especially valuable when systems are too complex for closed-form mathematical analysis.\n\nReal-world processes often involve nonlinear interactions, high-dimensional dependence, heterogeneous agents, time-varying parameters, and complicated feedback mechanisms. In such settings, deriving exact formulas may be impossible or impractical. Simulation trades algebraic elegance for flexibility: we sacrifice exactness in exchange for the ability to model reality more faithfully.\nModern computing environments such as R and Python make it possible to design, implement, and analyse simulation studies efficiently. However, effective simulation requires more than programming skill. It requires careful problem formulation, thoughtful modelling assumptions, validation of computational implementations, and rigorous statistical analysis of output. A poorly designed simulation can be as misleading as a flawed mathematical proof.\n\n\nGarbage in\n\n\n→\n\n\nGarbage out\n\n\n\nThis chapter introduces the foundational ideas that underpin all simulation methods studied in this unit. We begin by formalising what is meant by simulation and how simulation studies are structured. We then examine the essential role of random number generation, since all stochastic simulation ultimately depends on our ability to generate high-quality random variables. Finally, we review the probability concepts that provide the theoretical justification for simulation-based inference.\nSimulation is not merely a computational trick. It is a way of thinking about uncertainty, experimentation, and modelling in the presence of randomness. Understanding its scope and its limitations is the first step toward using it responsibly and effectively.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "01_1_simulation_intro.html",
    "href": "01_1_simulation_intro.html",
    "title": "1  Introduction to Simulation",
    "section": "",
    "text": "1.1 What is Simulation?\nSimulation is the systematic use of a computational model to study the behaviour of a system that evolves under uncertainty. More precisely, a simulation replaces analytical derivation or physical experimentation with a carefully constructed algorithm that generates artificial data according to specified probabilistic rules. By observing the behaviour of this artificial system repeatedly, we approximate characteristics of the real system.\nThe defining feature of simulation is that randomness is built into the mechanism of the model. Unlike deterministic computation, where the same inputs always produce the same outputs, a stochastic simulation produces different outcomes on each run (nondeterministic). This variability is not a flaw but a deliberate feature. It mirrors the variability present in real-world systems and allows us to quantify uncertainty in outcomes.\nTo understand what simulation achieves, consider the distinction between a single realisation and a distribution of outcomes. Running a queueing system for one simulated day yields one possible history of arrivals and waiting times. Running the simulation thousands of times produces a distribution of possible waiting times. It is this distribution—not any single run—that provides meaningful statistical insight. Simulation therefore shifts our focus from deterministic prediction to probabilistic understanding.\nThe validity of the conclusions depends entirely on the appropriateness of the modelling assumptions. If the model is unrealistic, the simulation will faithfully reproduce unrealistic behaviour. For this reason, simulation should always be viewed as a structured experiment conducted under explicitly stated assumptions.\nA useful way to conceptualise simulation is as a virtual laboratory. In a physical laboratory, we manipulate inputs and observe outputs. In a simulation, we encode the system’s rules into a computer program, manipulate parameters, and observe the resulting behaviour. This makes simulation especially powerful for “what-if” analysis. We can change arrival rates, service mechanisms, volatility parameters, or structural assumptions and immediately study the consequences, often at negligible cost compared to real-world experimentation.\nFinally, simulation is inherently approximate. Because we rely on finite repetition, our estimates contain sampling variability. However, probability theory provides the theoretical foundation that guarantees improvement with increased replication. As the number of runs grows, simulated averages stabilise and converge toward their true theoretical values. This convergence justifies the use of simulation as a legitimate inferential tool rather than a heuristic shortcut.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Simulation</span>"
    ]
  },
  {
    "objectID": "01_1_simulation_intro.html#what-is-simulation",
    "href": "01_1_simulation_intro.html#what-is-simulation",
    "title": "1  Introduction to Simulation",
    "section": "",
    "text": "It is important to recognise that a simulation does not replicate reality directly; it replicates a model of reality.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Simulation</span>"
    ]
  },
  {
    "objectID": "01_1_simulation_intro.html#simulation-workflow",
    "href": "01_1_simulation_intro.html#simulation-workflow",
    "title": "1  Introduction to Simulation",
    "section": "1.2 Simulation Workflow",
    "text": "1.2 Simulation Workflow\n\n\n\nProblem\n\n\n→\n\n\nModel\n\n\n→\n\n\nSimulation\n\n\n→\n\n\nAnalysis\n\n\n→\n\n\nConclusion\n\n\n\nAlthough simulation problems arise in many different domains, the structure of a well-designed simulation study follows a common logical progression. The process begins with problem formulation. The question of interest must be stated clearly and quantitatively. Vague objectives lead to unfocused models and ambiguous conclusions. For example, asking whether “queues are too long” is imprecise; asking for the expected waiting time and the probability that waiting exceeds ten minutes is specific and measurable.\nOnce the problem is defined, the next step is model construction. This involves identifying the essential components of the system and deciding how to represent them mathematically. In a queueing model, we must specify how customers arrive, how long service takes, how many servers operate, and how customers are prioritised. These assumptions may be based on empirical data, theoretical reasoning, or simplifying approximations. The art of modelling lies in balancing realism with tractability.\nAfter specifying the model, it must be translated into an algorithm. This step converts conceptual assumptions into computational procedures. Random variables must be generated from appropriate distributions, system states must be updated according to explicit rules, and outputs must be recorded systematically. At this stage, verification becomes crucial: we must ensure that the code implements the intended logic correctly.\nThe simulation is then executed repeatedly. Each run generates one realisation of the system’s behaviour. By aggregating results across many independent replications, we estimate performance measures such as means, variances, quantiles, and probabilities of rare events. Statistical analysis of the output is essential; without it, simulation reduces to uncontrolled numerical experimentation.\nFinally, conclusions are drawn and interpreted in the context of the original problem. If results are unsatisfactory or unrealistic, the process returns to earlier stages. Simulation is inherently iterative. Assumptions are revised, parameters are recalibrated, and models are refined. This feedback loop is not a sign of failure but a central feature of responsible modelling.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Simulation</span>"
    ]
  },
  {
    "objectID": "01_1_simulation_intro.html#simulation-vs-analytical-solutions",
    "href": "01_1_simulation_intro.html#simulation-vs-analytical-solutions",
    "title": "1  Introduction to Simulation",
    "section": "1.3 Simulation vs Analytical Solutions",
    "text": "1.3 Simulation vs Analytical Solutions\nSimulation and analytical methods represent two fundamentally different approaches to solving quantitative problems.\n\nAnalytical solutions rely on mathematical derivations that express relationships in closed form.\n\nWhen available, such solutions provide exact expressions that reveal structural properties of the system. They are often elegant, computationally efficient, and theoretically satisfying.\nHowever, analytical solutions typically require strong assumptions. Many real-world systems violate the simplifying conditions necessary for closed-form derivations. Introducing heterogeneous agents, time-varying dynamics, nonlinear interactions, or complex dependence structures often renders analytical treatment infeasible.\n\nSimulation, by contrast, replaces algebraic derivation with computational experimentation.\n\nRather than solving equations, we approximate quantities of interest by averaging outcomes from repeated random experiments. The result is approximate rather than exact, but the method accommodates far greater structural complexity. Simulation therefore trades exactness for flexibility.\nThe choice between analytical and simulation approaches is not a matter of superiority but of suitability. When exact formulas exist and assumptions are reasonable, analytical solutions are often preferable. When systems are too complex for tractable mathematics, simulation becomes indispensable. In practice, the two approaches complement one another: analytical results guide modelling intuition, while simulation explores behaviour beyond analytical reach.\n\n\n\n\n\n\n\nAnalytical Solutions\nSimulation\n\n\n\n\nNature\nNature\n\n\nFormulas/exact, showing how variables relate\nProduces approximate answers\n\n\nStrong assumptions\nHandles complex, realistic systems\n\n\nOften elegant and fast to compute\nRequires computational power\n\n\n\nFlexible: you can model almost anything\n\n\nWhen to Use\nWhen to Use\n\n\nThe system is simple enough to model with equations\nThe system is too complicated for closed-form math\n\n\nThe mathematics is tractable\nYou want to include realistic randomness\n\n\nYou need exact results\nYou want to test “what-if” scenarios\n\n\nYou want to understand the structure of the problem\nYou need distributions, not just averages\n\n\nExample\nExample\n\n\nFor a simple queue with arrival rate \\(\\lambda\\) and service rate \\(\\mu\\), the average number of customers in the system is: \\[L = \\frac{\\lambda}{\\mu - \\lambda}\\]\nIf the café has: • two baristas with different speeds  • customers who sometimes abandon the queue",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Simulation</span>"
    ]
  },
  {
    "objectID": "01_1_simulation_intro.html#random-number-generation",
    "href": "01_1_simulation_intro.html#random-number-generation",
    "title": "1  Introduction to Simulation",
    "section": "1.4 Random Number Generation",
    "text": "1.4 Random Number Generation\nAll stochastic simulation depends on our ability to generate random numbers. Without randomness, simulation reduces to deterministic computation and cannot represent uncertainty. Yet computers are deterministic machines: given the same inputs, they always produce the same outputs. This raises an important question: how can a deterministic machine generate randomness?\nThe answer lies in the concept of pseudorandomness. Computers do not generate truly random numbers in the physical sense. Instead, they generate sequences of numbers using deterministic algorithms designed to mimic the statistical properties of randomness. These algorithms are known as pseudorandom number generators (PRNGs).\nA PRNG begins with an initial value called a seed. It then applies a recursive formula to produce a sequence of numbers that appear random. If the same seed is used, the same sequence will be generated. This reproducibility is not a weakness; it is one of the greatest strengths of computational simulation. It allows experiments to be replicated exactly, facilitating debugging, validation, and scientific transparency.\nTrue randomness, by contrast, arises from inherently unpredictable physical processes such as radioactive decay, atmospheric noise, or quantum phenomena. Such randomness is nondeterministic. While it is possible to use hardware devices to capture physical randomness, simulation typically relies on algorithmic generators because they are fast, reproducible, and sufficiently random for statistical purposes.\nFor a pseudorandom sequence to be useful in simulation, it must satisfy several important properties. The sequence should have a very long period before repeating. Its values should be uniformly distributed over the intended range. There should be no detectable patterns or correlations that could bias simulation results. Poor-quality generators can produce subtle dependencies that distort high-dimensional simulations, particularly in Monte Carlo integration or Bayesian computation.\nModern statistical software such as R and Python uses high-quality PRNGs by default. Nevertheless, understanding their structure is essential because every simulated random variable ultimately depends on these underlying generators.\n\n1.4.1 Linear Congruential Generators (LCGs)\nOne of the earliest and simplest pseudorandom number generators is the Linear Congruential Generator (LCG). Historically important in the development of Monte Carlo methods, the LCG illustrates the basic mechanism of algorithmic randomness.\n\nAn LCG generates a sequence of integers according to the recurrence relation:\n\\[X_{n+1} = (aX_n + c) \\ \\text{mod} \\ m,\\]\nwhere \\(X\\) is the sequence of pseudo-random numbers, \\(m\\) is the modulus, \\(a\\) is the multiplier, \\(c\\) is the increment, and \\(X_0\\) is the seed. Also,\n\nthe modulus \\(m, 0 &lt; m\\)\nthe multiplier \\(a, 0 &lt; a &lt; m\\)\nthe increment \\(c, 0 \\leq c &lt; m\\)\nthe seed or start values \\(X_0, 0 \\leq X_0 &lt; m\\)\n\nThe modulo operation ensures that the sequence remains within the range \\(0 \\leq X_n &lt; m\\).\n\n\nModulo arithmetic\nIn arithmetic, the modulus refers to the operation that returns the remainder after division.\n\\[a \\bmod m = \\text{remainder when } a \\text{ is divided by } m\\]\nExamples:\n\n\\(7 \\bmod 5 = 2\\)\n\\(5 \\bmod 5 = 0\\)\n\\(17 \\bmod 5 = 2\\)\n\n\nTo use the sequence for simulation, the generated integers are typically scaled to the interval (0,1) by dividing by \\(m\\). This produces numbers that approximate draws from a Uniform(0,1) distribution.\nAlthough the formula is simple, the choice of parameters \\(a\\), \\(c\\), and \\(m\\) critically determines the quality of the generator. Poor parameter choices can produce short cycles and visible lattice structures when successive pairs \\((X_n, X_{n+1})\\) are plotted. These patterns indicate correlation and undermine the reliability of simulations.\nExample: Bad LCG with poor parameter choices\n\nlcg &lt;- function(n, seed = 1, a = 5, c = 1, m = 16) {\n  x &lt;- numeric(n)\n  x[1] &lt;- seed\n  for (i in 2:n) {\n    x[i] &lt;- (a * x[i-1] + c) %% m\n  }\n  u &lt;- x / m   # convert to Uniform(0,1)\n  return(u)\n}\nset.seed(123)\nu &lt;- lcg(5000)\n\npar(mfrow=c(1,2), mar = c(4, 4, 2, 1))\nhist(u, breaks = 30, col = \"skyblue\",\n     xlab = \"Value\", main = \"\")\nplot(u[-length(u)], u[-1], pch = 16, cex = 2, \n    xlab = \"u_n\", ylab = \"u_{n+1}\")\n\n\n\n\n\n\n\n\nWhile LCGs are computationally efficient and easy to implement, they suffer from structural weaknesses. In high-dimensional simulations, they may exhibit undesirable correlations. For this reason, LCGs are primarily of pedagogical and historical interest rather than practical use in modern statistical computing.\n\n\n1.4.2 Uniform(0,1) Random Number (Mersenne Twister)\nModern simulation relies on far more sophisticated generators. The most widely used PRNG in contemporary statistical software is the Mersenne Twister, which serves as the default generator in R, Python (NumPy), MATLAB, and many other systems.\nThe Mersenne Twister was designed to have an extremely long period (specifically \\(2^{19937} - 1\\)), excellent equidistribution properties, and strong performance in high-dimensional settings. For most practical simulation tasks, it provides randomness of sufficient quality.\n\nset.seed(123)\nu &lt;- runif(1000, 0, 1)\n\npar(mfrow=c(1,2), mar = c(4, 4, 2, 1))\nplot(u, pch = 16, col = \"darkgreen\", cex=.8,\n     main = \"Uniform(0,1) Random Draws\",\n     xlab = \"Index\", ylab = \"Value\")\nhist(u,\n     breaks = 20,\n     col = \"skyblue\",\n     main = \"Histogram of Uniform(0,1) Samples\",\n     xlab = \"Value\",\n     ylab = \"Frequency\",\n     freq = FALSE,)\nlines(density(u), col = \"blue\", lwd = 2)\n\n\n\n\n\n\n\n\nThe fundamental output of nearly all PRNGs is a sequence of values that approximate independent draws from the Uniform(0,1) distribution. This distribution plays a central role in simulation because it serves as the building block for generating all other random variables.\nIf we can generate \\(U \\sim \\text{Uniform}(0,1)\\), we can transform it to simulate other distributions. For example,\n\nA Bernoulli(\\(p\\)): returning 1 if \\(U &lt; p\\) and 0 otherwise.\nAn Exponential(\\(\\lambda\\)): using the inverse transform \\(X = -\\frac{1}{\\lambda} \\ln(1 - U)\\).\nA Normal(0, 1): using transformations such as the Box–Muller method.\n\nThis principle — generating complex distributions from uniform random numbers — underlies nearly all simulation techniques studied in this unit. Whether we simulate queueing systems, perform Monte Carlo (MC) integration, or implement Markov Chain Monte Carlo (MCMC) algorithms, the process always begins with high-quality Uniform(0,1) draws.\nIn this sense, the uniform random number generator is the engine of stochastic simulation. If that engine is flawed, every subsequent simulation result is compromised. If it is reliable, we can build complex probabilistic models with confidence.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Simulation</span>"
    ]
  },
  {
    "objectID": "01_2_probability_revision.html",
    "href": "01_2_probability_revision.html",
    "title": "2  Revision: Probability Tools for Simulation",
    "section": "",
    "text": "2.1 Random Variables\nSimulation is grounded in probability theory. When we generate artificial data, compute averages, or estimate probabilities through repetition, we are relying on well-established probabilistic principles. A simulation algorithm may be computational, but its justification is mathematical.\nThis section revisits the key ideas required for simulation-based reasoning.\nA random variable is a numerical function defined on the outcome of a random experiment. Formally, it is a mapping from a sample space \\(\\Omega\\) to the real numbers. Conceptually, it allows us to translate qualitative randomness into quantitative structure.\nIf a die is rolled, the outcome is an element of the sample space \\(\\{1,2,3,4,5,6\\}\\). Defining \\(X\\) as “the number shown on the die” converts this outcome into a random variable. Similarly, in a queueing system, we may define random variables representing arrival times, service durations, waiting times, or the number of customers in the system.\nIn simulation, random variables are the fundamental building blocks. Every stochastic model is constructed by specifying how certain random variables behave and how they interact. Once these variables are generated computationally, the system evolves according to deterministic rules applied to those random inputs.\nThis distinction determines how probabilities and expectations are computed, and it shapes the simulation methods used to generate the variables.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Revision: Probability Tools for Simulation</span>"
    ]
  },
  {
    "objectID": "01_2_probability_revision.html#random-variables",
    "href": "01_2_probability_revision.html#random-variables",
    "title": "2  Revision: Probability Tools for Simulation",
    "section": "",
    "text": "Random variables are typically classified as either discrete or continuous.\n\n\n\nA discrete random variable takes countable values, e.g., the number of arrivals in an hour, the number of failures in a batch, or the number of heads in ten coin tosses.\n\nA continuous random variable takes values in an interval of the real line, e.g., waiting times, lifetimes of components, and measurement errors.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Revision: Probability Tools for Simulation</span>"
    ]
  },
  {
    "objectID": "01_2_probability_revision.html#probability-distributions-pmf-pdf-cdf",
    "href": "01_2_probability_revision.html#probability-distributions-pmf-pdf-cdf",
    "title": "2  Revision: Probability Tools for Simulation",
    "section": "2.2 Probability Distributions (PMF / PDF / CDF)",
    "text": "2.2 Probability Distributions (PMF / PDF / CDF)\nThe behaviour of a random variable is described by its probability distribution.\nFor a discrete random variable \\(X\\), the distribution is specified by its probability mass function (PMF),\n\\[p(x) = P(X = x),\\]\nwhich assigns a probability to each possible value. These probabilities must satisfy\n\\[\\sum_x p(x) = 1.\\]\nFor a continuous random variable, the distribution is described by a probability density function (PDF), denoted \\(f(x)\\), satisfying\n\\[\nf(x) \\ge 0\n\\quad \\text{and} \\quad\n\\int_{-\\infty}^{\\infty} f(x)\\,dx = 1.\n\\]\nProbabilities are computed via integration:\n\\[P(a \\le X \\le b) = \\int_a^b f(x)\\,dx.\\]\nBoth discrete and continuous random variables can be described using the cumulative distribution function (CDF),\n\\[F(x) = P(X \\le x).\\]\nFor continuous random variables, the CDF is related to the density by\n\\[F(x) = \\int_{-\\infty}^{x} f(t)\\,dt.\\]\nThe CDF plays a crucial role in simulation. Many simulation techniques, including inverse transform sampling, rely on transforming Uniform(0,1) random variables through the inverse of the CDF to generate draws from more complex distributions.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Revision: Probability Tools for Simulation</span>"
    ]
  },
  {
    "objectID": "01_2_probability_revision.html#expectation-and-variance",
    "href": "01_2_probability_revision.html#expectation-and-variance",
    "title": "2  Revision: Probability Tools for Simulation",
    "section": "2.3 Expectation and Variance",
    "text": "2.3 Expectation and Variance\nOne of the primary goals of simulation is to estimate expectations. The expectation, or mean, represents the long-run average value of a random variable.\nFor a discrete random variable,\n\\[E[X] = \\sum_x x\\,p(x).\\]\nFor a continuous random variable,\n\\[E[X] = \\int x\\,f(x)\\,dx.\\]\nExpectation is linear. For constants a and b,\n\\[E[aX + bY] = aE[X] + bE[Y].\\]\nThis property is especially important in simulation when combining random components of a model.\nThe variance measures the variability of a random variable around its mean:\n\\[\\operatorname{Var}(X) = E[(X - E[X])^2].\\]\nAn equivalent computational formula is\n\\[\\operatorname{Var}(X) = E[X^2] - (E[X])^2.\\]\nIn simulation, variance determines how much Monte Carlo estimates fluctuate across replications and therefore influences how many repetitions are required to achieve stable results.\n\n2.3.1 R Examples\nExample 1: Discrete Case (Binomial)\nLet \\[X \\sim \\text{Binomial}(n=10, p=0.3).\\]\nTheoretical values: \\[E[X] = np = 3, \\qquad\n\\operatorname{Var}(X) = np(1-p) = 2.1.\\]\n\nset.seed(123)\n\n# Simulate\nx_sim &lt;- rbinom(10000, size = 10, prob = 0.3)\n\n# Simulated statistics\ncat(\"Simulated mean:\", mean(x_sim),\n    \", simulated variance:\", var(x_sim))\n\nSimulated mean: 2.9899 , simulated variance: 2.072205\n\n# Theoretical values\ncat(\"Theoretical mean:\", 10 * 0.3,\n    \", theoretical variance:\", 10 * 0.3 * (1 - 0.3))\n\nTheoretical mean: 3 , theoretical variance: 2.1\n\n\nExample 2: Continuous Case (Exponential)\nLet \\[X \\sim \\text{Exp}(\\lambda = 2).\\]\nTheoretical values: \\[E[X] = \\frac{1}{\\lambda} = 0.5,\n\\qquad\n\\operatorname{Var}(X) = \\frac{1}{\\lambda^2} = 0.25.\\]\n\nset.seed(123)\n\n# Simulate\nx_sim &lt;- rexp(10000, rate = 2)\n\n# Simulated statistics\ncat(\"Simulated mean:\", mean(x_sim),\n    \", simulated variance:\", var(x_sim))\n\nSimulated mean: 0.5018906 , simulated variance: 0.2499411\n\n# Theoretical values\ncat(\"Theoretical mean:\", 1/2,\n \", theoretical variance:\", 1 / (2^2))\n\nTheoretical mean: 0.5 , theoretical variance: 0.25",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Revision: Probability Tools for Simulation</span>"
    ]
  },
  {
    "objectID": "01_2_probability_revision.html#law-of-large-numbers",
    "href": "01_2_probability_revision.html#law-of-large-numbers",
    "title": "2  Revision: Probability Tools for Simulation",
    "section": "2.4 Law of Large Numbers",
    "text": "2.4 Law of Large Numbers\nThe theoretical foundation of simulation is the Law of Large Numbers (LLN).\nSuppose \\(X_1, X_2, \\dots, X_n\\) are independent and identically distributed random variables with mean \\(\\mu\\). The sample average\n\\[\\bar{X}_n = \\frac{1}{n} \\sum_{i=1}^n X_i\\]\nconverges to \\(\\mu\\) as \\(n \\to \\infty\\).\nThis result justifies Monte Carlo estimation. When we simulate a system repeatedly and compute the average outcome, we are approximating an expected value. Increasing the number of replications improves the stability of this estimate.\n\n2.4.1 R Examples\nExample 1: Demonstrate the LLN with coin tosses\nLet\n\\[X \\sim \\text{Binomial}(n=1, p=0.5).\\]\n\nset.seed(123)\nx &lt;- rbinom(5000, size = 1, prob = 0.5)   # 1 = Head, 0 = Tail\nrunning_mean &lt;- cumsum(x) / seq_len(5000)\n\nplot(running_mean, type = \"l\",\n     xlab = \"Number of tosses (n)\",\n     ylab = \"Running mean (proportion of heads)\",\n     main = \"LLNs: Proportion of Heads\",\n     )\n\nabline(h = 0.5, lwd = 2)  # true mean\n\n\n\n\n\n\n\n\nExample 2: Demonstrate the LLN with Exp(1)\nLet\n\\[X \\sim \\text{Exp}(1).\\]\n\nset.seed(123)\nx &lt;- rexp(5000, rate = 1)\nrunning_mean &lt;- cumsum(x) / seq_len(5000)\n\nplot(running_mean, type = \"l\",\n     xlab = \"Number of samples (n)\",\n     ylab = \"Running mean\",\n     main = \"LLNs: Exponential(1)\",\n     )\n\nabline(h = 1, lwd = 2)  # true mean",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Revision: Probability Tools for Simulation</span>"
    ]
  },
  {
    "objectID": "01_2_probability_revision.html#central-limit-theorem",
    "href": "01_2_probability_revision.html#central-limit-theorem",
    "title": "2  Revision: Probability Tools for Simulation",
    "section": "2.5 Central Limit Theorem",
    "text": "2.5 Central Limit Theorem\nThe Law of Large Numbers tells us that the sample average eventually stabilises near the true mean. However, it does not describe how the sample mean fluctuates for large but finite sample sizes. The Central Limit Theorem (CLT) fills this gap.\nLet \\(X_1, X_2, \\dots, X_n\\) be independent and identically distributed random variables with mean \\(\\mu\\) and variance \\(\\sigma^2 &lt; \\infty\\). Define the sample mean\n\\[\\bar{X}_n = \\frac{1}{n} \\sum_{i=1}^n X_i.\\]\nThe Central Limit Theorem states that as \\(n \\to \\infty\\),\n\\[\\sqrt{n}(\\bar{X}_n - \\mu)\\]\nconverges in distribution to a Normal distribution with mean \\(0\\) and variance \\(\\sigma^2\\).\nEquivalently, for sufficiently large \\(n\\),\n\\[\\bar{X}_n \\approx \\mathcal{N}\\left(\\mu, \\frac{\\sigma^2}{n}\\right).\\]\nSeveral important observations follow:\n\nThe original distribution of \\(X\\) does not need to be Normal.\nOnly the existence of a finite variance is required.\nThe variability of the sample mean decreases at rate \\(1/n\\).\nThe approximation improves as \\(n\\) increases.\n\nFor simulation, this result is fundamental. When we estimate an expectation using Monte Carlo methods, we compute a sample average. The CLT tells us that the error of this estimate is approximately Normally distributed for large \\(n\\), which allows us to construct confidence intervals and quantify simulation precision.\n\n2.5.1 Demonstration of the CLT in R\nTo see the CLT in action, we simulate from a distribution that is clearly not Normal — the Exponential distribution.\nLet\n\\[X \\sim \\text{Exp}(1).\\]\nThis distribution is strongly right-skewed.\nWe repeatedly compute sample means of size \\(n = 30\\), and examine their distribution.\n\nset.seed(123)\n\n# Parameters\nn &lt;- 30          # sample size\nR &lt;- 10000       # number of repetitions\n\n# Store sample means\nsample_means &lt;- numeric(R)\n\nfor (i in 1:R) {\n  x &lt;- rexp(n, rate = 1)\n  sample_means[i] &lt;- mean(x)\n}\n\n# Histogram of sample means\nhist(sample_means,\n     probability = TRUE,\n     breaks = 40,\n     col = \"lightblue\",\n     main = \"Distribution of Sample Means (n = 30)\",\n     xlab = \"Sample Mean\")\n\n# Overlay Normal approximation\ncurve(dnorm(x, mean = 1, sd = 1/sqrt(n)),\n      col = \"red\",\n      lwd = 2,\n      add = TRUE)\n\n\n\n\n\n\n\n\nThe red curve represents the Normal distribution predicted by the CLT:\n\\[\\bar{X}_n \\approx \\mathcal{N}\\left(1, \\frac{1}{n}\\right).\\]\nEven though the underlying data are skewed, the distribution of the sample mean is approximately symmetric and bell-shaped.\nTo emphasise the transformation:\n\npar(mfrow = c(1,2))\n\n# Original exponential distribution\nhist(rexp(10000, rate = 1),\n     probability = TRUE,\n     breaks = 40,\n     col = \"salmon\",\n     main = \"Exponential(1)\",\n     xlab = \"X\")\n\n# Distribution of sample means\nhist(sample_means,\n     probability = TRUE,\n     breaks = 40,\n     col = \"lightblue\",\n     main = \"Sample Means (n = 30)\",\n     xlab = \"Mean\")\n\n\n\n\n\n\n\n\nThe first plot is heavily skewed, while the second is approximately Normal. This is the Central Limit Theorem at work.\n\n\n2.5.2 Comparison: LLNs vs CLT\n\n\n\n\n\n\n\n\nFeature\nLaw of Large Numbers (LLN)\nCentral Limit Theorem (CLT)\n\n\n\n\nMain Question\nDoes the sample mean converge to the true mean?\nHow does the sample mean fluctuate around the true mean?\n\n\nStatement\n\\(\\bar{X}_n \\to \\mu\\) as \\(n \\to \\infty\\)\n\\(\\sqrt{n}(\\bar{X}_n - \\mu) \\Rightarrow \\mathcal{N}(0, \\sigma^2)\\)\n\n\nFocus\nConvergence\nDistribution of the error\n\n\nType of Result\nDeterministic limit result\nDistributional approximation\n\n\nWhat It Guarantees\nSample average gets closer to \\(\\mu\\)\nSample average is approximately Normal for large \\(n\\)\n\n\nRate Information\nDoes not specify rate\nVariance shrinks at rate \\(\\sigma^2/n\\)\n\n\nRequired Conditions\ni.i.d., finite mean\ni.i.d., finite variance\n\n\nRole in Simulation\nJustifies Monte Carlo estimation\nJustifies confidence intervals & standard errors\n\n\nPractical Meaning\nMore simulations → more stable estimate\nMore simulations → smaller uncertainty",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Revision: Probability Tools for Simulation</span>"
    ]
  },
  {
    "objectID": "01_3_workshop.html",
    "href": "01_3_workshop.html",
    "title": "3  Workshop Activities",
    "section": "",
    "text": "3.1 Objectives\nThis workshop consists of two parts:\nBy the end of this workshop, you should be able to:",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Workshop Activities</span>"
    ]
  },
  {
    "objectID": "01_3_workshop.html#objectives",
    "href": "01_3_workshop.html#objectives",
    "title": "3  Workshop Activities",
    "section": "",
    "text": "Part 1: Introduction to R and development environments\nPart 2: Probability Foundations for Simulation\n\n\n\nNavigate and use R effectively within R Notebook and RStudio;\nImplement basic R functions relevant to simulation tasks (e.g., variables, functions, loops, and random number generation);\nRecall and apply key probability concepts that underpin simulation methods.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Workshop Activities</span>"
    ]
  },
  {
    "objectID": "01_3_workshop.html#part-1-introduction-review-to-r",
    "href": "01_3_workshop.html#part-1-introduction-review-to-r",
    "title": "3  Workshop Activities",
    "section": "3.2 Part 1: Introduction & review to R",
    "text": "3.2 Part 1: Introduction & review to R\nMini tasks\n\nAdd your name + student ID to the YAML at the top of this notebook using author tag.\nTo verify that you have a running notebook, run a code cell:\n\n\nsessionInfo()\n\nR version 4.3.3 (2024-02-29)\nPlatform: x86_64-pc-linux-gnu (64-bit)\nRunning under: Ubuntu 24.04.4 LTS\n\nMatrix products: default\nBLAS:   /usr/lib/x86_64-linux-gnu/blas/libblas.so.3.12.0 \nLAPACK: /usr/lib/x86_64-linux-gnu/lapack/liblapack.so.3.12.0\n\nlocale:\n [1] LC_CTYPE=C.UTF-8       LC_NUMERIC=C           LC_TIME=C.UTF-8       \n [4] LC_COLLATE=C.UTF-8     LC_MONETARY=C.UTF-8    LC_MESSAGES=C.UTF-8   \n [7] LC_PAPER=C.UTF-8       LC_NAME=C              LC_ADDRESS=C          \n[10] LC_TELEPHONE=C         LC_MEASUREMENT=C.UTF-8 LC_IDENTIFICATION=C   \n\ntime zone: Australia/Perth\ntzcode source: system (glibc)\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nloaded via a namespace (and not attached):\n [1] htmlwidgets_1.6.4 compiler_4.3.3    fastmap_1.2.0     cli_3.6.5        \n [5] tools_4.3.3       htmltools_0.5.9   otel_0.2.0        rmarkdown_2.30   \n [9] knitr_1.51        jsonlite_2.0.0    xfun_0.56         digest_0.6.39    \n[13] rlang_1.1.7       evaluate_1.0.5   \n\n\nThis prints R version, platform, attached packages, locale, matrix products / BLAS info.\nRunning Code Chunks in RStudio\nRStudio provides several convenient ways to run code chunks in an R Markdown notebook.\nUsing the Run Button\n\nClick the Run button at the top right of a code chunk to execute that chunk.\n\nThe output appears immediately below the chunk.\n\nUsing Keyboard Shortcuts\n\nCtrl + Enter (Windows/Linux) or Cmd + Enter (macOS):\nRuns the current line or selected code and moves the cursor to the next line.\nCtrl + Shift + Enter (Windows/Linux) or Cmd + Shift + Enter (macOS):\nRuns the entire code chunk.\nAlt + Enter (Windows/Linux) or Option + Enter (macOS):\nInserts a new code chunk.\n\nRunning Multiple Chunks\n\nRun All Chunks Above:\nClick the small dropdown next to the Run button and choose Run All Chunks Above.\nRun All Chunks Below:\nSame dropdown → Run All Chunks Below.\nRun All:\nUse Run All to execute every chunk in the document sequentially.\n\nRunning the Entire Document\n\nClick Knit to run all chunks and render the document into HTML, PDF, or Word (depending on your YAML settings).\n\nInsert a code Chunk\nIf you are using R Notebook/Markdown within the RStudio IDE, you can also use a keyboard shortcut or toolbar button to insert a chunk with the necessary R syntax automatically.\n\nKeyboard Shortcut: Press Ctrl + Alt + I (Windows/Linux) or Cmd + Option + I (macOS).\n\n\n3.2.1 Learning R with the swirl Package\nThe swirl package is an interactive learning tool that teaches R programming and data science inside your RStudio console. It guides you through short lessons, checks your answers, and gives immediate feedback — perfect for beginners who want hands‑on practice. In this workshop, you will install swirl, start a lesson, and complete a few interactive exercises.\nWhy use swirl?\n\nIt teaches R from within R\nLessons are short, structured, and beginner‑friendly\nYou get instant feedback on your answers\nYou can learn at your own pace\nIt reinforces the skills you’ll use in simulation labs\n\ninstall.packages(\"swirl\")\nlibrary(swirl)\nswirl() # begin interactive session\nFor STAT2005, the following swirl lessons are most relevant:\n\nR Programming (especially: Basic Building Blocks, Workspace and Files, Sequences, Vectors, Logic, Functions, lapply/sapply)\n\nData Science: Foundations using R (optional but helpful)",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Workshop Activities</span>"
    ]
  },
  {
    "objectID": "01_3_workshop.html#part-2-probability-foundations-for-simulation",
    "href": "01_3_workshop.html#part-2-probability-foundations-for-simulation",
    "title": "3  Workshop Activities",
    "section": "3.3 Part 2: Probability Foundations for Simulation",
    "text": "3.3 Part 2: Probability Foundations for Simulation\nHere is a collection of summary and reference sheets I prepared for STAT1005:\n\nSummary of Theory\nProbability Distributions Reference Sheet\nHypothesis Testing Reference Sheet\n\n\n3.3.1 2.1 Random Number Generator\nSimulation begins with the ability to generate random‑looking numbers. Computers, however, are deterministic machines—they cannot produce true randomness on their own. Instead, they use pseudorandom number generators (PRNGs): algorithms that produce sequences of numbers that behave like random samples.\nIn this section, we will:\n\nimplement a simple Linear Congruential Generator (LCG)\ngenerate Uniform(0,1) samples\ncompare our LCG to built‑in generator\n\nThis gives us the computational foundation for all later simulation work.\n\n3.3.1.1 2.1.1 LCGs\nFrom the lecture, we learned that an LCG produces a sequence of integers using a recurrence relation: \\[\nX_{n+1} = (aX_n + c) \\mod m,\n\\] where \\(X\\) is the sequence of pseudo-random values and\n\nthe modulus \\(m, 0 &lt; m\\)\nthe multiplier \\(a, 0 &lt; a &lt; m\\)\nthe increment \\(c, 0 \\leq c &lt; m\\)\nthe seed or start values \\(X_0, 0 \\leq X_0 &lt; m\\)\n\nGood choices of \\((a,c,m)\\) give long, well‑distributed sequences. Poor choices give visible patterns—great for teaching, not great for real simulation.\n\n3.3.1.1.1 Exercise 2.1\n2.1.1 Write a function lcg() that produces a sequence of integers using a recurrence relation above. Try different parameter values.\n2.1.2 Write a function lcg_uniform() that returns the LCG integer output scaled to Uniform(0,1).\n2.1.3 Plot a histogram of lcg_uniform(10_000)\n2.1.4 Plot a histogram of the built in Uniform generator runif()\n\n\n\n\n3.3.2 2.2 Probability Distributions (PDF/PMF/CDF)\nDiscrete RVs (e.g., Bernoulli, Binomial) PMF gives the probability of each possible value \\[\nP(X=x)\n\\] Continuous RVs (e.g., Exponential, Normal) - PDF describes the shape of the distribution \\[\nf(x)\n\\] - CDF gives cumulative probability \\[\nF(x)=P(X\\leq x)\n\\] Simulation gives us samples. Theory gives us functions.\n\n3.3.2.1 2.2.1 Example: Exponential(λ)\nWe already simulated Exponential(λ) using inverse transform. Now we compare the simulated histogram to the theoretical PDF: \\[\nf(x)=\\lambda e^{-\\lambda x},\\quad x\\geq 0\n\\]\n\n# Simulate\nlam &lt;- 2\nsamples &lt;- -log(runif(5000)) / lam\n\n# Theoretical PDF\nx &lt;- seq(0, 3, length.out = 300)\npdf &lt;- lam * exp(-lam * x)\n\n# Plot\nhist(samples,\n     breaks = 40,\n     freq = FALSE,\n     col = rgb(0, 0, 1, 0.3),\n     border = \"black\",\n     main = \"Exponential(λ = 2): Simulation vs PDF\",\n     xlab = \"x\")\n\nlines(x, pdf, col = \"red\", lwd = 2)\nlegend(\"topright\",\n       legend = c(\"Simulated\", \"Theoretical PDF\"),\n       col = c(rgb(0, 0, 1, 0.6), \"red\"),\n       lwd = c(10, 2),\n       bty = \"n\")\n\n\n\n\n\n\n\n\n\n3.3.2.1.1 Exercise 2.2\nThe Normal distribution has PDF: \\[\nf(x)=\\frac{1}{\\sqrt{2\\pi }}e^{-x^2/2}\n\\] Use rnorm() to simulate Normal(0, 1) data and dnorm() to compute the theoretical PDF, then create a comparison plot similar to the example above.\n\n\n\n3.3.2.2 2.2.3 Example: Bernoulli(p)\nFor Bernoulli(p), the theoretical PMF: \\[\nP(X=1)=p,\\quad P(X=0)=1-p\n\\]\n\n# Parameter\np &lt;- 0.3\n\n# Simulate Bernoulli samples\nsamples_bern &lt;- as.integer(runif(5000) &lt; p)\n\n# Empirical PMF\ncounts &lt;- table(samples_bern)\nempirical &lt;- counts / sum(counts)\n\n# Theoretical PMF\ntheoretical &lt;- c(\"0\" = 1 - p, \"1\" = p)\n\n# Plot\nbarplot(\n  rbind(empirical, theoretical),\n  beside = TRUE,\n  col = c(rgb(0, 0, 1, 0.6), rgb(1, 0, 0, 0.6)),\n  names.arg = c(\"0\", \"1\"),\n  ylim = c(0, max(empirical, theoretical) * 1.2),\n  main = \"Bernoulli(p = 0.3): Empirical vs Theoretical PMF\",\n  ylab = \"Probability\"\n)\n\nlegend(\n  \"topright\",\n  legend = c(\"Empirical\", \"Theoretical\"),\n  fill = c(rgb(0, 0, 1, 0.6), rgb(1, 0, 0, 0.6)),\n  bty = \"n\"\n)\n\n\n\n\n\n\n\n\n\n\n\n3.3.3 2.3 Law of Large Numbers (LLNs)\nThe Law of Large Numbers (LLN) is one of the most important ideas in probability and simulation. It explains why simulation works and why empirical averages converge to theoretical expectations. In simple terms:\n\nAs we take more and more samples, the sample mean gets closer to the true mean.\n\nThis is why Monte Carlo methods are powerful: averages stabilise. We’ll demonstrate LLN using simulation.\n\n3.3.3.0.1 2.3.1 LLNs with Binomial\nConsider coin tosses example,\n\nset.seed(123)\nn &lt;- 5000\n\nx &lt;- rbinom(5000, size = 1, prob = 0.5)   # 1 = Head, 0 = Tail\nrunning_mean &lt;- cumsum(x) / seq_len(n)\n\nplot(running_mean, type = \"l\", col = \"blue\",\n     xlab = \"Number of tosses (n)\",\n     ylab = \"Running mean (proportion of heads)\",\n     main = \"LLNs: Proportion of Heads\",\n     cex.main = 2.2,\n     cex.lab = 1.8,\n     cex.axis = 1.8,\n     )\n\nabline(h = 0.5, lwd = 2)  # true mean\n\n\n\n\n\n\n\n\n\n\n3.3.3.0.2 2.3.1 LLNs with Exp(1)\n\nset.seed(2026)\nx &lt;- rexp(n, rate = 1)\nrunning_mean &lt;- cumsum(x) / seq_len(n)\n\nplot(running_mean, type = \"l\", col = \"red\",\n     xlab = \"Number of samples (n)\",\n     ylab = \"Running mean\",\n     main = \"LLNs: Exponential(1)\",\n      cex.main = 2.2,\n     cex.lab = 1.8,\n     cex.axis = 1.8,\n     )\n\nabline(h = 1, lwd = 2)  # true mean\n\n\n\n\n\n\n\n\n\n\n3.3.3.0.3 Exercise 2.3\nFor a Uniform(0,1) random variable with \\(\\mathbb{E}[X]=0.5\\), use runif() to simulate 5000 samples and create a plot to demonstrate the LLNs.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Workshop Activities</span>"
    ]
  },
  {
    "objectID": "01_4_workshop_sol.html",
    "href": "01_4_workshop_sol.html",
    "title": "4  Workshop Solutions",
    "section": "",
    "text": "4.1 Part 1: Introduction & review to R\nThis workshop consists of two parts:\nBy the end of this workshop, you should be able to:\nMini tasks\nsessionInfo()\nThis prints R version, platform, attached packages, locale, matrix products / BLAS info.\nRunning Code Chunks in RStudio\nRStudio provides several convenient ways to run code chunks in an R Markdown notebook.\nUsing the Run Button\nUsing Keyboard Shortcuts\nRunning Multiple Chunks\nRunning the Entire Document\nInsert a code Chunk\nIf you are using R Notebook/Markdown within the RStudio IDE, you can also use a keyboard shortcut or toolbar button to insert a chunk with the necessary R syntax automatically.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Workshop Solutions</span>"
    ]
  },
  {
    "objectID": "01_4_workshop_sol.html#part-1-introduction-review-to-r",
    "href": "01_4_workshop_sol.html#part-1-introduction-review-to-r",
    "title": "4  Workshop Solutions",
    "section": "",
    "text": "Add your name + student ID to the YAML at the top of this notebook using author tag.\nTo verify that you have a running notebook, run a code cell:\n\n\n\n\n\n\n\nClick the Run button at the top right of a code chunk to execute that chunk.\n\nThe output appears immediately below the chunk.\n\n\n\nCtrl + Enter (Windows/Linux) or Cmd + Enter (macOS):\nRuns the current line or selected code and moves the cursor to the next line.\nCtrl + Shift + Enter (Windows/Linux) or Cmd + Shift + Enter (macOS):\nRuns the entire code chunk.\nAlt + Enter (Windows/Linux) or Option + Enter (macOS):\nInserts a new code chunk.\n\n\n\nRun All Chunks Above:\nClick the small dropdown next to the Run button and choose Run All Chunks Above.\nRun All Chunks Below:\nSame dropdown → Run All Chunks Below.\nRun All:\nUse Run All to execute every chunk in the document sequentially.\n\n\n\nClick Knit to run all chunks and render the document into HTML, PDF, or Word (depending on your YAML settings).\n\n\n\n\nKeyboard Shortcut: Press Ctrl + Alt + I (Windows/Linux) or Cmd + Option + I (macOS).\n\n\n4.1.1 Learning R with the swirl Package\nThe swirl package is an interactive learning tool that teaches R programming and data science inside your RStudio console. It guides you through short lessons, checks your answers, and gives immediate feedback — perfect for beginners who want hands‑on practice. In this workshop, you will install swirl, start a lesson, and complete a few interactive exercises.\nWhy use swirl?\n\nIt teaches R from within R\nLessons are short, structured, and beginner‑friendly\nYou get instant feedback on your answers\nYou can learn at your own pace\nIt reinforces the skills you’ll use in simulation labs\n\ninstall.packages(\"swirl\")\nlibrary(swirl)\nswirl() # begin interactive session\nFor STAT2005, the following swirl lessons are most relevant:\n\nR Programming (especially: Basic Building Blocks, Workspace and Files, Sequences, Vectors, Logic, Functions, lapply/sapply)\n\nData Science: Foundations using R (optional but helpful)",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Workshop Solutions</span>"
    ]
  },
  {
    "objectID": "01_4_workshop_sol.html#part-2-probability-foundations-for-simulation",
    "href": "01_4_workshop_sol.html#part-2-probability-foundations-for-simulation",
    "title": "4  Workshop Solutions",
    "section": "4.2 Part 2: Probability Foundations for Simulation",
    "text": "4.2 Part 2: Probability Foundations for Simulation\n\n\n4.2.1 Random Number Generator\nSimulation begins with the ability to generate random‑looking numbers. Computers, however, are deterministic machines—they cannot produce true randomness on their own. Instead, they use pseudorandom number generators (PRNGs): algorithms that produce sequences of numbers that behave like random samples.\nIn this section, we will:\n\nimplement a simple Linear Congruential Generator (LCG)\ngenerate Uniform(0,1) samples\ncompare our LCG to built‑in generator\n\nThis gives us the computational foundation for all later simulation work.\n\n4.2.1.1 LCGs\nFrom the lecture, we learned that an LCG produces a sequence of integers using a recurrence relation: \\[\nX_{n+1} = (aX_n + c) \\mod m,\n\\] where \\(X\\) is the sequence of pseudo-random values and\n\nthe modulus \\(m, 0 &lt; m\\)\nthe multiplier \\(a, 0 &lt; a &lt; m\\)\nthe increment \\(c, 0 \\leq c &lt; m\\)\nthe seed or start values \\(X_0, 0 \\leq X_0 &lt; m\\)\n\nGood choices of \\((a,c,m)\\) give long, well‑distributed sequences. Poor choices give visible patterns, which is not great for real simulation.\n\nExercise 2.1.1\n\nWrite a function lcg() that produces a sequence of integers using a recurrence relation above. Try different parameter values.\n\nIn R, you can write a function with or without default arguments. If you don’t give an argument a default value (e.g. seed = 1, a = 5, c = 1, m = 16), you must specify it when you call the function.\n\nlcg &lt;- function(n, seed, a, c, m) { # this function has 5 arguments\n  x &lt;- numeric(n)  # initialise x as numeric vector of size n\n  x[1] &lt;- seed     # the first value is X_0 (seed)\n  for (i in 2:n) { # recurrent relation calculation in a loop\n    x[i] &lt;- (a * x[i - 1] + c) %% m # %%: modulus\n  }\n  return(x)        # don't forget to return a value!\n}\n\n\nThe Numerical Recipes parameters (from the lecture)\n\n\nset.seed(123)      # for reproducibility: only once per notebook\nu &lt;- lcg(n=10000, seed=1, a=1664525, c=1013904223, m=2^32) # simulate\nhead(u, n=10)      # returns the fist parts of a vector \n\n [1]          1 1015568748 1586005467 2165703038 3027450565  217083232\n [7] 1587069247 3327581586 2388811721   70837908\n\n\nLet’s try some other parameters…\nYou can look up some interesting parameters combinations from LCG: parameters in common use. For example,\n\nThe ZX81, ZX Spectrum parameters\n\n\nu &lt;- lcg(n=10000, seed=1, a=75, c=0, m=2^16+1)\nhead(u, n=20)\n\n [1]     1    75  5625 28653 51791 17642 12410 13232  9345 45505  4951 43640\n[13] 61687 38935 36497 50258 33741 40169 63510 44586\n\n\n\nThe Borland C/C++ parameters\n\n\nu &lt;- lcg(n=10000, seed=1, a=22695477, c=0, m=2^31)\nhead(u, n=20)\n\n [1]          1   22695477 2133350137  711188368 1567303888  650040080\n [7] 1836094032  889367184 1344614352  191120912  448487760  647071120\n[13] 1467123408  887703824 1211039824  804483216 1012018640 1914231824\n[19]  533884752  647719824\n\n\nWe need to scale these integer to Uniform(0,1).\n\n\nExercise 2.1.2\n\nWrite a function lcg_uniform() that returns the LCG integer output scaled to Uniform(0,1).\n\n\n# as.uniform = TRUE controls the output of the LCG.\nlcg_uniform &lt;- function(n, seed, a, c, m, as.uniform = TRUE) {\n  x &lt;- numeric(n)\n  x[1] &lt;- seed\n  for (i in 2:n) {\n    x[i] &lt;- (a * x[i - 1] + c) %% m\n  }\n  if (as.uniform) {\n    return(x / m)   # convert to Uniform(0,1)\n  } else {\n    return(x)       # return raw integers\n  }\n}\n\n\nThe Numerical Recipes parameters\n\n\nu_NR &lt;- lcg_uniform(n=10000, seed=1, a=1664525, c=1013904223, m=2^32)\nhead(u_NR, 20)\n\n [1] 2.328306e-10 2.364555e-01 3.692707e-01 5.042420e-01 7.048833e-01\n [6] 5.054363e-02 3.695184e-01 7.747630e-01 5.561886e-01 1.649324e-02\n[11] 6.392460e-01 2.504511e-01 4.223778e-01 5.906902e-01 8.369337e-01\n[16] 2.350759e-01 9.808460e-01 8.608871e-01 3.268755e-01 6.826027e-01\n\n\n\nThe ZX81, ZX Spectrum parameters\n\nZX Spectrum / ZX81 LCG is another classic historical example. It was/is used for applications like simple games, shuffling sprites, basic procedural effects, etc.\n\nu_ZX &lt;- lcg_uniform(n=10000, seed=1, a=75, c=0, m=2^16+1)\nhead(u_ZX, 20)\n\n [1] 1.525856e-05 1.144392e-03 8.582938e-02 4.372034e-01 7.902559e-01\n [6] 2.691914e-01 1.893587e-01 2.019012e-01 1.425912e-01 6.943406e-01\n[11] 7.554511e-02 6.658834e-01 9.412546e-01 5.940919e-01 5.568915e-01\n[16] 7.668645e-01 5.148389e-01 6.129209e-01 9.690709e-01 6.803180e-01\n\n\n\nThe Borland C/C++ parameters\n\nBorland C/C++ is also another classic historical LCG that was good enough for simple simulations, games, non-critical numerical work, etc.\n\nu_BL &lt;- lcg_uniform(n=10000, seed=1, a=22695477, c=0, m=2^31)\nhead(u_BL, n=20) # also needs diagnostic plots to figure out the quality\n\n [1] 4.656613e-10 1.056841e-02 9.934186e-01 3.311729e-01 7.298327e-01\n [6] 3.026985e-01 8.549979e-01 4.141439e-01 6.261349e-01 8.899761e-02\n[11] 2.088434e-01 3.013160e-01 6.831826e-01 4.133693e-01 5.639344e-01\n[16] 3.746167e-01 4.712579e-01 8.913837e-01 2.486095e-01 3.016180e-01\n\n\nWe need diagnostic plots to assess the quality of the parameters used.\n\n\nExercise 2.1.3-2.1.4\n\nPlot a diagnostic plots of lcg_uniform(10_000)\n\n\npar(mfrow = c(1, 2),      # control multi-figure structure (nrows, ncols)\n    mar = c(4, 4, 2, 1),  # bottom, left, top, right plot margins\n    oma = c(0, 0, 3, 0))  # outer margins (whole figure)\nhist(u_NR, breaks=30, col=\"skyblue\", main=\"\", xlab=\"Value\")\n# breaks control the breakpoints between histogram cells\nplot(u_NR[-length(u_NR)], u_NR[-1], pch=16, cex=0.25)\n# use [-length(u_NR)] for location of x to avoid confusion\n# [-1] = all elements except the first one\n# it's a standard trick to shift the vector by 1\nmtext(\"Diagnostic plots for Numerical Recipes LCG parameters\",\n      side = 3, outer = TRUE, cex = 1.25) # big title\n\n\n\n\n\n\n\n\nWith these parameters, the generator has a maximal period of \\(2^{32}\\), meaning it cycles through all possible 32‑bit states before repeating (assuming a non‑degenerate seed). So, the NR parameters still produce lattice structure when plotting successive pairs on higher-dimensional tuples. That’s why the NR LCG performs poorly on several modern statistical test suites compared with newer generators. Modern generators (e.g. Mersenne Twister) are vastly superior.\n\npar(mfrow = c(1, 2),      \n    mar = c(4, 4, 2, 1),\n    oma = c(0, 0, 3, 0))\nhist(u_ZX, breaks=30, col=\"skyblue\", main=\"\", xlab=\"Value\")\nplot(u_ZX[-length(u_ZX)], u_ZX[-1], pch=16, cex=0.25)\nmtext(\"Diagnostic plots for ZX81 / ZX Spectrum parameters\",\n      side = 3, outer = TRUE, cex = 1.25)\n\n\n\n\n\n\n\n\nThe ZX Spectrum LCG parameters were appropriate for early 8‑bit hardware due to their simplicity and speed, but they exhibit severe statistical weaknesses, including strong lattice structure and poor high‑dimensional behaviour. While adequate for simple games and demonstrations, they are unsuitable for modern simulation and are mainly of pedagogical value today.\n\npar(mfrow = c(1, 2),      \n    mar = c(4, 4, 2, 1),\n    oma = c(0, 0, 3, 0))\nhist(u_BL, breaks=30, col=\"skyblue\", main=\"\", xlab=\"Value\")\nplot(u_BL[-length(u_BL)], u_BL[-1], pch=16, cex=0.25)\nmtext(\"Diagnostic plots for Berland C/C++  parameters\",\n      side = 3, outer = TRUE, cex = 1.25)\n\n\n\n\n\n\n\n\nThe Borland C/C++ random number generator uses a mixed LCG with a 32‑bit modulus and full period. While it is fast and simple, it suffers from strong lattice structure and poor low‑order bits, making it unsuitable for modern statistical simulation and Monte Carlo methods.\n\n\n\n\n4.2.2 Probability Distributions (PDF/PMF/CDF)\nDiscrete RVs (e.g., Bernoulli, Binomial): PMF gives the probability of each possible value \\[\nP(X=x)\n\\] Continuous RVs (e.g., Exponential, Normal): PDF describes the shape of the distribution \\[\nf(x)\n\\]\nCDF gives cumulative probability \\[\nF(x)=P(X\\leq x)\n\\] Simulation gives us samples. Theory gives us functions.\n\nExample: Exponential(λ)\nLet’s compare the simulated histogram to the theoretical PDF:\n\\[\nf(x)=\\lambda e^{-\\lambda x},\\quad x\\geq 0\n\\]\n\n# Simulate\nlam &lt;- 2\nsamples &lt;- -log(runif(5000)) / lam    # inverse transform method\n# samples &lt;- rexp(5000, rate=lam)     # built-in rexp()\n\n# Theoretical PDF\nx &lt;- seq(0, 3, length.out = 300)\npdf &lt;- lam * exp(-lam * x)         # f(x)\n\n# Plot\nhist(samples,\n     breaks = 40,\n     freq = FALSE,\n     col = rgb(0, 0, 1, 0.3),\n     border = \"black\",\n     main = \"Exponential(λ = 2): Simulation vs PDF\",\n     xlab = \"x\")\n\nlines(x, pdf, col = \"red\", lwd = 2)\nlegend(\"topright\",\n       legend = c(\"Simulated\", \"Theoretical PDF\"),\n       col = c(rgb(0, 0, 1, 0.6), \"red\"),\n       lwd = c(10, 2),\n       bty = \"n\")\n\n\n\n\n\n\n\n\nThe code uses the inverse transform method to simulate Exponential(λ = 2) random variables, plots their empirical density using a histogram, and overlays the theoretical probability density function for comparison. This highlights the importance of reliable Uniform(0,1) random number generators, such as the Mersenne Twister, which historically enabled simulation from complex distributions before modern built‑in generators were widely available.\nYou can also try using the built-in rexp() function and compare the histograms.\n\nExercise 2.2\n\nThe Normal distribution has PDF: \\[\nf(x)=\\frac{1}{\\sqrt{2\\pi }}e^{-x^2/2}\n\\] Use rnorm() to simulate Normal(0, 1) data and dnorm() to compute the theoretical PDF, then create a comparison plot similar to the example above.\n\n\n# Simulate Normal(0, 1)\nsamples &lt;- rnorm(n = 5000, mean = 0, sd = 1)\n\n# Theoretical PDF\nx &lt;- seq(-4, 4, length.out = 300)\npdf &lt;- dnorm(x, mean = 0, sd = 1)\n\n# Plot\nhist(samples,\n     breaks = 40,\n     freq = FALSE,\n     col = rgb(0, 0, 1, 0.3),\n     border = \"black\",\n     main = \"Normal(0, 1): Simulation vs PDF\",\n     xlab = \"x\")\n\nlines(x, pdf, col = \"red\", lwd = 2)\n\nlegend(\"topright\",\n       legend = c(\"Simulated\", \"Theoretical PDF\"),\n       col = c(rgb(0, 0, 1, 0.6), \"red\"),\n       lwd = c(10, 2),\n       bty = \"n\")\n\n\n\n\n\n\n\n\nWhat this is doing (conceptually)\n\nrnorm(5000) generates i.i.d. samples from N(0,1)\nhist(..., freq = FALSE) scales the histogram to a density, so it’s comparable to a PDF\ndnorm() computes the exact Normal(0,1) density\nThe red curve should closely match the histogram as sample size increases\n\nThe histogram of simulated Normal(0,1) data approximates the theoretical PDF, and the agreement improves with larger sample sizes due to LLN.\n\n\n\nExample: Bernoulli(p)\nFor Bernoulli(p), the theoretical PMF: \\[\nP(X=1)=p,\\quad P(X=0)=1-p\n\\]\n\n# Parameter\np &lt;- 0.3\n\n# Simulate Bernoulli samples\nsamples_bern &lt;- as.integer(runif(5000) &lt; p)\n\n# Empirical PMF\ncounts &lt;- table(samples_bern)\nempirical &lt;- counts / sum(counts)\n\n# Theoretical PMF\ntheoretical &lt;- c(\"0\" = 1 - p, \"1\" = p)\n\n# Plot\nbarplot(\n  rbind(empirical, theoretical),\n  beside = TRUE,\n  col = c(rgb(0, 0, 1, 0.6), rgb(1, 0, 0, 0.6)),\n  names.arg = c(\"0\", \"1\"),\n  ylim = c(0, max(empirical, theoretical) * 1.2),\n  main = \"Bernoulli(p = 0.3): Empirical vs Theoretical PMF\",\n  ylab = \"Probability\"\n)\n\nlegend(\n  \"topright\",\n  legend = c(\"Empirical\", \"Theoretical\"),\n  fill = c(rgb(0, 0, 1, 0.6), rgb(1, 0, 0, 0.6)),\n  bty = \"n\"\n)\n\n\n\n\n\n\n\n\n\n\n\n4.2.3 Law of Large Numbers (LLNs)\nThe Law of Large Numbers (LLN) is one of the most important ideas in probability and simulation. It explains why simulation works and why empirical averages converge to theoretical expectations. In simple terms:\n\nAs we take more and more samples, the sample mean gets closer to the true mean.\n\nThis is why Monte Carlo methods are powerful: averages stabilise. We’ll demonstrate LLN using simulation.\n\nExample: LLNs with Binomial\nConsider coin tosses example,\n\nn &lt;- 5000\nx &lt;- rbinom(n, size = 1, prob = 0.5)   # 1 = Head, 0 = Tail\n\nrunning_mean &lt;- cumsum(x) / seq_len(n)\n# mean = sum of x_i from i-&gt;n divided by n\n# as n increases, we can calculate running means using the function\n# cum_sum(x) : cumulative sums\n# seq_len(n) : generate a sequence from 1,2,...,n\n\nplot(running_mean, type = \"l\", col = \"blue\",\n     xlab = \"Number of tosses (n)\",\n     ylab = \"Running mean (proportion of heads)\",\n     main = \"LLNs: Proportion of Heads\",\n     )\nabline(h = 0.5, lwd = 2)  # true mean\n\n\n\n\n\n\n\n\n\nExample: LLNs with Exp(1)\n\nx &lt;- rexp(n, rate = 1)\nrunning_mean &lt;- cumsum(x) / seq_len(n)\n\nplot(running_mean, type = \"l\", col = \"red\",\n     xlab = \"Number of samples (n)\",\n     ylab = \"Running mean\",\n     main = \"LLNs: Exponential(1)\",\n     )\n\nabline(h = 1, lwd = 2)  # true mean\n\n\n\n\n\n\n\n\n\n\nExercise 2.3\n\nFor a Uniform(0,1) random variable with \\(\\mathbb{E}[X]=0.5\\), use runif() to simulate 5000 samples and create a plot to demonstrate the LLNs.\n\n\n# Simulate Uniform(0,1) data\nsamples &lt;- runif(5000)\n\n# Compute running (cumulative) mean\nrunning_mean &lt;- cumsum(samples) / seq_along(samples)\n\n# Plot the running mean\nplot(running_mean,\n     type = \"l\",\n     lwd = 2,\n     col = \"blue\",\n     ylim = c(0, 1),\n     xlab = \"Sample size n\",\n     ylab = \"Sample mean\",\n     main = \"LLNs: Uniform(0,1)\")\n\n# Add true mean\nabline(h = 0.5, col = \"red\", lwd = 2, lty = 2)\n\nlegend(\"topright\",\n       legend = c(\"Running mean\", \"True mean = 0.5\"),\n       col = c(\"blue\", \"red\"),\n       lwd = 2,\n       lty = c(1, 2),\n       bty = \"n\")",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Workshop Solutions</span>"
    ]
  },
  {
    "objectID": "02_distributions.html",
    "href": "02_distributions.html",
    "title": "Statistical Distributions",
    "section": "",
    "text": "Simulation requires three essential ingredients:\n\n\n\nSimulation\n\n\n=\n\n\nModel\n\n\n+\n\n\nRandomness\n\n\n+\n\n\nRepetition\n\n\n\nTo move beyond uniform random numbers, we must understand how probability distributions describe random variables and how we can simulate from them.\nThis chapter develops the mathematical and computational foundations needed for:\n\nSimulating discrete and continuous distributions\nUnderstanding dependence\nTransforming random variables\nConstructing simulation algorithms\n\nLet’s get into it.",
    "crumbs": [
      "Statistical Distributions"
    ]
  },
  {
    "objectID": "02_1_univariate.html",
    "href": "02_1_univariate.html",
    "title": "5  Univariate Distributions",
    "section": "",
    "text": "5.1 Discrete Distributions\nSimulation is built upon the ability to generate random variables with specified probability distributions. Before developing simulation algorithms, we must revisit the mathematical structure of univariate distributions and clarify the probabilistic ideas that underlie them.\nA random variable is a numerical function defined on a sample space,\n\\[X : \\Omega \\to \\mathbb{R},\\]\nwhich assigns a real number to each outcome of a random experiment. In simulation, random variables are the primary objects we generate. Their distributions determine the behaviour of the systems we model.\nRandom variables may be discrete or continuous, and while the computational treatment differs slightly between these two cases, the underlying principles are unified.\nDiscrete distributions arise naturally whenever we simulate events, counts, or categorical outcomes. Each of the following plays a distinct structural role in modelling stochastic systems.\nWe can categorise discrete distributions into four groups:\nA discrete random variable takes values in a finite or countably infinite set. Its distribution is described by the probability mass function (PMF),\n\\[p(x) = P(X = x),\\]\nwhich must satisfy two fundamental conditions:\n\\[p(x) \\ge 0 \\;\\; \\forall x, \\quad \\sum_x p(x) = 1.\\]\nThe cumulative distribution function (CDF) is defined by\n\\[F(x) = P(X \\le x) = \\sum_{t \\le x} p(t).\\]\nThe CDF plays a central role in simulation theory, as it links probability with transformation methods that we will study later.",
    "crumbs": [
      "Statistical Distributions",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Univariate Distributions</span>"
    ]
  },
  {
    "objectID": "02_1_univariate.html#discrete-distributions",
    "href": "02_1_univariate.html#discrete-distributions",
    "title": "5  Univariate Distributions",
    "section": "",
    "text": "Single-Trial Model\n\nBernoulli → binary events, indicators, acceptance decisions\n\nRepeated Independent Trials\n\nBinomial → total successes across repetitions\nGeometric → time to first event (waiting time)\nNegative Binomial → flexible count modelling\n\nCount Processes\n\nPoisson → event counts in time/space\n\nMulti-Category Outcomes\n\nMultinomial → categorical sampling, Markov transitions\n\n\n\n\n\n\n\n\n\n\n5.1.1 Bernoulli Distribution\nThe simplest discrete distribution is the Bernoulli distribution. A Bernoulli random variable represents a single trial with two possible outcomes, often labelled 1 (success) and 0 (failure).\n\nWhy it matters in simulation:\nIt is the atomic unit of discrete randomness.\n\nMany complex models are constructed from repeated Bernoulli trials.\nIt underlies:\n\nBinary decision processes\nAcceptance–rejection algorithms (accept = 1, reject = 0)\nIndicator variables in Monte Carlo estimation\n\n\n\nIt can be used to represent a (possibly biased) coin toss where 1 and 0 would represent “heads” and “tails”, respectively, and \\(p\\) would be the probability of the coin landing on heads (or vice versa).\n\nIf \\(X\\) is an independent Bernoulli experiment with a probability of success \\(p\\): \\[X \\sim \\text{Bernoulli}(p)\\]\nthen the PMF of this distribution, over possible outcomes \\(k\\), is \\[\nP(X=x) = p^x(1-p)^{1-x} \\quad \\text{for } x \\in \\{0,1\\}.\n\\] \\[P(X=1)=p, \\quad P(X=0)=q=1-p.\\]\nThe CDF of the distribution is given by \\[\nF(x) = \\begin{cases}\n0 & x&lt;0, \\\\\n1-p & 0 \\leq x \\lt 1, \\\\\n1 & x \\geq 1.\n\\end{cases}\n\\]\nIts expectation and variance are \\[\\mathbb{E}[X]=p, \\qquad \\mathrm{Var}(X)=p(1-p)=pq.\\]\n\nExample: The following plots show the PMF and CDF of a biased coin with a 60% probability of landing heads (success).\n\n\nBernoulli(0.6) PMF and CDF\n# Parameter\np &lt;- 0.6   # Probability of success\n\n# Possible values\nx &lt;- c(0, 1)\n\n# PMF and CDF\npmf &lt;- dbinom(x, size = 1, prob = p)\ncdf &lt;- pbinom(x, size = 1, prob = p)\n\n# Set plotting area to 1 row, 2 columns\npar(mfrow = c(1, 2))\n\n# --- PMF Plot ---\nplot(x, pmf,\n     type = \"h\",\n     lwd = 6,\n     col = \"blue\",\n     ylim = c(0, 1),\n     xaxt = \"n\",\n     main = \"Bernoulli PMF\",\n     xlab = \"x\",\n     ylab = \"P(X = x)\")\n\naxis(1, at = x) # manually adds tick marks\npoints(x, pmf, pch = 19, col = \"red\", cex = 1.5)\n\n# --- CDF Plot ---\nplot(x, cdf,\n     type = \"s\",\n     lwd = 3,\n     col = \"darkgreen\",\n     ylim = c(0, 1),\n     xaxt = \"n\",\n     main = \"Bernoulli CDF\",\n     xlab = \"x\",\n     ylab = \"F(x)\")\n\naxis(1, at = x) # manually adds tick marks\npoints(x, cdf, pch = 19, col = \"red\", cex = 1.5)\n\n\n\n\n\n\n\n\n\nAdvanced: Using Plotly to create an interactive plot for different values of \\(p\\), where \\(0 \\leq p \\leq 1\\):\n\n\nBernoulli(p) with an interactive p slider\nlibrary(plotly)\nlibrary(dplyr)\nlibrary(tidyr)\n\n# Grid of p values for the slider\np_vals &lt;- seq(0, 1, by = 0.1)\nx_vals &lt;- c(0, 1)\n\n# Precompute PMF and CDF for all p (browser-only interactivity; no recomputation)\ndf &lt;- expand_grid(p = p_vals, x = x_vals) %&gt;%\n  mutate(\n    pmf = dbinom(x, size = 1, prob = p),\n    cdf = pbinom(x, size = 1, prob = p),\n    frame = sprintf(\"p = %.2f\", p),\n    x_chr = as.character(x)  # helps bar chart labeling\n  )\n\n# --- PMF (bar-like) ---\npmf_fig &lt;- df %&gt;%\n  plot_ly(\n    x = ~x_chr, y = ~pmf,\n    frame = ~frame,\n    type = \"bar\",\n    marker = list(color = \"steelblue\"),\n    hovertemplate = \"x=%{x}&lt;br&gt;P(X=x)=%{y:.2f}&lt;extra&gt;&lt;/extra&gt;\"\n  ) %&gt;%\n  layout(\n    title = list(text = \"Bernoulli PMF\"),\n    xaxis = list(title = \"x\", categoryorder = \"array\", categoryarray = c(\"0\",\"1\")),\n    yaxis = list(title = \"P(X = x)\", range = c(0, 1))\n  )\n\n# --- CDF (step + points) ---\ncdf_fig &lt;- df %&gt;%\n  plot_ly(\n    x = ~x, y = ~cdf,\n    frame = ~frame,\n    type = \"scatter\",\n    mode = \"lines+markers\",\n    line = list(color = \"darkgreen\", width = 3, shape = \"hv\"),\n    marker = list(color = \"red\", size = 8),\n    hovertemplate = \"x=%{x}&lt;br&gt;F(x)=%{y:.2f}&lt;extra&gt;&lt;/extra&gt;\"\n  ) %&gt;%\n  layout(\n    title = list(text = \"Bernoulli CDF\"),\n    xaxis = list(title = \"x\", tickmode = \"array\", tickvals = c(0,1)),\n    yaxis = list(title = \"F(x)\", range = c(0, 1))\n  )\n\n# Combine side-by-side and add slider + play/pause\nfig &lt;- subplot(pmf_fig, cdf_fig, nrows = 1, shareX = FALSE, titleX = TRUE, titleY = TRUE) %&gt;%\n  layout(\n    title = list(text = \"Bernoulli Distribution (interactive p slider)\"),\n    showlegend = FALSE\n  ) %&gt;%\n  animation_opts(frame = 0, transition = 0, redraw = TRUE) %&gt;%\n  animation_slider(currentvalue = list(prefix = \"\")) %&gt;%\n  animation_button(x = 1, xanchor = \"right\", y = 0, yanchor = \"bottom\")\n\nfig\n\n\n\n\n\n\n\n\n5.1.2 Binomial Distribution\nThe Binomial distribution counts the number of successes in \\(n\\) independent Bernoulli trials.\n\nWhy it matters in simulation:\n\nModels aggregated binary outcomes.\nAppears in reliability systems (number of working components).\nUsed in bootstrap and resampling procedures.\nForms a bridge between Bernoulli and Poisson processes.\n\nIn simulation studies, binomial counts often measure performance metrics across repeated trials.\n\n\nIf we repeat a Bernoulli experiment with probability of success equal to \\(p\\) for \\(n\\) times independently, the number of successes \\(X\\) follows a Binomial distribution: \\[X \\sim \\text{Binomial}(n,p),\\] with PMF and CDF \\[P(X=k)=\\binom{n}{k}p^k(1-p)^{n-k}, \\qquad\n\\sum_{k=0}^x P(X=k).\n\\]\nThe expectation and variance are \\[\\mathbb{E}[X]=np, \\qquad \\mathrm{Var}(X)=np(1-p).\\]\n\nIn simulation, the Binomial distribution often models counts of events within fixed trials. For example, defect counts in manufacturing or successful transmissions in a network.\nExample: Suppose you flip a fair coin 10 times. Each flip has a 50% chance of landing heads, and the flips are independent. Let X be the number of heads obtained in the 10 flips. The PMF plot shows the probability of obtaining each possible number of successes. Each point on the CDF plot gives the cumulative probability up to that value.\n\n\nBinomial(10, 0.5) PMF and CDF\n# Parameters\nn &lt;- 10      # number of trials\np &lt;- 0.5     # probability of success\n\n# Possible values\nx &lt;- 0:n\n\n# PMF and CDF\npmf &lt;- dbinom(x, size = n, prob = p)\ncdf &lt;- pbinom(x, size = n, prob = p)\n\n# Set plotting area: 1 row, 2 columns\npar(mfrow = c(1, 2))\n\n# --- PMF Plot ---\nplot(x, pmf,\n     type = \"h\",\n     lwd = 3,\n     col = \"blue\",\n     main = \"Binomial PMF\",\n     xlab = \"x\",\n     ylab = \"P(X = x)\",\n     ylim = c(0, max(pmf)))\n\npoints(x, pmf, pch = 19, col = \"red\")\n\n# --- CDF Plot ---\nplot(x, cdf,\n     type = \"s\",\n     lwd = 3,\n     col = \"darkgreen\",\n     main = \"Binomial CDF\",\n     xlab = \"x\",\n     ylab = \"F(x)\",\n     ylim = c(0, 1))\n\npoints(x, cdf, pch = 19, col = \"red\")\n\n\n\n\n\n\n\n\n\nAdvanced: Using Plotly to create an interactive plot for different values of \\(p\\), where \\(0 \\leq p \\leq 1\\):\n\n\nBinomial(10,p) with an interactive p slider\nlibrary(plotly)\nlibrary(dplyr)\nlibrary(tidyr)\n\nn &lt;- 10\np_vals &lt;- seq(0, 1, by = 0.1)     # coarser slider; change to 0.01 if you want smoother\nx_vals &lt;- 0:n\n\ndf &lt;- expand_grid(p = p_vals, x = x_vals) %&gt;%\n  mutate(\n    pmf = dbinom(x, size = n, prob = p),\n    cdf = pbinom(x, size = n, prob = p),\n    frame = sprintf(\"p = %.2f\", p)\n  )\n\npmf_fig &lt;- df %&gt;%\n  plot_ly(\n    x = ~x, y = ~pmf,\n    frame = ~frame,\n    type = \"bar\",\n    marker = list(color = \"steelblue\"),\n    hovertemplate = \"x=%{x}&lt;br&gt;P(X=x)=%{y:.4f}&lt;extra&gt;&lt;/extra&gt;\"\n  ) %&gt;%\n  layout(\n    title = list(text = paste0(\"Binomial PMF (n = \", n, \")\")),\n    xaxis = list(title = \"x\", dtick = 2),\n    yaxis = list(title = \"P(X = x)\")\n  )\n\ncdf_fig &lt;- df %&gt;%\n  plot_ly(\n    x = ~x, y = ~cdf,\n    frame = ~frame,\n    type = \"scatter\",\n    mode = \"lines+markers\",\n    line = list(color = \"darkgreen\", width = 3, shape = \"hv\"),\n    marker = list(color = \"red\", size = 7),\n    hovertemplate = \"x=%{x}&lt;br&gt;F(x)=%{y:.4f}&lt;extra&gt;&lt;/extra&gt;\"\n  ) %&gt;%\n  layout(\n    title = list(text = paste0(\"Binomial CDF (n = \", n, \")\")),\n    xaxis = list(title = \"x\", tickmode = \"array\", tickvals = seq(0, n, by = 2)),\n    yaxis = list(title = \"F(x)\", range = c(0, 1))\n  )\n\nfig &lt;- subplot(pmf_fig, cdf_fig, nrows = 1, shareX = FALSE, titleX = TRUE, titleY = FALSE) %&gt;%\n  layout(\n    title = list(text = \"Binomial Distribution (interactive p slider)\"),\n    showlegend = FALSE,\n    margin = list(l = 90, r = 30, t = 70, b = 55),\n    yaxis  = list(title = \"P(X = x)\", title_standoff = 15),\n    yaxis2 = list(title = \"F(x)\",     title_standoff = 15)\n  ) %&gt;%\n  animation_opts(frame = 0, transition = 0, redraw = TRUE) %&gt;%\n  animation_slider(currentvalue = list(prefix = \"\")) %&gt;%\n  animation_button(x = 1, xanchor = \"right\", y = 0, yanchor = \"bottom\")\n\nfig\n\n\n\n\n\n\n\n\n5.1.3 Geometric Distribution\nThe Geometric distribution models the number of trials required until the first success. in a sequence of independent Bernoulli trials, where each trial has a constant probability of success.\n\nWhy it matters in simulation:\n\nRepresents waiting times in discrete-time systems.\nDiscrete analogue of the exponential distribution.\nPossesses the memoryless property, making it important in stochastic process theory.\nUseful in modelling repeated attempts (e.g., retry mechanisms in networks).\n\n\n\nIf \\(X\\) is the number of trials (independent and identical Bernoulli experiment) required to get first success, and \\(p\\) is the success probability (\\(0 &lt; p \\leq 1\\)), then \\[X \\sim \\text{Geo}(p)\\]\nThe PMF of a geometric distribution (the probability that the \\(k\\)-th trial is the first success) is given by:\n\\[P(X = k) = (1 - p)^{k - 1} p\\]\nfor \\(k\\in \\mathbb{N}={1,2,3,...}\\).\nThe CDF of a geometric distribution is given by:\n\\[P(X \\leq k) = 1 - (1 - p)^k\\]\nThe mean and variance of a geometric distribution are:\n\\[E[X] = \\frac{1}{p}, \\qquad \\text{Var}[X] = \\frac{1 - p}{p^2}.\\]\n\nExample: An oil company conducts a geological study that indicates exploratory oil wells in a region have 10% chance of striking oil. The PMF plot shows the probability that the first strike will come on an \\(X-1\\) well, and the CDF plot gives the cumulative probability up to that value.\n\nWarning: Built-in geometric distribution functions in R model the number of failures before success occurs; therefore, we model \\(X-1\\) for \\(k \\in \\mathbb{N_0} = \\{0,1,2,...\\}.\\)\n\nWhat is the probability that first strike comes on a third well?\n\ndgeom(x = 2, prob = 0.1)\n\n[1] 0.081\n\n\n\n\nGeometric(0.1) PMF and CDF\n# Parameters\nn &lt;- 10      # number of trials\np &lt;- 0.1     # probability of success\n\n# Possible values\nx &lt;- 1:n\n\n# PMF and CDF\npmf &lt;- dgeom(x-1, prob = p)\ncdf &lt;- pgeom(x-1, prob = p)\n\n# Set plotting area: 1 row, 2 columns\npar(mfrow = c(1, 2))\n\n# --- PMF Plot ---\nplot(x, pmf,\n     type = \"h\",\n     lwd = 3,\n     col = \"blue\",\n     main = \"Geometric PMF\",\n     xlab = \"x\",\n     ylab = \"P(X = x)\",\n     ylim = c(0, max(pmf)))\n\npoints(x, pmf, pch = 19, col = \"red\")\n\n# --- CDF Plot ---\nplot(x, cdf,\n     type = \"s\",\n     lwd = 3,\n     col = \"darkgreen\",\n     main = \"Geometric CDF\",\n     xlab = \"x\",\n     ylab = \"F(x)\",\n     ylim = c(0, 1))\n\npoints(x, cdf, pch = 19, col = \"red\")\n\n\n\n\n\n\n\n\n\nNote: The plots above are generated using x = x-1 to model the number of trialsl.\n\n\n5.1.4 Negative Binomial Distribution\nThe Negative Binomial generalises the geometric distribution to the number of trials before \\(r\\) successes.\n\nWhy it matters in simulation:\n\nModels overdispersed count data (variance greater than mean).\nFrequently used in arrival modelling when Poisson assumptions are too restrictive.\nArises as a mixture of Poisson distributions (important in Bayesian modelling).\n\nIn simulation, it allows realistic modelling of variability beyond simple Poisson assumptions.\n\nLet \\(X\\) be the number of trials (independent and identical Bernoulli experiments) required to achieve \\(r\\) successes in independent Bernoulli trials with success probability \\(p\\), then\n\n\\[X \\sim \\text{NegBin}(r,p)\\]\nwith PMF\n\\[P(X = r) = \\binom{x - 1}{r-1} p^r (1 - p)^{x-r}.\\]\nThe CDF can be expressed in terms of the regularised incomplete beta function.\nThe mean and variance are\n\\[E[X] = \\frac{r}{p}, \\qquad \\text{Var}[X] = \\frac{r(1 - p)}{p^2}.\\]\n\nExample: We can use the same example as seen in the Geometric Distribution section. Previously, an oil company conducts a geological study that indicates exploratory oil wells in a region have 10% chance of striking oil. Now we can model the PMF, the probability that \\(r\\)-th strike comes on \\(X\\)-th well, and the corresponding CDF.\n\nWarning: Same as geometric distribution, negative binomial distribution functions in R model the number of failures before success occurs; therefore, we model \\(X-r\\) for \\(r &gt; 0.\\)\n\nWhat is the probability that second strike comes on seventh well?\n\ndnbinom(x = 7-2, size = 2, prob = 0.1)\n\n[1] 0.0354294\n\n\n\n\nNegBin(2,0.1) PMF and CDF\n# Parameters\nmax_x &lt;- 50  # number of trials\nr &lt;- 2       # number of successes\np &lt;- 0.1     # probability of success\n\n# Possible values\nx &lt;- r:max_x\n\n# PMF and CDF\npmf &lt;- dnbinom(x-r, size = r, prob = p)\ncdf &lt;- pnbinom(x-r, size = r, prob = p)\n\n# Set plotting area: 1 row, 2 columns\npar(mfrow = c(1, 2))\n\n# --- PMF Plot ---\nplot(x, pmf,\n     type = \"h\",\n     lwd = 3,\n     col = \"blue\",\n     main = \"Negative Binomial PMF\",\n     xlab = \"x\",\n     ylab = \"P(X = x)\",\n     ylim = c(0, max(pmf)))\n\npoints(x, pmf, pch = 19, col = \"red\")\n\n# --- CDF Plot ---\nplot(x, cdf,\n     type = \"s\",\n     lwd = 3,\n     col = \"darkgreen\",\n     main = \"Negative Binomial CDF\",\n     xlab = \"x\",\n     ylab = \"F(x)\",\n     ylim = c(0, 1))\n\npoints(x, cdf, pch = 19, col = \"red\")\n\n\n\n\n\n\n\n\n\n\n\n5.1.5 Poisson Distribution\nThe Poisson distribution arises naturally in modelling the number of events occurring, \\(X\\), in a fixed interval of time or space.\n\nWhy it matters in simulation:\n\nFundamental to queueing theory.\nGoverns random arrival systems.\nProvides a natural approximation to binomial counts with rare events.\nPlays a central role in discrete-event simulation.\n\nMany simulation models begin with:\n\n“Assume arrivals follow a Poisson process.”\n\n\nIf events occur independently at a constant average rate \\(\\lambda\\), then\n\n\\[X \\sim \\text{Poisson}(\\lambda),\\]\nwith PMF and CDF\n\\[\nP(X=k)=\\frac{e^{-\\lambda}\\lambda^k}{k!}, \\qquad\n\\sum_{k=0}^x P(X=k)\n\\]\nA distinctive feature is that both expectation and variance,\n\\[\\mathbb{E}[X]=\\mathrm{Var}(X)=\\lambda.\\]\n\nExample: Suppose customers arrive at a service desk at an average rate of 4 customers per hour. Assume arrivals occur independently and at a constant average rate. The PMF plot shows the probability of observing each possible number of arrivals in one hour. Each point on the CDF plot gives the cumulative probability of observing at most that many arrivals.\n\n\nPoisson(4) PMF and CDF\n# Parameter\nlambda &lt;- 4   # mean (rate)\n\n# Reasonable range of x values\nx &lt;- 0:(lambda + 4*sqrt(lambda))\nx &lt;- 0:max(x)\n\n# PMF and CDF\npmf &lt;- dpois(x, lambda = lambda)\ncdf &lt;- ppois(x, lambda = lambda)\n\n# Set plotting area: 1 row, 2 columns\npar(mfrow = c(1, 2))\n\n# --- PMF Plot ---\nplot(x, pmf,\n     type = \"h\",\n     lwd = 3,\n     col = \"blue\",\n     main = expression(paste(\"Poisson PMF (\", lambda, \" = \", 4, \")\")),\n     xlab = \"x\",\n     ylab = \"P(X = x)\",\n     ylim = c(0, max(pmf)))\n\npoints(x, pmf, pch = 19, col = \"red\")\n\n# --- CDF Plot ---\nplot(x, cdf,\n     type = \"s\",\n     lwd = 3,\n     col = \"darkgreen\",\n     main = expression(paste(\"Poisson CDF (\", lambda, \" = \", 4, \")\")),\n     xlab = \"x\",\n     ylab = \"F(x)\",\n     ylim = c(0, 1))\n\npoints(x, cdf, pch = 19, col = \"red\")\n\n\n\n\n\n\n\n\n\nAdvanced: Using Plotly to create an interactive plot for different values of \\(\\lambda\\):\n\n\nPoisson(\\(\\lambda\\)) with an interactive \\(\\lambda\\) slider\nlibrary(plotly)\nlibrary(dplyr)\nlibrary(tidyr)\n\n# ---- Parameters ----\nlambda_vals &lt;- seq(0, 9, by = 1)   # slider values\nlambda_max  &lt;- max(lambda_vals)\nx_max &lt;- ceiling(lambda_max + 4 * sqrt(lambda_max))\nx_vals &lt;- 0:x_max\n\n# ---- Precompute PMF & CDF ----\ndf &lt;- expand_grid(lambda = lambda_vals, x = x_vals) %&gt;%\n  mutate(\n    pmf = dpois(x, lambda),\n    cdf = ppois(x, lambda),\n    frame = sprintf(\"λ = %.1f\", lambda)\n  )\n\n# ---- PMF ----\npmf_fig &lt;- df %&gt;%\n  plot_ly(\n    x = ~x, y = ~pmf,\n    frame = ~frame,\n    type = \"bar\",\n    marker = list(color = \"steelblue\"),\n    hovertemplate = \"x=%{x}&lt;br&gt;P(X=x)=%{y:.4f}&lt;extra&gt;&lt;/extra&gt;\"\n  ) %&gt;%\n  layout(\n    title = list(text = \"Poisson PMF\"),\n    xaxis = list(title = \"x\", dtick = 2),\n    yaxis = list(title = \"P(X = x)\")\n  )\n\n# ---- CDF ----\ncdf_fig &lt;- df %&gt;%\n  plot_ly(\n    x = ~x, y = ~cdf,\n    frame = ~frame,\n    type = \"scatter\",\n    mode = \"lines+markers\",\n    line = list(shape = \"hv\", width = 3, color = \"darkgreen\"),\n    marker = list(size = 7, color = \"red\"),\n    hovertemplate = \"x=%{x}&lt;br&gt;F(x)=%{y:.4f}&lt;extra&gt;&lt;/extra&gt;\"\n  ) %&gt;%\n  layout(\n    title = list(text = \"Poisson CDF\"),\n    xaxis = list(title = \"x\", tickmode = \"array\", tickvals = seq(0, x_max, by = 2)),\n    yaxis = list(title = \"F(x)\", range = c(0, 1))\n  )\n\n# ---- Combine + slider ----\nfig &lt;- subplot(\n  pmf_fig, cdf_fig,\n  nrows = 1,\n  shareX = FALSE,\n  titleX = TRUE,\n  titleY = FALSE\n) %&gt;%\n  layout(\n    title = list(text = \"Poisson Distribution (interactive λ slider)\"),\n    showlegend = FALSE,\n    margin = list(l = 90, r = 30, t = 70, b = 55),\n    yaxis  = list(title = \"P(X = x)\", title_standoff = 15),\n    yaxis2 = list(title = \"F(x)\",     title_standoff = 15)\n  ) %&gt;%\n  animation_opts(frame = 0, transition = 0, redraw = TRUE) %&gt;%\n  animation_slider(currentvalue = list(prefix = \"\")) %&gt;%\n  animation_button(x = 1, xanchor = \"right\", y = 0, yanchor = \"bottom\")\n\nfig\n\n\n\n\n\n\n\n\n5.1.6 Multinomial Distribution\nThe Multinomial distribution generalises the binomial to more than two outcome categories.\n\nWhy it matters in simulation:\n\nModels categorical random outcomes.\nEssential for simulating:\n\nMarkov chains\nDiscrete-state stochastic processes\nClassification and allocation models\n\nUnderlies Gibbs sampling in discrete settings.\n\nIn simulation algorithms, sampling from a categorical distribution is a core primitive operation.\n\nSuppose we perform \\(n\\) independent trials.\nEach trial results in exactly one of \\(k\\) possible categories, with probabilities\n\\[p_1, p_2, \\dots, p_k, \\qquad \\text{where } p_i \\ge 0, \\quad \\sum_{i=1}^k p_i = 1.\\]\n\nWhen \\(k = 2\\), the multinomial reduces to the binomial distribution.\n\n\nLet \\(X_i =\\) number of times outcome \\(i\\) occurs in \\(n\\) trials.\nThen the random vector \\((X_1, X_2, \\dots, X_k)\\) follows a Multinomial distribution, written as\n\\[(X_1, \\dots, X_k) \\sim \\text{Multinomial}(n; p_1, \\dots, p_k),\\]\nsubject to the constraint: \\(\\displaystyle\\sum_{i=1}^k X_i = n.\\)\nThus, the multinomial distribution describes the joint counts of categorical outcomes across repeated independent trials.\nThe joint PMF is\n\\[\nP(X_1 = x_1, \\dots, X_k = x_k)\n=\n\\frac{n!}{x_1! x_2! \\cdots x_k!}\n\\prod_{i=1}^k p_i^{x_i}.\n\\]\nThe joint CDF is\n\\[\nF(x_1, \\dots, x_k)\n=\nP(X_1 \\le x_1, \\dots, X_k \\le x_k).\n\\]\nThere is no simple closed-form expression for the general multinomial CDF. It is computed by summing the PMF over all count vectors satisfying:\n\\[\n\\sum_{i=1}^k y_i = n\n\\quad \\text{and} \\quad\n0 \\le y_i \\le x_i.\n\\]\nEach component has mean and variance:\n\\[\\mathbb{E}[X_i] = n p_i, \\quad \\mathrm{Var}(X_i) = n p_i (1 - p_i).\\]\nThis resembles the binomial mean and variance because each \\(X_i\\) marginally follows \\(X_i \\sim \\text{Binomial}(n,p_i)\\)\n\nExample: Suppose we roll a fair six-sided die \\(n = 10\\) times. Each roll results in one of 6 possible outcomes. Each outcome has probability of 1/6. Let \\(X_i\\) be number of times face \\(i\\) appears in 10 rolls. Then,\n\\[\n(X_1, X_2, X_3, X_4, X_5, X_6)\n\\sim\n\\text{Multinomial}\\left(10; \\frac16, \\frac16, \\frac16, \\frac16, \\frac16, \\frac16\\right).\n\\]\nWhat is the probability that the 10 rolls result in:\n\n1 appears 2 times\n2 appears 1 time\n3 appears 3 times\n4 appears 0 times\n5 appears 2 times\n6 appears 2 times\n\nSo, \\((x_1,x_2,x_3,x_4,x_5,x_6) = (2,1,3,0,2,2)\\)\nThe probability of this exact configuration is\n\\[\nP =\n\\frac{10!}{2!1!3!0!2!2!}\n\\left(\\frac16\\right)^{10}.\n\\]\n\ndmultinom(x = c(2,1,3,0,2,2), prob = rep(1/6, 6))\n\n[1] 0.001250286\n\n\nThe multinomial distribution is multivariate, so it does not have a simple 1D PMF curve like the binomial or Poisson. We must decide what we want to visualise. If we visualise a marginal distribution of one face \\(X_i \\sim \\text{Binomial}(10,1/6)\\), it will look just like the same old binomial distribution.\n\n\nMarginal Distribution of \\(X_1\\)\nn &lt;- 10\np &lt;- 1/6\nk &lt;- 0:n\n\npmf &lt;- dbinom(k, size = n, prob = p)\n\nplot(k, pmf,\n     type = \"h\",\n     lwd = 3,\n     col = \"blue\",\n     xlab = \"Number of times face 1 appears\",\n     ylab = \"Probability\",\n     ylim = c(0, max(pmf)))\n\npoints(k, pmf, pch = 19, col = \"red\")\n\n\n\n\n\n\n\n\n\nInstead, we can try simulating many multinomial experiments and visualise the counts.\n\n\nSimulated Multinomial Counts\nset.seed(123)\nsim &lt;- rmultinom(10000, size = 10, prob = rep(1/6, 6))\nboxplot(t(sim),\n        names = paste(\"Face\", 1:6),\n        ylab = \"Count in 10 rolls\")\n\n\n\n\n\n\n\n\n\n\nAll six boxplots look almost identical because each face has the same marginal distribution and probability (fair die).\nThe counts fluctuate around about 1 or 2, which matches the expected value of 10 divided by 6.\nNotice that large counts (&gt;3) are rare, since this is just 10 rolls.\nAlthough the boxplots look independent, the total number of rolls is fixed at 10, so if one face appears more often, others must appear less often.\n\nConsider the dependence between the count of Face 1 \\(X_1\\) and the dependence of Face 2 \\(X_2\\):\n\n\nNegative Dependence in Multinomial\nplot(sim[1,], sim[2,],\n     xlab = \"Count of Face 1\",\n     ylab = \"Count of Face 2\")\n\n\n\n\n\n\n\n\n\n\nAs the count of Face 1 increases, the possible values for Face 2 decrease. (The categories competes.)\nThat competition produces negative dependence, which we can literally see as a downward-sloping triangular region.",
    "crumbs": [
      "Statistical Distributions",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Univariate Distributions</span>"
    ]
  },
  {
    "objectID": "02_1_univariate.html#continuous-distributions",
    "href": "02_1_univariate.html#continuous-distributions",
    "title": "5  Univariate Distributions",
    "section": "5.2 Continuous Distributions",
    "text": "5.2 Continuous Distributions\nContinuous distributions play two fundamental roles in simulation:\n\nModelling real-world variability (waiting times, measurement noise, lifetimes).\nServing as algorithmic building blocks (via transformation and composition).\n\nWe can categorise continuous distributions into:\n\nPrimitive distributions\n\nUniform → foundation of simulation algorithms.\n\nEvent-time distributions (positive support)\n\nExponential → waiting time for first Poisson event.\nGamma → sum of exponentials.\n\nLimit and noise distributions\n\nNormal → limit of sums (CLT).\n\nTransformation-based distributions (shape-flexible)\n\nLog-normal → exponential of normal.\nBeta → normalised gamma variables.\n\n\nIt is noted that\n\nComplex distributions are often constructed from simpler ones.\n\nUnlike discrete variables, continuous random variables assign probability to intervals rather than individual points. Their distribution is described by a probability density function (PDF), \\(f(x)\\), satisfying\n\\[f(x) \\ge 0, \\qquad \\int_{-\\infty}^{\\infty} f(x)\\,dx = 1.\\]\nProbabilities are obtained via integration:\n\\[P(a \\le X \\le b) = \\int_a^b f(x)\\,dx.\\]\nThe cumulative distribution function (CDF) is\n\\[F(x) = \\int_{-\\infty}^x f(t)\\,dt.\\]\nThe CDF is always non-decreasing and continuous from the right. Importantly, it provides the bridge between probability theory and simulation algorithms.\n\n5.2.1 Uniform Distribution\nUniform is a distribution where all outcomes in the range \\([a,b]\\) are equally likely.\n\nWhy it matters in simulation:\n\nAll inverse transform methods start from Uniform(0,1).\nAcceptance–rejection relies on uniform draws.\nMonte Carlo integration uses uniform sampling.\nRandom number generators are designed to approximate Uniform(0,1).\n\nIn simulation theory, Uniform(0,1) is the “raw material” from which all randomness is manufactured.\n\n\nFor \\[\nX \\sim \\text{Uniform}(a,b),\n\\]\nthe PDF is defined by\n\\[\nf(x)=\n\\begin{cases}\n\\dfrac{1}{b-a} & a\\leq x \\leq b \\\\\n0             & x &lt; a \\;\\text{ or }\\; x &gt; b,\n\\end{cases}\n\\]\nand the CDF is given by\n\\[\nF(x)=\n\\begin{cases}\n0 & x&lt;a \\\\\n\\dfrac{x-a}{b-a} & a\\leq x \\leq b \\\\\n1             & x &gt; b.\n\\end{cases}\n\\]\nIts expectation and variance are\n\\[\n\\mathbb{E}[X]=\\frac{a+b}{2}, \\qquad\n\\mathrm{Var}(X)=\\frac{(b-a)^2}{12}.\n\\]\n\nExample:\nA random variable \\(X\\) follows a Uniform(2,8) distribution. What is the probability that \\(X\\) takes a value greater than 5?\n\n1 - punif(q = 5, min = 2, max = 8)\n\n[1] 0.5\n\n# or\npunif(q = 5, min = 2, max = 8, lower.tail=FALSE)\n\n[1] 0.5\n\n\n\n\nUniform(2,8) PDF and CDF\n# Parameters\na &lt;- 2     # lower bound\nb &lt;- 8     # upper bound\n\n# Sequence of x values\nx &lt;- seq(a - 1, b + 1, length = 500)\n\n# PDF and CDF\npdf &lt;- dunif(x, min = a, max = b)\ncdf &lt;- punif(x, min = a, max = b)\n\n# Set plotting area: 1 row, 2 columns\npar(mfrow = c(1, 2))\n\n# --- PDF Plot ---\nplot(x, pdf,\n     type = \"l\",\n     lwd = 3,\n     col = \"blue\",\n     main = paste(\"Uniform(\", a, \",\", b, \") PDF\", sep=\"\"),\n     xlab = \"x\",\n     ylab = \"f(x)\",\n     ylim = c(0, max(pdf) * 1.2))\n\nabline(v = c(a, b), lty = 2, col = \"red\")\n\n# --- CDF Plot ---\nplot(x, cdf,\n     type = \"l\",\n     lwd = 3,\n     col = \"darkgreen\",\n     main = paste(\"Uniform(\", a, \",\", b, \") CDF\", sep=\"\"),\n     xlab = \"x\",\n     ylab = \"F(x)\",\n     ylim = c(0, 1))\n\nabline(v = c(a, b), lty = 2, col = \"red\")\n\n\n\n\n\n\n\n\n\n\n\n5.2.2 Exponential Distribution\nThe exponential distribution models waiting times between events in a Poisson process. If the number of events in any interval of length \\(t\\) follows\n\\[\nN(t)\\sim \\mathrm{Poisson}(\\lambda t).\n\\]\nThe waiting time until the next event, call it \\(T\\), satisfies \\[\nP(T&gt;t)=P(\\mathrm{no\\  events\\  occur\\  in\\  }[0,t])=e^{-\\lambda t}.\n\\]\nThis survival function corresponds exactly to an Exponential(\\(\\lambda\\)) distribution. Thus: \\[\nT\\sim \\mathrm{Exponential}(\\lambda ).\n\\]\nSo the exponential distribution is precisely the model for inter‑arrival times in a Poisson process.\n\nWhy it matters in simulation\n\nThe memoryless property: \\[P(X&gt;s+t \\mid X&gt;s)=P(X&gt;t),\\] makes the exponential distribution fundamental in\n\ncontinuous-time Markov processes,\nqueueing theory,\nreliability modelling (modelling system lifetime, time before failure, etc.)\n\nIn discrete-event simulation, inter-arrival and service times are often assumed exponential.\nIt forms the backbone of many stochastic timing models.\n\n\n\nIf\n\\[X \\sim \\text{Exp}(\\lambda),\\]\nthen PMF is\n\\[f(x)=\\lambda e^{-\\lambda x}, \\qquad x&gt;0,\\]\nand CDF is\n\\[F(x)=1-e^{-\\lambda x}.\\]\nIts mean and variance are\n\\[\n\\mathbb{E}[X]=\\frac{1}{\\lambda}, \\qquad\n\\mathrm{Var}(X)=\\frac{1}{\\lambda^2}.\n\\]\n\nExample: A machine in a manufacturing plant occasionally breaks down. The time (in hours) between breakdowns follows an Exponential distribution with rate \\(\\lambda =1.5\\). What is the probability that the machine runs for more than 1 hour before the next breakdown?\n\npexp(q = 1, rate = 1.5, lower.tail = FALSE)\n\n[1] 0.2231302\n\n\n\n\nExponential(1.5) PDF and CDF\n# Parameter\nlambda &lt;- 1.5   # rate parameter\n\n# Sequence of x values (reasonable range)\nx &lt;- seq(0, 5/lambda, length = 500)\n\n# PDF and CDF\npdf &lt;- dexp(x, rate = lambda)\ncdf &lt;- pexp(x, rate = lambda)\n\n# Set plotting area: 1 row, 2 columns\npar(mfrow = c(1, 2))\n\n# --- PDF Plot ---\nplot(x, pdf,\n     type = \"l\",\n     lwd = 3,\n     col = \"blue\",\n     main = expression(paste(\"Exponential(\", lambda, \" = \", 1.5, \") PDF\")),\n     xlab = \"x\",\n     ylab = \"f(x)\")\n\n# --- CDF Plot ---\nplot(x, cdf,\n     type = \"l\",\n     lwd = 3,\n     col = \"darkgreen\",\n     main = expression(paste(\"Exponential(\", lambda, \" = \", 1.5, \") CDF\")),\n     xlab = \"x\",\n     ylab = \"F(x)\",\n     ylim = c(0, 1))\n\n\n\n\n\n\n\n\n\nExample: Linking Exponential and Poisson\nCustomers arrive at a small service kiosk according to a Poisson process with rate \\(\\lambda =1.5\\) customers per hour. That means:\n\nThe number of arrivals in time \\(t\\) is \\(N(t)\\sim \\mathrm{Poisson}(\\lambda t)\\).\nThe waiting time between arrivals is \\(X\\sim \\mathrm{Exponential}(\\lambda )\\).\n\nWhat is the probability that the next customer arrives within 30 minutes (interarrival-time; exponential)?\n\npexp(q = 0.5, rate = 1.5)\n\n[1] 0.5276334\n\n\nWhat is the probability that exactly 2 customers arrive in the next hour (count; poisson)?\n\ndpois(x = 2, lambda = 1.5)\n\n[1] 0.2510214\n\n\n\n\n5.2.3 Gamma Distribution\nThe gamma distribution generalises the exponential distribution. It represents the sum of independent exponential random variables, and also waiting time until the \\(\\alpha\\)-th event in a Poisson process.\n\nWhy it matters in simulation\n\nProvides flexible modelling of positive, skewed data.\nUsed in reliability and survival analysis.\nAppears as a conjugate prior in Bayesian inference.\nForms the basis of many hierarchical models.\n\nThe gamma distribution allows more realistic modelling of waiting times than the exponential, by relaxing the memoryless assumption.\n\n\nLet\n\\[\nX\\sim \\mathrm{Gamma}(\\alpha ,\\beta )\n\\]\nwhere \\(\\alpha &gt;0\\) is the shape parameter and \\(\\beta &gt;0\\) is the rate parameter (so the scale is \\(1/\\beta\\).)\nThe PDF for Gamma distribution is\n\\[\nf(x)=\\frac{\\beta ^{\\alpha }}{\\Gamma (\\alpha )}x^{\\alpha -1}e^{-\\beta x},\\qquad x&gt;0.\n\\]\nThe CDF is\n\\[\nF(x)=P(X\\leq x)=\\frac{\\gamma (\\alpha ,\\beta x)}{\\Gamma (\\alpha )},\n\\]\nwhere \\(\\gamma (\\alpha ,\\beta x)\\) is the lower incomplete gamma function.\nThe expectation and variance are \\[\n\\mathbb{E}[X]=\\frac{\\alpha }{\\beta }, \\qquad\n\\mathrm{Var}(X)=\\frac{\\alpha }{\\beta ^2}.\n\\]\n\nExample: Suppose a claim size X of a general insurance has gamma distribution with \\(\\alpha = 8\\) and \\(\\beta = 1/15\\). Calculate the probability that claim size is between 60 and 120.\n\ns &lt;- 8 ; r &lt;- 1/15\npgamma(q=120, shape=s, rate=r) - pgamma(q=60, shape=s, rate=r)\n\n[1] 0.4959056\n\n\n\n\nGamma(8, 1/15) PDF and CDF\n# Parameter\nalpha &lt;- 8   # shape parameter\nbeta &lt;- 1/15  # rate parameter\n\n# Sequence of x values (reasonable range)\nx &lt;- seq(0, alpha*50, length = 500)\n\n# PDF and CDF\npdf &lt;- dgamma(x, shape = alpha, rate = beta)\ncdf &lt;- pgamma(x, shape = alpha, rate = beta)\n\n# Set plotting area: 1 row, 2 columns\npar(mfrow = c(1, 2))\n\n# --- PDF Plot ---\nplot(x, pdf,\n     type = \"l\",\n     lwd = 3,\n     col = \"blue\",\n     main = \"Gamma(8,1/15) PDF\",\n     xlab = \"x\",\n     ylab = \"f(x)\")\n\n# --- CDF Plot ---\nplot(x, cdf,\n     type = \"l\",\n     lwd = 3,\n     col = \"darkgreen\",\n     main = \"Gamma(8,1/15) CDF\",\n     xlab = \"x\",\n     ylab = \"F(x)\",\n     ylim = c(0, 1))\n\n\n\n\n\n\n\n\n\nFor integer \\(\\alpha =k\\), the CDF simplifies to a Poisson‑style sum:\n\\[\nF(x)=1-e^{-\\beta x}\\sum _{j=0}^{k-1}\\frac{(\\beta x)^j}{j!}.\n\\]\nThis is a beautiful link to the Poisson process — the Gamma(\\(k\\),\\(\\lambda\\)) is the waiting time for the \\(k\\)‑th event.\nExample: Customers arrive at a service counter according to a Poisson process with rate \\(\\lambda =2\\) customers per hour. Let \\(T_3\\) be the waiting time until the 3rd customer arrives. Because the waiting time for the \\(k\\)-th event in a Poisson process is\n\\[\nT_k\\sim \\mathrm{Gamma}(\\alpha =k,\\; \\beta =\\lambda ),\n\\]\nwe have\n\\[\nT_3\\sim \\mathrm{Gamma}(3,2).\n\\]\nWhat is the probability that the 3rd customer arrives within 2 hours?\n\npgamma(q = 2, shape = 3, rate = 2)\n\n[1] 0.7618967\n\n\n\n\n5.2.4 Normal Distribution\nThe normal distribution or Gaussian distribution describes uncertainty as a symmetric bell-shaped curve. Arising through the Central Limit Theorem, it is considered the most important continuous distribution in applied probability.\n\nWhat it matters in simulation\n\nMany models assume normally distributed errors.\nGaussian random walks underlie MCMC algorithms.\nBrownian motion is constructed from normal increments.\nVariance estimation often relies on normal approximations.\n\nEven when the true distribution is not normal, simulation output is often analysed using normal approximations.\n\n\nA random variable\n\\[X \\sim \\text{Normal}(\\mu,\\sigma^2)\\]\nhas PDF\n\\[\nf(x)=\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\n\\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right),\n\\]\nand CDF\n\\[\nF(x)=\\Phi\\left(\\frac{x-\\mu}{\\sigma}\\right)\n\\]\nThe parameters \\(\\mu\\) and \\(\\sigma^2\\) represent the mean and variance.\n\nExample A standardised test is designed so that student scores follow a standard normal distribution. If a student receives a z-score of 1.2 on the exam, what proportion of students scored below this student?\n\npnorm(1.2)\n\n[1] 0.8849303\n\n\n\n\nStandard Normal PMF and CDF\n# Parameters\nmu &lt;- 0        # mean\nsigma &lt;- 1     # standard deviation\n\n# Sequence of x values (cover most of the distribution)\nx &lt;- seq(mu - 4*sigma, mu + 4*sigma, length = 500)\n\n# PDF and CDF\npdf &lt;- dnorm(x, mean = mu, sd = sigma)\ncdf &lt;- pnorm(x, mean = mu, sd = sigma)\n\n# Set plotting area: 1 row, 2 columns\npar(mfrow = c(1, 2))\n\n# --- PDF Plot ---\nplot(x, pdf,\n     type = \"l\",\n     lwd = 3,\n     col = \"blue\",\n     main = \"Normal(0, 1) PDF\",\n     xlab = \"x\",\n     ylab = \"f(x)\")\n\nabline(v = mu, lty = 2, col = \"red\")\n\n# --- CDF Plot ---\nplot(x, cdf,\n     type = \"l\",\n     lwd = 3,\n     col = \"darkgreen\",\n     main = \"Normal(0, 1) CDF\",\n     xlab = \"x\",\n     ylab = \"F(x)\",\n     ylim = c(0, 1))\n\nabline(v = mu, lty = 2, col = \"red\")\n\n\n\n\n\n\n\n\n\n\n\n5.2.5 Log-Normal Distribution\nThe normal distribution is symmetric and defined over all real numbers, so it may not suit variables that are strictly positive or strongly skewed, such as body weight or stock prices. In such cases, distributions like the log‑normal are often more appropriate.\n\nWhy it matters in simulation\n\nUsed in financial modelling (asset prices).\nModels service times with heavy right tails.\nArises naturally via transformation methods.\nDemonstrates how distributions can be constructed via nonlinear transformation.\n\nIt is an important example of how tail behaviour changes under transformation.\n\n\nLet \\[\nX\\sim \\mathrm{Lognormal}(\\mu ,\\sigma ^2)\n\\]\nmeaning\n\\[\\ln X\\sim N(\\mu ,\\sigma ^2).\\]\nThe PDF is given by \\[\nf(x)=\\frac{1}{x\\sigma \\sqrt{2\\pi }}\\exp \\! \\left( -\\frac{(\\ln x-\\mu )^2}{2\\sigma ^2}\\right) ,\\qquad x&gt;0.\n\\]\nThe CDF is given by \\[\nF(x)=P(X\\leq x)=\\Phi \\! \\left( \\frac{\\ln x-\\mu }{\\sigma }\\right) ,\\qquad x&gt;0,\n\\]\nwhere \\(\\Phi (\\cdot )\\) is the standard normal CDF.\nThen, the mean and variance are \\[\n\\mathbb{E}[X]=e^{\\mu +\\frac{1}{2}\\sigma ^2}, \\qquad \\mathrm{Var}(X)=\\left( e^{\\sigma ^2}-1\\right) e^{2\\mu +\\sigma ^2}.\\]\n\nExample: the price of a certain stock is modeled as log‑normally distributed with parameters \\(\\mu =2\\) and \\(\\sigma =0.3\\). This means\n\\[\n\\ln (X)\\sim N(2,\\, 0.3^2).\\]\nWhat is the probability that the stock price is less than 10?\n\nplnorm(q = 10, meanlog = 2, sdlog = 0.3)\n\n[1] 0.8434208\n\n\n\n\nLogNormal(2, 0.09) PDF and CDF\n# Parameter\nmu &lt;- 2   # shape parameter\nsigma &lt;- 0.3  # rate parameter\n\n# Sequence of x values (reasonable range)\nx &lt;- seq(0, mu*10, length = 500)\n\n# PDF and CDF\npdf &lt;- dlnorm(x, meanlog = 2, sdlog = 0.3)\ncdf &lt;- plnorm(x, meanlog = 2, sdlog = 0.3)\n\n# Set plotting area: 1 row, 2 columns\npar(mfrow = c(1, 2))\n\n# --- PDF Plot ---\nplot(x, pdf,\n     type = \"l\",\n     lwd = 3,\n     col = \"blue\",\n     main = \"LogNormal(2, 0.09) PDF\",\n     xlab = \"x\",\n     ylab = \"f(x)\")\n\n# --- CDF Plot ---\nplot(x, cdf,\n     type = \"l\",\n     lwd = 3,\n     col = \"darkgreen\",\n     main = \"LogNormal(2, 0.09) CDF\",\n     xlab = \"x\",\n     ylab = \"F(x)\",\n     ylim = c(0, 1))\n\n\n\n\n\n\n\n\n\n\n\n5.2.6 Beta Distribution\nThe beta distribution is defined on (0,1), making it ideal for modelling probabilities and proportions. In Bayesian inference, the beta distribution is the conjugate prior probability distribution for the Bernoulli, binomial, negative binomial, and geometric distributions. It has flexible shapes (U-shaped, symmetric, skewed).\n\nWhy it matters in simulation\n\nUsed in Bayesian updating.\nModels uncertainty about probabilities.\nAppears in hierarchical simulation models.\nUseful in sensitivity analysis.\n\nThe beta distribution provides controlled randomness on a finite interval.\n\n\nLet\n\\[\nX\\sim \\mathrm{Beta}(\\alpha ,\\beta )\n\\]\nwith shape parameters \\(\\alpha &gt;0\\) and \\(\\beta &gt;0\\). The support is the interval \\(0&lt;x&lt;1\\).\nThe PDF is\n\\[\nf(x)=\\frac{x^{\\alpha -1}(1-x)^{\\beta -1}}{B(\\alpha ,\\beta )},\\qquad 0&lt;x&lt;1,\n\\]\nwhere the \\(B(\\alpha,\\beta)\\) is the Beta function given by\n\\[\nB(\\alpha ,\\beta )=\\frac{\\Gamma (\\alpha )\\Gamma (\\beta )}{\\Gamma (\\alpha +\\beta )}\n\\]\nThe CDF is\n\\[F(x)=P(X\\leq x)=I_x(\\alpha ,\\beta ),\\]\nwhere \\(I_x(\\alpha ,\\beta )\\) is the regularised incomplete beta function.\nThe mean and variance are \\[\n\\mathbb{E}[X]=\\frac{\\alpha }{\\alpha +\\beta }, \\qquad\n\\mathrm{Var}(X)=\\frac{\\alpha \\beta }{(\\alpha +\\beta )^2(\\alpha +\\beta +1)}.\n\\]\n\nExample: A website tracks the proportion of visitors who click on a new advertisement. Based on past data, the marketing team models the click‑through rate \\(p\\) as:\n\\[\np\\sim \\mathrm{Beta}(\\alpha =4,\\; \\beta =16).\n\\]\nWhat is the probability that a randomly chosen visitor clicks on the ad on average?\n\n4 / (4+16)\n\n[1] 0.2\n\n\n\n\nBeta(alpha, beta) with interactive sliders\nlibrary(plotly)\nlibrary(dplyr)\nlibrary(tidyr)\n\nalpha_vals &lt;- seq(.5, 5, by = .5)\nbeta_vals  &lt;- seq(.5, 5, by = .5)\nx_vals &lt;- seq(0, 1, length.out = 200)\n\ndf &lt;- expand_grid(alpha = alpha_vals,\n                  beta  = beta_vals,\n                  x = x_vals) %&gt;%\n  mutate(\n    pdf = dbeta(x, shape1 = alpha, shape2 = beta),\n    cdf = pbeta(x, shape1 = alpha, shape2 = beta),\n    frame = paste0(\"α=\", alpha, \", β=\", beta)\n  )\n\npdf_fig &lt;- df %&gt;%\n  plot_ly(\n    x = ~x, y = ~pdf,\n    frame = ~frame,\n    type = \"scatter\",\n    mode = \"lines\",\n    line = list(color = \"steelblue\", width = 3),\n    hovertemplate = \"x=%{x:.2f}&lt;br&gt;f(x)=%{y:.4f}&lt;extra&gt;&lt;/extra&gt;\"\n  ) %&gt;%\n  layout(\n    title = \"Beta PDF\",\n    xaxis = list(title = \"x\"),\n    yaxis = list(title = \"f(x)\")\n  )\n\ncdf_fig &lt;- df %&gt;%\n  plot_ly(\n    x = ~x, y = ~cdf,\n    frame = ~frame,\n    type = \"scatter\",\n    mode = \"lines\",\n    line = list(color = \"darkgreen\", width = 3),\n    hovertemplate = \"x=%{x:.2f}&lt;br&gt;F(x)=%{y:.4f}&lt;extra&gt;&lt;/extra&gt;\"\n  ) %&gt;%\n  layout(\n    title = \"Beta CDF\",\n    xaxis = list(title = \"x\"),\n    yaxis = list(title = \"F(x)\", range = c(0, 1))\n  )\n\nfig &lt;- subplot(pdf_fig, cdf_fig, nrows = 1, shareX = FALSE) %&gt;%\n  layout(\n    title = \"Beta Distribution (interactive α, β sliders)\",\n    showlegend = FALSE,\n    margin = list(l = 80, r = 30, t = 70, b = 50)\n  ) %&gt;%\n  animation_opts(frame = 0, transition = 0, redraw = TRUE) %&gt;%\n  animation_slider(currentvalue = list(prefix = \"\"))\n\nfig",
    "crumbs": [
      "Statistical Distributions",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Univariate Distributions</span>"
    ]
  },
  {
    "objectID": "02_1_univariate.html#r-functions-for-probability-distributions",
    "href": "02_1_univariate.html#r-functions-for-probability-distributions",
    "title": "5  Univariate Distributions",
    "section": "5.3 R Functions for Probability Distributions",
    "text": "5.3 R Functions for Probability Distributions\nR provides a consistent and elegant naming convention for working with probability distributions. For nearly every standard distribution, four core functions are available. These functions allow us to compute probabilities, evaluate densities, calculate quantiles, and generate random samples.\nThe naming structure follows the pattern:\nprefix + distribution\nwhere the prefix determines the type of calculation being performed.\nThe four prefixes are:\n\nd — density or mass function\np — cumulative distribution function\nq — quantile function (inverse CDF)\nr — random number generation\n\nThis systematic structure makes R particularly powerful for simulation work, since generating random variables, evaluating probabilities, and computing theoretical quantities all use parallel syntax.\nFor example, for the Normal distribution:\n\ndnorm() evaluates the density\npnorm() evaluates the CDF\nqnorm() computes quantiles\nrnorm() generates random samples\n\nThis pattern is consistent across nearly all commonly used distributions.\n\n\n\n\n\n\n\n\n\n\nDistribution\nPDF / PMF\nCDF\nQuantile\nRandom Generation\n\n\n\n\nBinomial\ndbinom(x, size, prob)\npbinom()\nqbinom()\nrbinom()\n\n\nGeometric\ndgeom(x, prob)\npgeom()\nqgeom()\nrgeom()\n\n\nNegative Binomial\ndnbinom(x, size, prob, mu)\npnbinom()\nqnbinom()\nrnbnom()\n\n\nPoisson\ndpois(x, lambda)\nppois()\nqpois()\nrpois()\n\n\nMultinomial\ndmultinom(x, size, prob)\n-\n-\nrmultinom()\n\n\nExponential\ndexp()\npexp()\nqexp()\nrexp()\n\n\nUniform\ndunif(x, min, max)\npunif()\nqunif()\nrunif()\n\n\nExponential\ndexp(x, rate)\npexp()\nqexp()\nrexp()\n\n\nGamma\ndgamma()\npgamma()\nqgamma()\nrgamma()\n\n\nNormal\ndnorm(x, mean, sd)\npnorm()\nqnorm()\nrnorm()\n\n\nLog-normal\ndlnorm(x, meanlog, sdlog)\nplnorm()\nqlnorm()\nrlnorm()\n\n\nBeta\ndbeta(x, shape1, shape2)\npbeta()\nqbeta()\nrbeta()\n\n\nt\ndt(x, df)\npt()\nqt()\nrt()\n\n\nChi-square\ndchisq(x, df)\npchisq()\nqchisq()\nrchisq()",
    "crumbs": [
      "Statistical Distributions",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Univariate Distributions</span>"
    ]
  },
  {
    "objectID": "02_2_joint_dependence.html",
    "href": "02_2_joint_dependence.html",
    "title": "6  Joint Distributions and Dependence",
    "section": "",
    "text": "6.1 Why Dependence Matters in Simulation\nIn practical simulation studies, we rarely encounter isolated random variables. Instead, systems consist of multiple random components interacting with one another. A queueing system, for example, involves both interarrival times and service times. A financial model may involve asset returns, interest rates, and volatility. Understanding how random variables behave jointly is therefore fundamental.\nBefore introducing formal definitions, consider the following motivating example.\nTo model such behaviour rigorously, we require the concept of a joint distribution.\nIncorrectly assuming independence can lead to:\nFor example:\nSimulation provides flexibility to incorporate complex dependence structures — but only if we understand the joint distribution.",
    "crumbs": [
      "Statistical Distributions",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Joint Distributions and Dependence</span>"
    ]
  },
  {
    "objectID": "02_2_joint_dependence.html#why-dependence-matters-in-simulation",
    "href": "02_2_joint_dependence.html#why-dependence-matters-in-simulation",
    "title": "6  Joint Distributions and Dependence",
    "section": "",
    "text": "Underestimated variance\nBiased risk estimates\nUnrealistic system behaviour\n\n\n\nIn finance, asset returns are correlated.\nIn queueing systems, service time may depend on customer type.\nIn reliability models, component failures may not be independent.",
    "crumbs": [
      "Statistical Distributions",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Joint Distributions and Dependence</span>"
    ]
  },
  {
    "objectID": "02_2_joint_dependence.html#joint-distributions",
    "href": "02_2_joint_dependence.html#joint-distributions",
    "title": "6  Joint Distributions and Dependence",
    "section": "6.2 Joint Distributions",
    "text": "6.2 Joint Distributions\n\n\n\n\n\n\nFigure 6.1: Joint probability distribution samples (black) and marginal densities (blue and red)\n\n\n\nLet \\(X\\) and \\(Y\\) be two random variables defined on the same probability space. Their behaviour is described by the joint distribution, which specifies probabilities for events involving both variables simultaneously.\nFrom Figure 6.1, the surface is the joint density \\(f_{X,Y}(x,y)\\). The curves along the axes are the marginals \\(f_X(x)\\) and \\(f_Y(y)\\). The scatter cloud shows sample points from the joint distribution.\nA marginal distribution tells you how one variable behaves on its own, regardless of the other.\nAlso, a joint distribution is a multivariate distribution. It is the probability structure that describes how several random variables behave together.\n\nFor two variables \\(X\\) and \\(Y\\), the joint distribution \\(f_{X,Y}(x,y)\\) is a bivariate distribution.\nFor \\(k\\) variables, \\(X_1, \\dots, X_k\\), the joint distribution is \\(f_{X_1,\\dots ,X_k}(x_1,\\dots ,x_k)\\), which is a multivariate distribution.\n\nFor this chapter, we will focus on bivariate distribution.\n\n6.2.1 Discrete Case\n\nIf \\(X\\) and \\(Y\\) are discrete, their joint distribution is given by the joint probability mass function\n\\[p_{X,Y}(x,y) = P(X=x, Y=y),\\]\nwhich satisfies:\n\\[p_{X,Y}(x,y) \\ge 0, \\qquad \\sum_x \\sum_y p_{X,Y}(x,y) = 1.\\]\nFrom the joint distribution, we can recover the individual (marginal) distributions:\n\\[\np_X(x) = \\sum_y p_{X,Y}(x,y), \\qquad\np_Y(y) = \\sum_x p_{X,Y}(x,y).\n\\]\n\n\nExample: Suppose \\(X\\) and \\(Y\\) are discrete random variables with joint probability mass function\n\\[\np_{X,Y}(x,y) = P(X=x, Y=y).\n\\]\nTheir joint pmf is given by the following table:\n\\[\n\\begin{array}{c|ccc}\n& Y=0 & Y=1 & Y=2 \\\\\n\\hline\nX=0 & 0.12 & 0.06 & 0.07 \\\\\nX=1 & 0.10 & 0.15 & 0.05 \\\\\nX=3 & 0.08 & 0.14 & 0.23\n\\end{array}\n\\]\n\nVerify that this is a valid joint PMF\nFind the marginal PMF of \\(X\\)\nFind the marginal PMF of \\(Y\\) \n\n\na. Verify this is a valid joint PMF\n\nAll probabilities are nonnegative.\nThe total probability must sum to 1:\n\n\\[\n\\begin{aligned}\n\\sum_x \\sum_y p_{X,Y}(x,y)\n&= (0.12+0.06+0.07) \\\\\n&\\quad + (0.10+0.15+0.05) \\\\\n&\\quad + (0.08+0.14+0.23) \\\\\n&= 0.25 + 0.30 + 0.45 \\\\\n&= 1.00.\n\\end{aligned}\n\\]\nSo this is a valid joint pmf.\nb. Finding the Marginal Distribution of \\(X\\)\nTo obtain the marginal pmf of \\(X\\), sum across the rows:\n\\[\np_X(x) = \\sum_y p_{X,Y}(x,y).\n\\]\n\nFor \\(x=0\\): \\[\np_X(0) = 0.12 + 0.06 + 0.07 = 0.25\n\\]\nFor \\(x=1\\): \\[\np_X(1) = 0.10 + 0.15 + 0.05 = 0.30\n\\]\nFor \\(x=3\\): \\[\np_X(3) = 0.08 + 0.14 + 0.23 = 0.45\n\\]\n\nTherefore,\n\\[\np_X(x) =\n\\begin{cases}\n0.25, & x=0 \\\\\n0.30, & x=1 \\\\\n0.45, & x=3 \\\\\n0, & \\text{otherwise}.\n\\end{cases}\n\\]\nc. Finding the Marginal Distribution of \\(Y\\)\nTo obtain the marginal pmf of \\(Y\\), sum down the columns:\n\\[\np_Y(y) = \\sum_x p_{X,Y}(x,y).\n\\]\n\nFor \\(y=0\\): \\[\np_Y(0) = 0.12 + 0.10 + 0.08 = 0.30\n\\]\nFor \\(y=1\\): \\[\np_Y(1) = 0.06 + 0.15 + 0.14 = 0.35\n\\]\nFor \\(y=2\\): \\[\np_Y(2) = 0.07 + 0.05 + 0.23 = 0.35\n\\]\n\nTherefore,\n\\[\np_Y(y) =\n\\begin{cases}\n0.30, & y=0 \\\\\n0.35, & y=1 \\\\\n0.35, & y=2 \\\\\n0, & \\text{otherwise}.\n\\end{cases}\n\\]\nCompleted Table with Marginals\n\\[\n\\begin{array}{c|ccc|c}\n& Y=0 & Y=1 & Y=2 & p_X(x) \\\\\n\\hline\nX=0 & 0.12 & 0.06 & 0.07 & 0.25 \\\\\nX=1 & 0.10 & 0.15 & 0.05 & 0.30 \\\\\nX=3 & 0.08 & 0.14 & 0.23 & 0.45 \\\\\n\\hline\np_Y(y) & 0.30 & 0.35 & 0.35 & 1\n\\end{array}\n\\]\n\n\n\n6.2.2 Continuous Case\n\nIf \\(X\\) and \\(Y\\) are continuous, their joint distribution is described by a joint density function\n\\[f_{X,Y}(x,y)\\]\nwith\n\\[\nf(x,y) \\ge 0, \\qquad\n\\int_{-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} f_{X,Y}(x,y)\\,dx\\,dy = 1.\n\\]\nThe marginal densities are obtained by integration:\n\\[\nf_X(x) = \\int f_{X,Y}(x,y)\\,dy, \\qquad\nf_Y(y) = \\int f_{X,Y}(x,y)\\,dx.\n\\]\nThe joint cumulative distribution function is\n\\[\nF_{X,Y}(x,y) = P(X \\le x, Y \\le y).\n\\]\n\n\nExample: Suppose \\(X\\) and \\(Y\\) have joint density\n\\[\nf_{X,Y}(x,y) =\n\\begin{cases}\n2, & 0 &lt; y &lt; x &lt; 1, \\\\\n0, & \\text{otherwise}.\n\\end{cases}\n\\]\nThis means the density is constant over the triangular region\n\\[\n0 &lt; y &lt; x &lt; 1.\n\\]\n\nVerify that this is a valid joint density\nFind the marginal desity of \\(X\\)\nFind the marginal density of \\(Y\\) \n\n\na. Verify this is a valid joint density\nWe must check:\n\n\\(f(x,y) \\ge 0\\).\nThe total integral equals 1.\n\nThe support is the triangular region \\(0&lt;y&lt;x&lt;1\\), so\n\\[\n\\int_0^1 \\int_0^x 2 \\, dy \\, dx.\n\\]\nCompute the inner integral:\n\\[\n\\int_0^x 2\\,dy = 2x.\n\\]\nNow integrate with respect to \\(x\\):\n\\[\n\\int_0^1 2x\\,dx\n=\n\\left[x^2\\right]_0^1\n=\n1.\n\\]\nSo this is a valid joint density.\nb. Marginal Density of \\(X\\)\nMarginals are obtained by integrating over slices of triangular region.\nWe integrate out \\(y\\):\n\\[\nf_X(x)\n=\n\\int f(x,y)\\,dy.\n\\]\nSince \\(0&lt;y&lt;x&lt;1\\),\n\\[\nf_X(x)\n=\n\\int_0^x 2\\,dy\n=\n2x,\n\\qquad 0&lt;x&lt;1.\n\\]\nThus,\n\\[\nf_X(x)\n=\n\\begin{cases}\n2x, & 0&lt;x&lt;1, \\\\\n0, & \\text{otherwise}.\n\\end{cases}\n\\]\nc. Marginal Density of \\(Y\\)\nNow integrate out \\(x\\).\nFrom the triangular region, for fixed \\(y\\) we have \\(y&lt;x&lt;1\\).\nSo,\n\\[\nf_Y(y)\n=\n\\int_y^1 2\\,dx.\n\\]\nCompute:\n\\[\nf_Y(y)\n=\n2(1-y),\n\\qquad 0&lt;y&lt;1.\n\\]\nThus,\n\\[\nf_Y(y)\n=\n\\begin{cases}\n2(1-y), & 0&lt;y&lt;1, \\\\\n0, & \\text{otherwise}.\n\\end{cases}\n\\]\n\n\n\n6.2.3 Mixed Case\n\nIn practice, random variables need not both be discrete or both continuous. A common situation in simulation is that one variable is discrete while another is continuous.\nSuppose:\n\\[X \\in \\{1,2,\\dots,k\\}, \\qquad Y \\text{ continuous}.\\]\nThen the joint distribution is described by:\n\\[P(X=x) \\quad \\text{and} \\quad f_{Y|X}(y \\mid x).\\]\nThe joint structure is\n\\[\nf_{X,Y}(x,y)\n=\nP(X=x)\\,f_{Y|X}(y \\mid x).\n\\]\n\nThis representation is especially important in simulation because it suggests a natural algorithm:\n\nGenerate \\(X\\).\nGenerate \\(Y\\) from \\(f_{Y|X}(\\cdot \\mid X)\\).\n\nThis is the basic structure of mixture models and hierarchical simulation.\n\nExample: Mixed Distribution (Discrete + Continuous)\nSuppose\n\\[\nX \\in \\{1,2\\},\n\\qquad\nY \\text{ is continuous}.\n\\]\nAssume:\n\\[\nP(X=1)=0.4,\n\\qquad\nP(X=2)=0.6.\n\\]\nConditional on \\(X=x\\), suppose \\(Y\\) has the density\n\\[\nf_{Y \\mid X}(y \\mid x)\n=\n\\begin{cases}\nx e^{-x y}, & y&gt;0, \\\\\n0, & \\text{otherwise}.\n\\end{cases}\n\\]\nSo:\n\nIf \\(X=1\\), then \\(Y \\sim \\text{Exponential}(1)\\).\nIf \\(X=2\\), then \\(Y \\sim \\text{Exponential}(2)\\).\n\n\nDetermine the joint distribution structure\nVerify that this is a valid joint distribution\nFind the marginal density of \\(Y\\)\nFind the marginal probability of \\(X\\) \n\n\na. Joint Distribution\nFor mixed distributions,\n\\[\nf_{X,Y}(x,y)\n=\nP(X=x)\\, f_{Y \\mid X}(y \\mid x).\n\\]\nThus:\nFor \\(X=1\\):\n\\[\nf_{X,Y}(1,y)\n=\n0.4 \\cdot (1)e^{-y}\n=\n0.4 e^{-y},\n\\qquad y&gt;0.\n\\]\nFor \\(X=2\\):\n\\[\nf_{X,Y}(2,y)\n=\n0.6 \\cdot (2)e^{-2y}\n=\n1.2 e^{-2y},\n\\qquad y&gt;0.\n\\]\nSo the joint structure is\n\\[\nf_{X,Y}(x,y)\n=\n\\begin{cases}\n0.4 e^{-y}, & x=1,\\, y&gt;0, \\\\\n1.2 e^{-2y}, & x=2,\\, y&gt;0, \\\\\n0, & \\text{otherwise}.\n\\end{cases}\n\\]\nb. Verify Total Probability = 1\nWe must check:\n\\[\n\\sum_x \\int_0^\\infty f_{X,Y}(x,y)\\,dy = 1.\n\\]\nCompute:\n\\[\n\\int_0^\\infty 0.4 e^{-y}\\,dy = 0.4,\n\\]\n\\[\n\\int_0^\\infty 1.2 e^{-2y}\\,dy\n=\n1.2 \\cdot \\frac{1}{2}\n=\n0.6.\n\\]\nTherefore,\n\\[\n0.4 + 0.6 = 1.\n\\]\nSo this is a valid joint distribution.\nc. Marginal Density of \\(Y\\)\nWe sum over the discrete values of \\(X\\):\n\\[\nf_Y(y)\n=\n\\sum_x f_{X,Y}(x,y).\n\\]\nSo,\n\\[\nf_Y(y)\n=\n0.4 e^{-y}\n+\n1.2 e^{-2y},\n\\qquad y&gt;0.\n\\]\nThis is a mixture of exponential densities.\nd. Recovering \\(P(X=x)\\) from the Joint\nFor mixed distributions,\n\\[\nP(X=x)\n=\n\\int_{-\\infty}^{\\infty} f_{X,Y}(x,y)\\,dy.\n\\]\nWe already computed:\n\\[\nP(X=1)=0.4,\n\\qquad\nP(X=2)=0.6.\n\\]",
    "crumbs": [
      "Statistical Distributions",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Joint Distributions and Dependence</span>"
    ]
  },
  {
    "objectID": "02_2_joint_dependence.html#independence",
    "href": "02_2_joint_dependence.html#independence",
    "title": "6  Joint Distributions and Dependence",
    "section": "6.3 Independence",
    "text": "6.3 Independence\nUp to this point, we have described the most general form of joint behaviour. The joint distribution \\(p_{X,Y}(x,y)\\) or \\(f_{X,Y}(x,y)\\) allows arbitrary dependence between \\(X\\) and \\(Y\\). In principle, the variables may influence one another in any possible way.\nHowever, among all possible joint distributions, one case is particularly important: the case in which the variables do not influence each other at all. This leads to the concept of independence.\n\nExample: Return to the coffee shop model. Suppose we now assume\n\nCustomer type \\(X\\) is determined before entering the shop.\nService time \\(Y\\) depends only on the drink ordered, not on how long it has been since the previous customer arrived.\n\nIn this scenario, knowing the interarrival time gives us no information about the service time. Observing one variable does not change our beliefs about the other.\n\nThis intuitive idea — that learning one value tells us nothing about the other — is formalised mathematically as independence.\n\nTwo random variables are independent if knowledge of one provides no information about the other.\n\n\nFormally, \\(X\\) and \\(Y\\) are independent if\nDiscrete Case\n\\[p_{X,Y}(x,y) = p_X(x)p_Y(y)\\]\nContinuous Case\n\\[f_{X,Y}(x,y) = f_X(x)f_Y(y)\\]\nfor all \\(x,y\\).\n\nIndependence simplifies simulation dramatically. If variables are independent, we may generate them separately. However, many real-world systems exhibit dependence, and modelling this dependence correctly is crucial.\n\nDiscrete Case Example\nLet \\(X\\) and \\(Y\\) be two discrete independent random variables:\n\n\\(X\\sim \\mathrm{Bernoulli}(0.5)\\)\n\\(Y\\sim \\mathrm{Bernoulli}(0.3)\\)\n\nBecause \\(X\\) and \\(Y\\) are independent, we simulate them separately.\n\n\nset.seed(123)\nn &lt;- 5000\nX &lt;- rbinom(n, 1, 0.5)\nY &lt;- rbinom(n, 1, 0.3)\ncor(X, Y)   # close to 0\n\n[1] -0.01700378\n\n\n\nContinuous Case Example\nLet X and Y be continuous independent random variables:\n\n\\(X\\sim \\mathcal{N}(0,1)\\)\n\\(Y\\sim \\mathcal{N}(5,4)\\)\n\nBecause of independence:\n\\[f_{X,Y}(x,y)=f_X(x)\\, f_Y(y)=\\frac{1}{\\sqrt{2\\pi }}e^{-x^2/2}\\times \\frac{1}{\\sqrt{8\\pi }}e^{-(y-5)^2/8}\\]\nThe joint density is a product of two unrelated bell curves.\n\n\nset.seed(123)\nn &lt;- 5000\nX &lt;- rnorm(n, 0, 1)\nY &lt;- rnorm(n, 5, 2)\ncor(X, Y)   # close to 0\n\n[1] -0.00598005\n\nplot(X, Y, pch=16, cex=0.4)\n\n\n\n\n\n\n\n\nThe scatterplot looks like a round cloud, with no directional trend.",
    "crumbs": [
      "Statistical Distributions",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Joint Distributions and Dependence</span>"
    ]
  },
  {
    "objectID": "02_2_joint_dependence.html#conditional-distributions",
    "href": "02_2_joint_dependence.html#conditional-distributions",
    "title": "6  Joint Distributions and Dependence",
    "section": "6.4 Conditional Distributions",
    "text": "6.4 Conditional Distributions\nIndependence describes the simplest possible joint structure: learning the value of one variable does not change the distribution of the other.\nBut most real systems are not independent.\nIn practice, we often observe one variable first and then ask:\n\nHow does this information affect the behaviour of the other variable?\n\nThis question leads naturally to the concept of a conditional distribution.\n\nExample: Return once more to the coffee shop example. Suppose we observe that the interarrival time \\(X\\) is very small (customers are arriving rapidly).\nWould we still expect the same distribution of service times \\(Y\\)?\nPerhaps not. During busy periods, baristas may rush, producing shorter service times on average. Thus the distribution of \\(Y\\) may depend on the observed value of \\(X\\).\nIn this case:\n\\[\\text{Distribution of } Y \\mid X=x\\]\nis different for different values of \\(x\\).\nDependence is formalised through conditional distributions.\n\n\nConditional probability describes how the probability of one event changes when we are given that another event has occurred.\nLet \\(A\\) and \\(B\\) be two events in a sample space \\(\\Omega\\), with \\(P(B)&gt;0\\). The conditional probability of \\(A\\) given \\(B\\) is defined as\n\\[\nP(A \\mid B) = \\frac{P(A \\cap B)}{P(B)}.\n\\]\nAs an axiom of probability, we have\n\\[\nP(A \\cap B) = P(A \\mid B) P(B).\n\\]\nDiscrete Case\nLet \\(A = \\{X=x\\}\\) and \\(B = \\{Y=y\\}\\),\n\\[\n\\begin{align}\nP(X=x \\mid Y=y) &= \\frac{P(X=x,Y=y)}{P(Y=y)} \\\\\n                &= \\frac{p_{X,Y}(x,y)}{p_Y(y)}, \\quad p_Y(y)&gt;0\n\\end{align}\n\\]\nContinuous Case\nIn continuous distributions, we replace probabilities with densities, so\n\\[f_{X|Y}(x \\mid y) = \\frac{f_{X,Y}(x,y)}{f_Y(y)}, \\quad f_Y(y)&gt;0\\]\nA conditional distribution is obtained by taking the joint distribution and dividing by the corresponding marginal distribution.\n\nRecall that independence was defined by\n\\[\nf_{X,Y}(x,y) = f_X(x) f_Y(y).\n\\]\nRewriting this gives\n\\[\nf_{Y|X}(y \\mid x)\n=\n\\frac{f_{X,Y}(x,y)}{f_X(x)}.\n\\]\nIf independence holds, then\n\\[\nf_{Y|X}(y \\mid x)\n=\nf_Y(y).\n\\]\nThus:\n\nIndependence is exactly the situation in which conditioning has no effect.\n\nThis observation is crucial. It shows that conditional distributions are not a new idea unrelated to independence — rather, they generalise it.\n\nIn the independent case, conditioning changes nothing.\nIn the dependent case, conditioning alters the distribution.\n\nThis transition is particularly important in simulation because:\n\nIf variables are independent, we can simulate them separately.\nIf they are dependent, we often simulate sequentially:\n\nGenerate \\(X\\),\nGenerate \\(Y\\) from \\(f_{Y|X}(\\cdot \\mid X)\\).\n\n\n\nDiscrete Case Example\n\nSuppose a small call center with two types of customers:\n\nType A customers are quick to serve.\nType B customers take longer.\n\nLet \\(X\\) be customer type, where \\(X=0\\) and \\(X=1\\) denote customer Type A and Type B, respectively. Let \\(Y\\) be number of support tickets resolved in the next hour.\n\nIf the customer is Type A, agents resolve more tickets (higher success probability).\nIf the customer is Type B, agents resolve fewer tickets (lower success probability).\n\nWe model:\n\\[\nY\\mid X=0\\sim \\mathrm{Binomial}(n=10,p=0.7)\n\\]\n\\[\nY\\mid X=1\\sim \\mathrm{Binomial}(n=10,p=0.4)\n\\]\nThis creates dependence between \\(X\\) and \\(Y\\). Even if we know the marginal distribution of \\(X\\) and the marginal distribution of \\(Y\\), we cannot simulate the system correctly without the conditional structure.\n\nset.seed(123)\n\nn &lt;- 5000\n\n# Customer type: 0 = A, 1 = B\nX &lt;- rbinom(n, size = 1, prob = 0.4)   # 40% Type B customers\n\n# Conditional distribution for Y\nY &lt;- ifelse(\n  X == 0,\n  rbinom(n, size = 10, prob = 0.7),    # Type A\n  rbinom(n, size = 10, prob = 0.4)     # Type B\n)\n\ncor(X, Y)\n\n[1] -0.7032966\n\n\n\n\nCode\n# Conditional PMFs\ny &lt;- 0:10\npmf_A &lt;- dbinom(y, size = 10, prob = 0.7)   # X = 0 (Type A)\npmf_B &lt;- dbinom(y, size = 10, prob = 0.4)   # X = 1 (Type B)\n\n# Set up side-by-side plotting area\npar(mfrow = c(2, 1), mar = c(4, 4, 3, 1))\n\n# PMF for X = 0\nbarplot(\n  pmf_A,\n  names.arg = y,\n  col = \"steelblue\",\n  main = \"P(Y | X = 0)\",\n  xlab = \"Y\",\n  ylab = \"Probability\",\n  ylim = c(0, max(pmf_A, pmf_B))\n)\n\n# PMF for X = 1\nbarplot(\n  pmf_B,\n  names.arg = y,\n  col = \"firebrick\",\n  main = \"P(Y | X = 1)\",\n  xlab = \"Y\",\n  ylab = \"Probability\",\n  ylim = c(0, max(pmf_A, pmf_B))\n)\n\n\n\n\n\n\n\n\n\nThe histograms show the conditional distributions of \\(Y\\) for the two customer types. When \\(X=0\\) (Type A), the bars are concentrated at higher values of Y, meaning agents resolve more tickets on average. When \\(X=1\\) (Type B), the distribution shifts downward, with higher probability on smaller values of \\(Y\\). The contrast between the two panels makes the dependence obvious: the value of \\(X\\) changes the entire shape of the distribution of \\(Y\\), not just its mean.\n\nContinuous Case Example\n\nSuppose interarrival time \\(X\\) of a coffee shop influences service time \\(Y\\), such that\n\\[Y = 2 + 0.5X + \\varepsilon,\\]\nwhere\n\\[\nX \\sim \\text{Exp}(1),\n\\quad\n\\varepsilon \\sim \\mathcal{N}(0, 0.2^2),\n\\]\nand \\(X\\) and \\(\\varepsilon\\) are independent.\nHere:\n\nWhen customers arrive slowly (large \\(X\\)), baristas work more carefully.\nWhen arrivals are rapid (small \\(X\\)), service is faster.\n\nThis creates dependence between \\(X\\) and \\(Y\\).\n\nset.seed(123)\n\nn &lt;- 5000\nx &lt;- rexp(n, rate = 1)\neps &lt;- rnorm(n, mean = 0, sd = 0.2)\ny &lt;- 2 + 0.5*x + eps\n\ncor(x, y)\n\n[1] 0.9287826\n\nplot(x, y, pch=16, cex=0.4)\n\n\n\n\n\n\n\n\nThe scatterplot reveals a clear linear trend, and the correlation is positive.\nEven if we know the distribution of \\(X\\) and the distribution of \\(Y\\), without their joint structure we cannot simulate the system correctly.",
    "crumbs": [
      "Statistical Distributions",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Joint Distributions and Dependence</span>"
    ]
  },
  {
    "objectID": "02_2_joint_dependence.html#joint-vs-conditional-distribution",
    "href": "02_2_joint_dependence.html#joint-vs-conditional-distribution",
    "title": "6  Joint Distributions and Dependence",
    "section": "6.5 Joint vs Conditional Distribution",
    "text": "6.5 Joint vs Conditional Distribution\nLet \\(X\\) and \\(Y\\) be random variables.\n\n\n\n\n\n\n\n\n\nJoint\nConditional\n\n\n\n\nDiscrete\n\\(p_{X,Y}(x,y)\\)\n\\(p_{X|Y}(x \\mid y)=\\dfrac{p_{X,Y}(x,y)}{p_Y(y)}\\)\n\n\nContinuous\n\\(f_{X,Y}(x,y)\\)\n\\(f_{X|Y}(x \\mid y)=\\dfrac{f_{X,Y}(x,y)}{f_Y(y)}\\)\n\n\nIt describes\nHow \\(X\\) and \\(Y\\) behave together (full dependence structure).\nDistribution of \\(X\\) given fixed \\(Y=y\\) (a “slice” of the joint).\n\n\nSimulation perspective\nEncodes full dependence structure between variables.\nUsed for stepwise simulation: simulate one variable, then simulate the other conditionally.\n\n\nRelationship\nMarginals and conditionals are derived from the joint.\nDerived from the joint distribution.",
    "crumbs": [
      "Statistical Distributions",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Joint Distributions and Dependence</span>"
    ]
  },
  {
    "objectID": "02_2_joint_dependence.html#bayes-rule",
    "href": "02_2_joint_dependence.html#bayes-rule",
    "title": "6  Joint Distributions and Dependence",
    "section": "6.6 Bayes’ Rule",
    "text": "6.6 Bayes’ Rule",
    "crumbs": [
      "Statistical Distributions",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Joint Distributions and Dependence</span>"
    ]
  },
  {
    "objectID": "02_2_joint_dependence.html#covariance-and-correlation",
    "href": "02_2_joint_dependence.html#covariance-and-correlation",
    "title": "6  Joint Distributions and Dependence",
    "section": "6.7 Covariance and Correlation",
    "text": "6.7 Covariance and Correlation\n\nTo quantify linear dependence, we define the covariance:\n\\[\\mathrm{Cov}(X,Y)\n=\n\\mathbb{E}\\left[(X-\\mathbb{E}[X])(Y-\\mathbb{E}[Y])\\right].\\]\nAn equivalent expression is\n\\[\\mathrm{Cov}(X,Y)\n=\n\\mathbb{E}[XY] - \\mathbb{E}[X]\\mathbb{E}[Y].\\]\nIf \\(X\\) and \\(Y\\) are independent, then\n\\[\\mathrm{Cov}(X,Y) = 0.\\]\nThe correlation coefficient is\n\\[\\rho = \\frac{\\mathrm{Cov}(X,Y)}{\\sigma_X \\sigma_Y},\\]\nwhich lies in \\([-1,1]\\).\nCorrelation measures linear association but does not fully describe dependence. Two variables may be uncorrelated yet dependent — an important subtlety in simulation modelling.\n\n\n\nThe sign of the covariance of two random variables X and Y\nset.seed(123)\n\npar(mfrow = c(1, 3), mar = c(4, 4, 3, 1))\n\n### 1. Negative covariance\nx1 &lt;- rnorm(500)\ny1 &lt;- -0.8 * x1 + rnorm(500, sd = 0.5)\nplot(x1, y1,\n     pch = 16, cex = 0.5, col = \"steelblue\",\n     main = paste(\"Cov &lt; 0\\n\", round(cov(x1, y1), 2)),\n     xlab = \"X\", ylab = \"Y\")\n\n### 2. Approximately zero covariance\nx2 &lt;- rnorm(500)\ny2 &lt;- rnorm(500)\nplot(x2, y2,\n     pch = 16, cex = 0.5, col = \"darkgray\",\n     main = paste(\"Cov ≈ 0\\n\", round(cov(x2, y2), 2)),\n     xlab = \"X\", ylab = \"Y\")\n\n### 3. Positive covariance\nx3 &lt;- rnorm(500)\ny3 &lt;- 0.8 * x3 + rnorm(500, sd = 0.5)\nplot(x3, y3,\n     pch = 16, cex = 0.5, col = \"firebrick\",\n     main = paste(\"Cov &gt; 0\\n\", round(cov(x3, y3), 2)),\n     xlab = \"X\", ylab = \"Y\")\n\n\n\n\n\n\n\n\n\n\n\\(Cov(X, Y) &lt; 0\\) shows a clear downward trend. As X increases, Y tends to decrease.\n\\(Cov(X, Y) ≈ 0\\) shows a round, structureless cloud — classic independence‑looking scatter.\n\\(Cov(X, Y) &gt; 0\\) shows a clear upward trend. As X increases, Y tends to increase.\n\n\n\nThe sign of the correlation of two random variables X and Y\nset.seed(123)\n\npar(mfrow = c(1, 3), mar = c(4, 4, 3, 1))\n\n### 1. Negative correlation\nx1 &lt;- rnorm(500)\ny1 &lt;- -0.8 * x1 + rnorm(500, sd = 0.5)\nplot(x1, y1,\n     pch = 16, cex = 0.5, col = \"steelblue\",\n     main = paste(\"Cor &lt; 0\\n\", round(cor(x1, y1), 2)),\n     xlab = \"X\", ylab = \"Y\")\n\n### 2. Approximately zero correlation\nx2 &lt;- rnorm(500)\ny2 &lt;- rnorm(500)\nplot(x2, y2,\n     pch = 16, cex = 0.5, col = \"darkgray\",\n     main = paste(\"Cor ≈ 0\\n\", round(cor(x2, y2), 2)),\n     xlab = \"X\", ylab = \"Y\")\n\n### 3. Positive correlation\nx3 &lt;- rnorm(500)\ny3 &lt;- 0.8 * x3 + rnorm(500, sd = 0.5)\nplot(x3, y3,\n     pch = 16, cex = 0.5, col = \"firebrick\",\n     main = paste(\"Cor &gt; 0\\n\", round(cor(x3, y3), 2)),\n     xlab = \"X\", ylab = \"Y\")\n\n\n\n\n\n\n\n\n\nWhen to use covariance\nCovariance is useful when you care about the direction of the relationship.\nCovariance answers:\n\n“Do X and Y increase together or move in opposite directions?”\n\nBut the magnitude is not interpretable, because it depends on the units of X and Y.\nWhen to use correlation\nCorrelation is the standardised version of covariance. It removes units and rescales the relationship to the familiar range [-1,1].\nCorrelation answers:\n\n“How strong is the linear relationship between X and Y, on a universal scale?”\n\nBecause it’s standardised, you can compare height vs weight, income vs education, temperature vs electricity use, etc, even though all are in different units.",
    "crumbs": [
      "Statistical Distributions",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Joint Distributions and Dependence</span>"
    ]
  },
  {
    "objectID": "02_2_joint_dependence.html#bivariate-normal-distribution",
    "href": "02_2_joint_dependence.html#bivariate-normal-distribution",
    "title": "6  Joint Distributions and Dependence",
    "section": "6.8 Bivariate Normal Distribution",
    "text": "6.8 Bivariate Normal Distribution\nA central example in simulation is the bivariate normal distribution. A bivariate normal distribution is a two‑dimensional version of the normal distribution. It describes the joint behaviour of two continuous random variables, usually written as:\n\\[(X,Y)\\sim \\mathrm{BVN}(\\mu _X,\\mu _Y,\\sigma _X^2,\\sigma _Y^2,\\rho )\\]\nor\n\\[\\begin{pmatrix}\nX \\\\\nY\n\\end{pmatrix}\n\\sim\n\\mathcal{N}\n\\left(\n\\begin{pmatrix}\n\\mu_X \\\\\n\\mu_Y\n\\end{pmatrix},\n\\begin{pmatrix}\n\\sigma_X^2 & \\rho \\sigma_X \\sigma_Y \\\\\n\\rho \\sigma_X \\sigma_Y & \\sigma_Y^2\n\\end{pmatrix}\n\\right)\\]\nIt is fully determined by five parameters:\n\nmean of \\(X\\): \\(\\mu _X\\)\nmean of \\(Y\\): \\(\\mu _Y\\)\nvariance of \\(X\\): \\(\\sigma _X^2\\)\nvariance of \\(Y\\): \\(\\sigma _Y^2\\)\ncorrelation between \\(X\\) and \\(Y\\): \\(\\rho\\)\n\nHere, the parameter \\(\\rho\\) controls linear dependence.\nKey properties\n\nEach marginal is normal\n\n\\[X\\sim \\mathcal{N}(\\mu _X,\\sigma _X^2),\\qquad Y\\sim \\mathcal{N}(\\mu _Y,\\sigma _Y^2)\\]\n\nThe joint density forms a 3D bell surface The height of the surface at point \\((x,y)\\) is the joint density \\(f_{X,Y}(x,y)\\). The shape of this surface depends heavily on the correlation \\(\\rho\\):\n\n\\(\\rho &gt;0\\): ridge tilts upward\n\\(\\rho &lt;0\\): ridge tilts downward\n\\(\\rho =0\\): symmetric “hill” with circular contours\n\nContours are ellipses. If you slice the 3D surface horizontally, you get ellipses. The orientation of the ellipse tells you the sign of the correlation.\nIndependence happens only when \\(\\rho =0\\). This is not true for most distributions. It’s a unique property of the multivariate normal family.\n\n\n\n3D bivariate normal surfaces for different correlations\nlibrary(mvtnorm)\n\n# Grid for evaluation\nx &lt;- seq(-3, 3, length = 50)\ny &lt;- seq(-3, 3, length = 50)\ngrid &lt;- expand.grid(x = x, y = y)\n\n# Function to compute BVN density matrix\nbvn_matrix &lt;- function(rho) {\n  Sigma &lt;- matrix(c(1, rho, rho, 1), 2, 2)\n  z &lt;- dmvnorm(grid, mean = c(0, 0), sigma = Sigma)\n  matrix(z, nrow = length(x), ncol = length(y))\n}\n\nz_neg &lt;- bvn_matrix(-0.8)\nz_zero &lt;- bvn_matrix(0)\nz_pos &lt;- bvn_matrix(0.8)\n\npar(mfrow = c(1, 3), mar = c(2, 2, 3, 1))\n\n# 1. Negative correlation\npersp(x, y, z_neg,\n      theta = 30, phi = 25,\n      col = \"lightblue\",\n      main = \"ρ = -0.8\",\n      xlab = \"X\", ylab = \"Y\", zlab = \"f(x,y)\")\n\n# 2. Zero correlation\npersp(x, y, z_zero,\n      theta = 30, phi = 25,\n      col = \"lightgray\",\n      main = \"ρ = 0\",\n      xlab = \"X\", ylab = \"Y\", zlab = \"f(x,y)\")\n\n# 3. Positive correlation\npersp(x, y, z_pos,\n      theta = 30, phi = 25,\n      col = \"salmon\",\n      main = \"ρ = 0.8\",\n      xlab = \"X\", ylab = \"Y\", zlab = \"f(x,y)\")\n\n\n\n\n\n\n\n\n\n\nFor \\(\\rho=-0.8\\) (negative correlation), the surface leans downward along the diagonal. The ridge slopes from top‑left to bottom‑right.\nFor \\(\\rho = 0\\) (no correlation), the surface shows a perfectly symmetric hill. Contours would be circles; no tilt in any direction.\nFor \\(\\rho = 0.8\\) (positive correlation), the surface leans upward along the diagonal. The ridge slopes from bottom‑left to top‑right.\n\nWe can simulate correlated normals using a linear transformation.\n\nset.seed(123)\n\nn &lt;- 5000\nrho &lt;- 0.8\n\nz1 &lt;- rnorm(n)\nz2 &lt;- rnorm(n)\n\nx &lt;- z1\ny &lt;- rho*z1 + sqrt(1 - rho^2)*z2\n\ncor(x, y)\n\n[1] 0.7963105\n\n\n\n# Define parameters\nrho &lt;- 0.7\n\n# Create grid\nx &lt;- seq(-3, 3, length = 100)\ny &lt;- seq(-3, 3, length = 100)\ngrid &lt;- expand.grid(x = x, y = y)\n\n# Bivariate normal density (mean=0, var=1)\nf &lt;- function(x, y, rho) {\n  1/(2*pi*sqrt(1 - rho^2)) *\n    exp(-(x^2 - 2*rho*x*y + y^2) / (2*(1 - rho^2)))\n}\n\n# Compute density values\nz &lt;- matrix(f(grid$x, grid$y, rho), nrow = 100)\n\n# 3D perspective plot\npersp(x, y, z,\n      theta = 30, phi = 30,\n      expand = 0.5,\n      col = \"lightblue\",\n      xlab = \"X\",\n      ylab = \"Y\",\n      zlab = \"Density\",\n      ticktype = \"detailed\")\n\n\n\n\n\n\n\n\nThis construction will later connect to the transformation methods we study in the following section.",
    "crumbs": [
      "Statistical Distributions",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Joint Distributions and Dependence</span>"
    ]
  },
  {
    "objectID": "02_3_transformation_rvs.html",
    "href": "02_3_transformation_rvs.html",
    "title": "7  Transformation of Random Variables",
    "section": "",
    "text": "7.1 Transformation of a Single Random Variable\nUp to this point, we have focused on how random variables behave together—their joint distributions, dependence structures, and how correlation shapes the geometry of the bivariate normal. In the final example, we saw that we can construct correlated normals by applying a linear transformation to independent ones. This idea is much more general than it first appears.\nIn this section, we shift our attention to how random variables change when we transform them. Whether we apply a simple function to a single variable or a linear transformation to a vector of variables, the distribution changes in systematic, mathematically predictable ways. Understanding these transformation rules is essential for simulation, modelling, and deriving new distributions from known ones.\nWhen we apply a function to a random variable, the result is itself a new random variable. If \\(X\\) has a known distribution and we define\n\\[\nY = g(X),\n\\]\nthen the key question is:\nThis section develops the tools needed to answer that question for both discrete and continuous cases.",
    "crumbs": [
      "Statistical Distributions",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Transformation of Random Variables</span>"
    ]
  },
  {
    "objectID": "02_3_transformation_rvs.html#transformation-of-a-single-random-variable",
    "href": "02_3_transformation_rvs.html#transformation-of-a-single-random-variable",
    "title": "7  Transformation of Random Variables",
    "section": "",
    "text": "What is the distribution of \\(Y\\)?\n\n\n\n7.1.1 Discrete Case\n\nIf \\(X\\) is discrete with PMF \\(p_X(x)\\), and \\(Y = g(X)\\), then the PMF of \\(Y\\) is obtained by collecting all values of \\(X\\) that map to the same value of \\(Y\\):\n\\[\np_Y(y) = \\sum_{x : g(x) = y} p_X(x).\n\\]\nThis is simply a “probability bookkeeping” rule: every value of \\(Y\\) inherits probability from the \\(X\\)-values that produce it.\n\nExample: Let \\(X\\) take values \\(\\{ 1,2,3,4\\}\\) with \\[\nP(X=1)=0.1,\\quad P(X=2)=0.2,\n\\]\n\\[\nP(X=3)=0.3,\\quad P(X=4)=0.4.\n\\]\nDefine\n\\[\nY=g(X)=\\left\\{ \\, \\begin{array}{ll}\\textstyle 0,&\\textstyle X\\mathrm{\\  is\\  odd}\\\\ \\textstyle 1,&\\textstyle X\\mathrm{\\  is\\  even}\\end{array}\\right.\n\\]\nso \\(Y\\) indicates whether \\(X\\) is even.\nDetermine the PMF of \\(Y\\) with possible values of 0 and 1.\n\nFor \\(Y=0\\): this happens when \\(X\\) is odd, i.e. \\(X\\in \\{ 1,3\\}\\).\n\n\\[\nP(Y=0)=P(X=1)+P(X=3)=0.1+0.3=0.4.\n\\]\n\nFor \\(Y=1\\): this happens when \\(X\\) is even, i.e. \\(X\\in \\{ 2,4\\}\\).\n\n\\[\nP(Y=1)=P(X=2)+P(X=4)=0.2+0.4=0.6.\n\\]\nSo the PMF of \\(Y\\) is\n\\[\nP(Y=0)=0.4,\\qquad P(Y=1)=0.6.\n\\]\nWe can try simulating the example:\n\nset.seed(123)\n\n# Original discrete variable\nX &lt;- sample(1:4, size = 5000, replace = TRUE,\n            prob = c(0.1, 0.2, 0.3, 0.4))\n\n# Transformation\nY &lt;- ifelse(X %% 2 == 1, 0, 1)\n\n# Plot empirical pmf of Y\nbarplot(prop.table(table(Y)),\n        col = c(\"skyblue\", \"tomato\"),\n        main = \"Empirical PMF of Y = g(X)\",\n        ylab = \"Probability\")\n\n\n\n\n\n\n\n\nThe heights close to the theoretical values of 0.4 and 0.6.\n\n\n7.1.2 Continuous Case\nThe method used for transforming discrete random variables cannot be applied directly to the continuous case because probabilities at individual points no longer make sense: for a continuous random variable, \\(P(X=x)=0\\) for all \\(x\\). As a result, we cannot “collect probabilities” like the previous section; instead, we must work with how probability density is redistributed under a transformation.\nFor continuous random variables, there are in principle two approaches: a CDF‑based method, which derives the distribution of \\(Y\\) by working with cumulative probabilities, and a PDF‑based method, which determines how density changes under the transformation. In this subsection, we focus on the PDF‑based method, as it provides a unified and transparent way to handle both monotone and non‑monotone transformations.\nWhen the transformation is monotone, each value of \\(Y\\) corresponds to exactly one value of \\(X\\), so the density of \\(Y\\) is obtained from a single contribution. In contrast, for non‑monotone transformations, multiple values of \\(X\\) may map to the same value of \\(Y\\), and the density of \\(Y\\) must account for all such contributions. This perspective emphasises conservation of probability and naturally extends to more complex transformations, including multivariate cases.\n\n\n\n\n\nflowchart LR\n  %% Monotone transformation\n  subgraph M[\"Monotone Y = g(X)\"]\n    direction TB\n    x1[\"x₁\"] --&gt;|\"g\"| y1[\"y₁\"]\n    x2[\"x₂\"] --&gt;|\"g\"| y2[\"y₂\"]\n    x3[\"x₃\"] --&gt;|\"g\"| y3[\"y₃\"]\n  end\n\n  %% Non-monotone transformation\n  subgraph N[\"Non‑monotone Y = g(X)\"]\n    direction TB\n    a1[\"x₁\"] --&gt;|\"g\"| b1[\"y₁\"]\n    a2[\"x₂\"] --&gt;|\"g\"| b2[\"y₂\"]\n    a3[\"x₃\"] --&gt;|\"g\"| b2\n    a4[\"x₄\"] --&gt;|\"g\"| b3[\"y₃\"]\n  end\n\n  %% Notes\n  note1[\"Each y has exactly one preimage\"]:::note\n  note2[\"A y may have multiple preimages\"]:::note\n\n  M --- note1\n  N --- note2\n\n  %% Styling\n  classDef note fill:#f5f5f5,stroke:#999,stroke-dasharray: 4 4,color:#111;\n\n  style M fill:#f5f5f5,stroke:#999,stroke-dasharray: 4 4,color:#111;\n  style N fill:#f5f5f5,stroke:#999,stroke-dasharray: 4 4,color:#111;\n\n  %% Make all arrows black\n  linkStyle default stroke:#000,stroke-width:1.5px;\n\n\n\n\n\n\n\n7.1.2.1 Monotone Transformations\n\nIf \\(X\\) is continuous with PDF \\(f_X(x)\\) and \\(Y = g(X)\\) where \\(g\\) is strictly increasing or decreasing, then the PDF of \\(Y\\) is given by the change‑of‑variables formula:\n\\[\nf_Y(y) = f_X(g^{-1}(y)) \\left| \\frac{d}{dy} g^{-1}(y) \\right|.\n\\]\nThis formula ensures that probability is preserved under the transformation.\n\nExample: Let \\(X\\sim \\mathcal{N}(0,1)\\) and define \\(Y=3X+2\\). Determine the PDF of \\(Y\\).\nWe know \\(Y=g(X)=3X+2\\), which is strictly increasing. The inverse is\n\\[x=g^{-1}(y)=\\frac{y-2}{3},\\]\nand\n\\[\\frac{d}{dy}g^{-1}(y)=\\frac{1}{3}.\\]\nThe PDF of \\(X\\) is\n\\[f_X(x)=\\frac{1}{\\sqrt{2\\pi }}e^{-x^2/2}.\\]\nBy the change‑of‑variables formula,\n\\[f_Y(y)=f_X\\! \\left( g^{-1}(y)\\right) \\left| \\frac{d}{dy}g^{-1}(y)\\right| =f_X\\! \\left( \\frac{y-2}{3}\\right) \\cdot \\frac{1}{3}.\\]\nSo\n\\[f_Y(y)=\\frac{1}{3}\\cdot \\frac{1}{\\sqrt{2\\pi }}\\exp \\! \\left( -\\frac{1}{2}\\left( \\frac{y-2}{3}\\right) ^2\\right).\\]\nWe recognise this as the PDF of a normal distribution with mean \\(2\\) and variance \\(3^2=9\\), i.e.\n\\[Y\\sim \\mathcal{N}(2,9).\\]\nThis matches the general rule: if \\(X\\sim \\mathcal{N}(\\mu ,\\sigma ^2)\\) and \\(Y=aX+b\\), then \\(Y\\sim \\mathcal{N}(a\\mu +b,a^2\\sigma ^2)\\).\n\nset.seed(1)\n\n# Original variable\nX &lt;- rnorm(5000)\n\n# Transformation\nY &lt;- 3*X + 2\n\n# Plot\nplot(density(Y),\n     col = \"darkred\", lwd = 2,\n     main = \"Density of Y = 3X + 2\",\n     xlab = \"y\")\n\n# Add theoretical curve\ncurve(dnorm(x, mean = 2, sd = 3),\n      col = \"blue\", lwd = 2, add = TRUE)\n\nlegend(\"topright\",\n       legend = c(\"Simulated\", \"Theoretical N(2, 9)\"),\n       col = c(\"darkred\", \"blue\"), lwd = 2, bty = \"n\")\n\n\n\n\n\n\n\n\nThe simulated density (red) matches the theoretical \\(\\mathcal{N}(2,9)\\) curve (blue).\n\n\n7.1.2.2 Non‑Monotone Transformations\n\nIf \\(g\\) is not one‑to‑one (e.g., \\(Y = X^2\\)), then multiple \\(X\\)-values may map to the same \\(Y\\). In that case:\n\\[\nf_Y(y) = \\sum_{x : g(x) = y}\nf_X(x)\\left|\\frac{dx}{dy}\\right|.\n\\]\n\nExample: Let \\(X\\sim \\mathcal{N}(0,1)\\) and define \\(Y=X^2\\). Find the PDF of \\(Y\\).\nStep 1: Support of \\(Y\\)\nSince \\(Y=X^2\\), we must have Y.\nStep 2: Solve \\(y=x^2\\) for \\(x\\)\nFor a given \\(y&gt;0\\), the equation \\(y=x^2\\) has two solutions:\n\\[x=\\sqrt{y},\\quad x=-\\sqrt{y}.\\]\nSo both contribute to the density at \\(y\\).\nStep 3: Apply the non‑monotone transformation rule\nFor each branch, we use\n\\[f_Y(y)=\\sum _{x:x^2=y}f_X(x)\\left| \\frac{dx}{dy}\\right|.\\]\nThe PDF of \\(X\\) is\n\\[f_X(x)=\\frac{1}{\\sqrt{2\\pi }}e^{-x^2/2}.\\]\nThus, for \\(y&gt;0\\),\n\\[\\begin{aligned}f_Y(y)&=f_X(\\sqrt{y})\\left| \\frac{d}{dy}\\sqrt{y}\\right| +f_X(-\\sqrt{y})\\left| \\frac{d}{dy}(-\\sqrt{y})\\right| \\\\ &=\\frac{1}{\\sqrt{2\\pi }}e^{-(\\sqrt{y})^2/2}\\cdot \\frac{1}{2\\sqrt{y}}+\\frac{1}{\\sqrt{2\\pi }}e^{-(-\\sqrt{y})^2/2}\\cdot \\frac{1}{2\\sqrt{y}}\\\\ &=\\frac{1}{\\sqrt{2\\pi }}e^{-y/2}\\cdot \\frac{1}{2\\sqrt{y}}+\\frac{1}{\\sqrt{2\\pi }}e^{-y/2}\\cdot \\frac{1}{2\\sqrt{y}}\\\\ &=\\frac{1}{\\sqrt{2\\pi }}e^{-y/2}\\cdot \\frac{1}{\\sqrt{y}}.\\end{aligned}\\]\nThis is the PDF of a \\(\\chi^2\\) distribution with 1 degree of freedom, i.e. \\(Y\\sim \\chi_1^2\\).\n\nset.seed(1)\n\n# Original variable\nX &lt;- rnorm(5000)\n\n# Transformation\nY &lt;- X^2\n\n# Plot histogram\nhist(Y, breaks = 40, freq = FALSE,\n     col = \"lightgray\",\n     main = \"Distribution of Y = X^2\",\n     xlab = \"y\")\n\n# Add theoretical chi-square(1) density\ncurve(dchisq(x, df = 1),\n      col = \"darkgreen\", lwd = 2, add = TRUE)\n\nlegend(\"topright\",\n       legend = c(\"Simulated\", \"Chi-square(1)\"),\n       col = c(\"black\", \"darkgreen\"), lwd = 2, bty = \"n\")\n\n\n\n\n\n\n\n\nThe simulated \\(Y\\) shows a right‑skewed histogram that overlays beautifully with the theoretical \\(\\chi_1^2\\) curve.",
    "crumbs": [
      "Statistical Distributions",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Transformation of Random Variables</span>"
    ]
  },
  {
    "objectID": "02_3_transformation_rvs.html#inverse-transform-sampling",
    "href": "02_3_transformation_rvs.html#inverse-transform-sampling",
    "title": "7  Transformation of Random Variables",
    "section": "7.2 Inverse Transform Sampling",
    "text": "7.2 Inverse Transform Sampling\nIn the previous section, we studied how the distribution of a random variable changes when we apply a transformation, focusing on how probability density is redistributed under monotone and non‑monotone mappings. In simulation, however, we often face the inverse problem: rather than starting with a random variable and determining the distribution of its transformation, we want to construct a random variable with a specified distribution in the first place. Inverse transform sampling provides a direct solution to this problem by reversing the logic of transformation.\nStarting from a Uniform(0,1) random variable—the fundamental output of most random number generators—we apply an appropriate inverse transformation to obtain samples from a desired distribution. This method makes explicit the connection between transformation theory and simulation practice, and it highlights why understanding how probability behaves under transformations is essential for building simulation algorithms.\nWe have already seen examples of this idea in practice, such as generating Bernoulli and Exponential random variables from Uniform(0,1) values; inverse transform sampling now formalises this approach and explains why it works in general.\n\n7.2.1 Probability Integral Transform\nA key theoretical result underlying inverse transform sampling is the Probability Integral Transform. The probability integral transform explains why uniform random variables play a fundamental role in simulation and provides the theoretical justification for constructing new random variables by transforming Uniform(0,1) draws.\nFor inverse transform sampling, we work directly with the cumulative distribution function. If \\(X\\) is a continuous random variable with cumulative distribution function \\(F_X\\), then the transformed variable: \\[\nU = F_X(X) \\sim \\text{Uniform}(0,1).\n\\]\nTurning it around gives the sampling rule:\n\\[X = F_X^{-1}(U), \\qquad U \\sim \\text{Uniform}(0,1).\\]\nThis result allows us to turn the theoretical relationship between a random variable and its CDF into a practical simulation algorithm.\n\n\n7.2.2 Inverse Transform Sampling Algorithm\nThe probability integral transform provides the theoretical justification for inverse transform sampling. We now turn this result into a practical simulation algorithm. The goal is to generate random variables with a specified distribution using only Uniform(0,1) random numbers.\nSuppose we want to simulate a continuous random variable with cumulative distribution function \\(F_X\\), and assume that the inverse CDF \\(F_X^{-1}\\) is available in closed form. The inverse transform sampling algorithm proceeds as follows:\n\nGenerate a random number \\[\nU \\sim \\text{Uniform}(0,1).\n\\]\nTransform this value using the inverse CDF: \\[\nX = F_X^{-1}(U).\n\\]\nThe resulting value \\(X\\) is a random draw from the target distribution with CDF \\(F_X\\).\n\nThis algorithm works because the inverse transformation exactly reverses the probability integral transform. Since \\(F_X(X)\\) is uniformly distributed on \\([0,1]\\), applying the inverse CDF to a Uniform(0,1) random variable reconstructs the original distribution. In this sense, inverse transform sampling is a direct application of transformation theory: it constructs a desired distribution by transforming a simpler one.\nInverse transform sampling is conceptually simple and widely used in simulation, particularly when the inverse CDF has a closed‑form expression. However, not all distributions admit an explicit inverse CDF, which motivates alternative simulation methods introduced later in the course.\nWe now illustrate this algorithm with a simple example where the inverse CDF can be derived explicitly.\n\n\n7.2.3 Examples and Limitations\nExample (Exponential distribution).\nWe briefly previewed this idea earlier; here we are using it as a worked example of the general inverse transform algorithm.\nInverse transform sampling is easiest to see when the inverse CDF has a simple closed form. For the Exponential distribution with rate \\(\\lambda\\), the CDF is\n\\[\nF(x)=1-e^{-\\lambda x},\\qquad x\\ge 0.\n\\]\nSetting \\(U=F(X)\\) and solving for \\(X\\) gives the inverse transform\n\\[\nX = F^{-1}(U)= -\\frac{1}{\\lambda}\\log(1-U),\\qquad U\\sim \\text{Uniform}(0,1).\n\\]\nThe following R code implements the algorithm in three steps—generate uniforms, apply the inverse CDF, then validate the result by comparing the simulated histogram to rexp(). This mirrors the core simulation workflow:\n\nset.seed(123)\n\n# Step 1: generate Uniform(0,1) values\nu &lt;- runif(1000)\n\n# Step 2: transform them into Exponential(lambda)\nlambda &lt;- 2   # rate parameter\nx &lt;- -log(1 - u) / lambda\n\n# Step 3: Plot and compare with R's built-in rexp()\nx_builtin &lt;- rexp(1000, rate = lambda)\nhist(x, breaks = 30, col = rgb(0,0,1,0.4), freq = FALSE,\n     main = \"Comparison of PDF of exponential distribution: Transform vs rexp()\")\nhist(x_builtin, breaks = 30, col = rgb(1,0,0,0.4), freq = FALSE, add = TRUE)\nlegend(\"topright\", legend = c(\"Transform\", \"rexp()\"),\n       fill = c(rgb(0,0,1,0.4), rgb(1,0,0,0.4)))\n\n\n\n\n\n\n\n\nLimitations (why we need other methods).\nInverse transform sampling is conceptually simple, but it has practical limitations:\n\nYou need an invertible CDF (or a tractable numerical inverse).\nMany common distributions do not have a neat closed-form inverse CDF, so applying \\(F^{-1}\\) directly may be difficult or computationally expensive.\nNot all transformations are “one-line” like the exponential.\nEven when the CDF exists and is monotone, solving \\(u = F(x)\\) for \\(x\\) may not be algebraically possible.\nThis motivates alternative sampling techniques.\nWhen \\(F^{-1}\\) is unavailable or inefficient, we use other approaches (e.g., acceptance–rejection or specialised transformations such as Box–Muller for Normal simulation), which you cover later in the unit.",
    "crumbs": [
      "Statistical Distributions",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Transformation of Random Variables</span>"
    ]
  },
  {
    "objectID": "02_3_transformation_rvs.html#transformation-of-multiple-random-variables",
    "href": "02_3_transformation_rvs.html#transformation-of-multiple-random-variables",
    "title": "7  Transformation of Random Variables",
    "section": "7.3 Transformation of Multiple Random Variables",
    "text": "7.3 Transformation of Multiple Random Variables\nIn many applications, we work not with a single random variable but with a pair (or vector) of variables whose joint distribution is known. When we apply a transformation to such a pair, the result is a new pair of random variables whose joint distribution we often need to determine. This section develops the tools for understanding how probability densities change under transformations of two (or more) variables.\nTransformations of multiple random variables arise naturally in statistics, simulation, and modelling. Examples include converting Cartesian coordinates to polar coordinates, forming sums and differences of random variables, and applying linear transformations to generate correlated normals. The key idea is that probability must be preserved under the transformation, and the mathematical tool that ensures this is the Jacobian.\n\n7.3.1 Transformations of Two Random Variables\nWhy This Matters\nTransformations of multiple random variables allow us to:\n\nderive distributions of sums, ratios, and other combinations,\n\nchange coordinate systems to simplify integrals,\n\nunderstand how linear maps create covariance,\n\nsimulate multivariate distributions efficiently,\n\nand prepare for higher‑dimensional transformations in later topics.\n\nThis section builds the foundation for the next major topic: linear transformations and the multivariate normal, where the Jacobian plays a central role in understanding how densities behave under matrix transformations.\nSuppose \\((X, Y)\\) has a known joint density \\(f_{X,Y}(x,y)\\), and we define a new pair:\n\\[\nU = g_1(X, Y), \\qquad V = g_2(X, Y).\n\\]\nOur goal is to find the joint density \\(f_{U,V}(u,v)\\).\nIf the transformation is one‑to‑one and differentiable, we can invert it:\n\\[\nx = h_1(u,v), \\qquad y = h_2(u,v),\n\\]\nand use the Jacobian determinant to adjust for how the transformation stretches or compresses area.\n\n\n7.3.2 The Jacobian Formula (Continuous Case)\nIf the transformation is smooth and invertible, then\n\\[\nf_{U,V}(u,v)\n= f_{X,Y}(h_1(u,v),\\, h_2(u,v))\n\\left| \\det J \\right|,\n\\]\nwhere\n\\[\nJ =\n\\begin{pmatrix}\n\\dfrac{\\partial x}{\\partial u} & \\dfrac{\\partial x}{\\partial v} \\\\\n\\dfrac{\\partial y}{\\partial u} & \\dfrac{\\partial y}{\\partial v}\n\\end{pmatrix}\n\\]\nis the Jacobian matrix of the inverse transformation.\nThe determinant \\(|\\det J|\\) measures how the transformation changes area, ensuring that total probability remains 1.\n\nExample 1: Sums and Differences\n\nLet \\((X,Y)\\) have joint density \\(f_{X,Y}(x,y)\\). Defin\n\\[\nU = X + Y, \\qquad V = X - Y.\n\\]\nThis transformation is linear and invertible, and the Jacobian is constant.\nIt is widely used in deriving distributions of sums of independent normals.\nStep 1: Invert the transformation\nSolve for \\(X, Y\\) in terms of \\(U, V\\):\n\\[\nX = \\dfrac{U + V}{2}, \\qquad Y = \\dfrac{U - V}{2}.\n\\]\nSo\n\\[\nx = h_1(u,v) = \\dfrac{u+v}{2}, \\quad\ny = h_2(u,v) = \\dfrac{u-v}{2}.\n\\]\nStep 2: Compute the Jacobian\n\\[\nJ =\n\\begin{pmatrix}\n\\dfrac{\\partial x}{\\partial u} & \\dfrac{\\partial x}{\\partial v} \\\\\n\\dfrac{\\partial y}{\\partial u} & \\dfrac{\\partial y}{\\partial v}\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n\\dfrac{1}{2} & \\dfrac{1}{2} \\\\\n\\dfrac{1}{2} & -\\dfrac{1}{2}\n\\end{pmatrix}.\n\\]\nThen\n\\[\n\\det J\n= \\dfrac{1}{2}\\cdot\\left(-\\dfrac{1}{2}\\right)\n- \\dfrac{1}{2}\\cdot\\dfrac{1}{2}\n= -\\dfrac{1}{4} - \\dfrac{1}{4}\n= -\\dfrac{1}{2},\n\\]\nso\n\\[\n|\\det J| = \\dfrac{1}{2}.\n\\]\nStep 3: Write the joint density of \\((U, V)\\)\n\\[\nf_{U,V}(u,v)\n= f_{X,Y}\\!\\left(\\dfrac{u+v}{2}, \\dfrac{u-v}{2}\\right)\\cdot \\dfrac{1}{2}.\n\\]\n\nset.seed(123)\n\nn &lt;- 5000\nX &lt;- rnorm(n)\nY &lt;- rnorm(n)\n\nU &lt;- X + Y\nV &lt;- X - Y\n\npar(mfrow = c(1, 2))\n\nplot(X, Y, pch = 16, cex = 0.4, col = \"steelblue\",\n     main = \"Original (X, Y)\",\n     xlab = \"X\", ylab = \"Y\")\n\nplot(U, V, pch = 16, cex = 0.4, col = \"tomato\",\n     main = \"Transformed (U = X+Y, V = X-Y)\",\n     xlab = \"U\", ylab = \"V\")\n\n\n\n\n\n\n\n\n\n\\((X,Y)\\) is a round cloud (independent normals).\n\\((U,V)\\) becomes an axis‑aligned ellipse — the transformation stretches and rotates the cloud.\nThis visually confirms the Jacobian‑based derivation.\n\n\nExample 2: Polar Coordinates\n\nLet \\((X, Y)\\) have joint density \\(f_{X,Y}(x,y)\\). Define\n\\[\nU = R = \\sqrt{X^2 + Y^2}, \\qquad V = \\Theta = \\arctan(Y/X).\n\\]\nThe Jacobian determinant is \\(r\\), which explains why polar integrals include the factor \\(r\\).\nWe want \\(f_{R,\\Theta}(r,\\theta)\\).\nStep 1: Invert the transformation\nThe inverse map is the usual polar–Cartesian relation:\n\\[\nX = r\\cos\\theta, \\qquad Y = r\\sin\\theta.\n\\]\nSo\n\\[\nx = h_1(r,\\theta) = r\\cos\\theta, \\quad\ny = h_2(r,\\theta) = r\\sin\\theta.\n\\]\nStep 2: Compute the Jacobian\n\\[\nJ =\n\\begin{pmatrix}\n\\dfrac{\\partial x}{\\partial r} & \\dfrac{\\partial x}{\\partial \\theta} \\\\\n\\dfrac{\\partial y}{\\partial r} & \\dfrac{\\partial y}{\\partial \\theta}\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n\\cos\\theta & -r\\sin\\theta \\\\\n\\sin\\theta & r\\cos\\theta\n\\end{pmatrix}.\n\\]\nThen\n\\[\n\\det J\n= \\cos\\theta \\cdot r\\cos\\theta\n- (-r\\sin\\theta)\\cdot\\sin\\theta\n= r(\\cos^2\\theta + \\sin^2\\theta)\n= r.\n\\]\nSo\n\\[\n|\\det J| = r.\n\\]\nStep 3: Joint density of \\((R,\\Theta)\\)\n\\[\nf_{R,\\Theta}(r,\\theta)\n= f_{X,Y}(r\\cos\\theta, r\\sin\\theta)\\cdot r.\n\\]\n\nset.seed(1)\n\nn &lt;- 5000\nX &lt;- rnorm(n)\nY &lt;- rnorm(n)\n\nR &lt;- sqrt(X^2 + Y^2)\nTheta &lt;- atan2(Y, X)\n\npar(mfrow = c(1, 2))\n\nhist(R, breaks = 40, freq = FALSE, col = \"lightgray\",\n     main = \"Distribution of R\",\n     xlab = \"R\")\ncurve(dchisq(x^2, df = 2) * 2*x, add = TRUE, col = \"darkgreen\", lwd = 2)\n\nhist(Theta, breaks = 40, freq = FALSE, col = \"lightblue\",\n     main = \"Distribution of Theta\",\n     xlab = expression(theta))\n\n\n\n\n\n\n\n\n\nR follows the Rayleigh distribution (equivalently, \\(R^2\\sim \\chi_2^2\\)).\n\\(\\Theta\\) is uniform on \\([-\\pi ,\\pi ]\\).\n\n\nExample 3: Linear Transformations\n\nLet \\((X, Y)\\) have joint density \\(f_{X,Y}(x,y)\\), and let \\[\n\\begin{pmatrix} U \\\\ V \\end{pmatrix}\n=\nA \\begin{pmatrix} X \\\\ Y \\end{pmatrix},\n\\quad\nA =\n\\begin{pmatrix}\na & b \\\\\nc & d\n\\end{pmatrix},\n\\]\nwith \\(\\det A \\neq 0\\). Then the Jacobian determinant is simply \\(|\\det A|\\).\nThis case is especially important for the multivariate normal distribution, where linear transformations create covariance structures. The correlated‑normal simulation you used earlier is a special case of this idea.\nStep 1: Invert the transformation\nBecause \\(\\det A \\neq 0\\), \\(A\\) is invertible:\n\\[\n\\begin{pmatrix} X \\\\ Y \\end{pmatrix}\n= A^{-1} \\begin{pmatrix} U \\\\ V \\end{pmatrix}.\n\\]\nWrite\n\\[\nA^{-1} =\n\\frac{1}{ad - bc}\n\\begin{pmatrix}\nd & -b \\\\\n-c & a\n\\end{pmatrix}.\n\\]\nSo\n\\[\nx = h_1(u,v), \\quad y = h_2(u,v)\n\\]\nare linear functions of \\((u,v)\\).\nStep 2: Jacobian of the inverse\nFor a linear map, the Jacobian matrix of the inverse is just \\(A^{-1}\\), so\n\\[\n|\\det J| = |\\det A^{-1}| = \\frac{1}{|\\det A|}.\n\\]\nStep 3: Joint density of \\((U,V)\\)\n\\[\nf_{U,V}(u,v)\n= f_{X,Y}\\big(h_1(u,v), h_2(u,v)\\big)\\cdot \\frac{1}{|\\det A|}.\n\\]\n\nset.seed(1)\n\nn &lt;- 5000\nX &lt;- rnorm(n)\nY &lt;- rnorm(n)\n\nA &lt;- matrix(c(1, 0.8,\n              0.2, 1), nrow = 2, byrow = TRUE)\n\nUV &lt;- A %*% rbind(X, Y)\nU &lt;- UV[1, ]\nV &lt;- UV[2, ]\n\npar(mfrow = c(1, 2))\n\nplot(X, Y, pch = 16, cex = 0.4, col = \"gray40\",\n     main = \"Original (X, Y)\",\n     xlab = \"X\", ylab = \"Y\")\n\nplot(U, V, pch = 16, cex = 0.4, col = \"firebrick\",\n     main = \"Transformed (U, V) = A(X, Y)\",\n     xlab = \"U\", ylab = \"V\")\n\n\n\n\n\n\n\n\n\nThe original cloud is round (independent normals).\nThe transformed cloud becomes tilted and stretched, showing induced correlation.",
    "crumbs": [
      "Statistical Distributions",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Transformation of Random Variables</span>"
    ]
  },
  {
    "objectID": "02_3_transformation_rvs.html#linear-transformations-and-the-multivariate-normal",
    "href": "02_3_transformation_rvs.html#linear-transformations-and-the-multivariate-normal",
    "title": "7  Transformation of Random Variables",
    "section": "7.4 Linear Transformations and the Multivariate Normal",
    "text": "7.4 Linear Transformations and the Multivariate Normal\nLinear transformations play a central role in probability and statistics, especially when working with multivariate normal distributions. In earlier sections, we saw how a linear transformation of two independent standard normals can create correlation. Here, we generalise that idea and show how any multivariate normal distribution can be constructed from a standard one using a matrix transformation.\nWhy This Matters\nLinear transformations of multivariate normals underpin:\n\nsimulation of correlated variables,\n\nregression and linear models,\n\nprincipal component analysis (PCA),\n\nBayesian multivariate priors,\n\nand nearly all multivariate statistical methods.\n\nThis section forms the conceptual bridge between probability theory and practical statistical modelling.\n\n7.4.1 Linear Transformations of Random Vectors\nLet\n\\[\n\\mathbf{X} = (X_1, X_2, \\dots, X_k)^\\top\n\\] be a random vector, and let \\(A\\) be a fixed \\(m \\times k\\) matrix. A linear transformation of \\(\\mathbf{X}\\) is\n\\[\n\\mathbf{Y} = A\\mathbf{X} + \\mathbf{b},\n\\] where \\(\\mathbf{b}\\) is a constant vector.\nThis transformation:\n\nrotates, stretches, or compresses the space,\n\nshifts the mean by \\(\\mathbf{b}\\),\n\nand reshapes the covariance structure through the matrix \\(A\\).\n\n\n\n7.4.2 The Multivariate Normal Distribution\nA random vector \\(\\mathbf{X}\\) is multivariate normal if every linear combination of its components is normally distributed.\nIf\n\\[\n\\mathbf{X} \\sim N(\\boldsymbol{\\mu}, \\Sigma),\n\\] then:\n\n\\(\\boldsymbol{\\mu}\\) is the mean vector,\n\n\\(\\Sigma\\) is the covariance matrix (symmetric and positive‑definite).\n\nThe density is\n\\[\nf_{\\mathbf{X}}(\\mathbf{x})\n= \\frac{1}{(2\\pi)^{k/2} |\\Sigma|^{1/2}}\n\\exp\\!\\left(\n-\\frac{1}{2}(\\mathbf{x}-\\boldsymbol{\\mu})^\\top\n\\Sigma^{-1}(\\mathbf{x}-\\boldsymbol{\\mu})\n\\right).\n\\]\nA key property makes the multivariate normal uniquely convenient:\n\\[\n\\mathbf{X} \\sim N(\\boldsymbol{\\mu}, \\Sigma)\n\\quad \\Longrightarrow \\quad\nA\\mathbf{X} + \\mathbf{b} \\sim N(A\\boldsymbol{\\mu} + \\mathbf{b},\\; A\\Sigma A^\\top).\n\\]\nThis means:\n\nlinear transformations of normals are still normal (preserve normality),\n\nthe mean transforms linearly,\n\nthe covariance transforms quadratically.\n\nThis is why the multivariate normal is so widely used: it behaves perfectly under linear operations.\nExample: Linear Transformation of a Bivariate Normal\nWe simulate \\(\\mathbf{X}\\sim N(\\mu ,\\Sigma )\\) with mean vector and covariance matrix:\n\\[\n\\mu = (1,2)^{\\top}, \\qquad\n\\Sigma =\\left( \\begin{matrix}4&1\\\\ 1&1\\end{matrix}\\right).\n\\]\nThe entries encode \\(\\text{Var}(X_1) = 4\\), \\(\\text{Var}(X_2) = 1\\) and \\(\\text{Cov}(X_1,X_2) = 1\\).\nThen, apply \\(\\mathbf{Y}=A\\mathbf{X}+b\\) with\n\\[\nA =\\left( \\begin{matrix}1&0\\\\ -1&2\\end{matrix}\\right), \\qquad\nb = (0, 1)^{\\top}\n\\]\nCompare the scatterplots.\n\nset.seed(1)\n\nlibrary(MASS)\n\n# Original distribution\nmu  &lt;- c(1, 2)\nSigma &lt;- matrix(c(4, 1,\n                  1, 1), 2, 2)\n\nn &lt;- 5000\nX &lt;- mvrnorm(n, mu = mu, Sigma = Sigma)\n\n# Linear transformation\nA &lt;- matrix(c(1, 0,\n             -1, 2), 2, 2)\nb &lt;- c(0, 1)\n\n# Matrix algebra\nY &lt;- t(A %*% t(X)) + matrix(b, n, 2, byrow = TRUE)\n\npar(mfrow = c(1, 2))\n\nplot(X[,1], X[,2], pch = 16, cex = 0.4, col = \"steelblue\",\n     main = \"Original X ~ N(mu, Sigma)\",\n     xlab = \"X1\", ylab = \"X2\")\n\nplot(Y[,1], Y[,2], pch = 16, cex = 0.4, col = \"tomato\",\n     main = \"Transformed Y = A X + b\",\n     xlab = \"Y1\", ylab = \"Y2\")\n\n\n\n\n\n\n\n\n\nThe original cloud has a certain tilt and spread.\nThe transformed cloud is rotated and stretched exactly as predicted by \\(A\\Sigma A^{\\top }\\).\n\n\n\n7.4.3 Constructing a Multivariate Normal via Cholesky\nIf \\(\\Sigma\\) is a covariance matrix, we can write:\n\\[\n\\Sigma = LL^\\top,\n\\]\nwhere \\(L\\) is the Cholesky factor (lower triangular).\nIf \\(\\mathbf{Z} \\sim N(\\mathbf{0}, I)\\), then\n\\[\n\\mathbf{X} = L\\mathbf{Z} + \\boldsymbol{\\mu}\n\\]\nhas distribution \\(N(\\boldsymbol{\\mu}, \\Sigma)\\).\nThis generalises earlier 2D construction:\n\\[\nY = \\rho Z_1 + \\sqrt{1-\\rho^2}\\, Z_2,\n\\]\nwhich is exactly the Cholesky factor of \\(\\begin{pmatrix}1 & \\rho \\\\ \\rho & 1\\end{pmatrix}\\).\nExample: Constructing Correlated Normals via Cholesky\nDefine the covariance matrix\n\\[\\Sigma =\\left( \\begin{matrix}1&\\rho \\\\ \\rho &1\\end{matrix}\\right) =\\left( \\begin{matrix}1&0.8\\\\ 0.8&1\\end{matrix}\\right).\\]\nBoth variables \\(X\\) and \\(Y\\) have variance 1, and covariance \\(\\rho\\).\n\nset.seed(1)\n\nn &lt;- 5000\nrho &lt;- 0.8\n\n# Covariance matrix\nSigma &lt;- matrix(c(1, rho,\n                  rho, 1), 2, 2)\n\n# Cholesky factor: L^T L (upper triangular)\nL &lt;- chol(Sigma)\n\n# Generate 2n independent N(0,1) values\nZ &lt;- matrix(rnorm(2*n), n, 2)\n\n# Construct X = Z L^T (linear transformation)\nX &lt;- Z %*% t(L)\n\npar(mfrow = c(1, 2))\n\nplot(Z[,1], Z[,2], pch = 16, cex = 0.4, col = \"gray60\",\n     main = \"Z ~ N(0, I)\",\n     xlab = \"Z1\", ylab = \"Z2\")\n\nplot(X[,1], X[,2], pch = 16, cex = 0.4, col = \"firebrick\",\n     main = \"X = Z L^T (Correlated Normals)\",\n     xlab = \"X1\", ylab = \"X2\")\n\n\n\n\n\n\n\n\n\n\\(Z\\) is a round cloud (independent normals).\n\\(X\\) becomes a tilted ellipse with correlation \\(\\rho =0.8\\).\nThis is how Cholesky induces covariance.\n\nExample: General k‑Dimensional Construction\nSimulate \\(\\mathbf{X}\\sim N(\\mu ,\\Sigma )\\) using Cholesky.\nDefine a mean vector and a covariance matrix:\n\\[\n\\mu =(0,\\; 1,\\; 2)^{\\top }, \\qquad\n\\Sigma =\\left( \\begin{matrix}1&0.5&0.2\\\\ 0.5&2&0.3\\\\ 0.2&0.3&1\\end{matrix}\\right).\n\\]\n\nset.seed(1)\n\n# 3D mean and covariance\nmu &lt;- c(0, 1, 2)\nSigma &lt;- matrix(c(1, 0.5, 0.2,\n                  0.5, 2, 0.3,\n                  0.2, 0.3, 1), 3, 3)\n\nn &lt;- 5000\n\n# Cholesky factor\nL &lt;- chol(Sigma)\n\n# Standard normals\nZ &lt;- matrix(rnorm(3*n), n, 3)\n\n# Construct X = Z L^T + mu\nX &lt;- Z %*% t(L) + matrix(mu, n, 3, byrow = TRUE)\n\n# Quick diagnostic: sample covariance\nround(cov(X), 2)\n\n     [,1] [,2] [,3]\n[1,] 1.35 0.69 0.21\n[2,] 0.69 1.77 0.15\n[3,] 0.21 0.15 0.90\n\n\n\nThe sample covariance of \\(X\\) matches \\(\\Sigma\\).\nThe construction works in any dimension.\n\n\nset.seed(1)\n\n# 3D mean and covariance\nmu &lt;- c(0, 1, 2)\nSigma &lt;- matrix(c(1, 0.5, 0.2,\n                  0.5, 2, 0.3,\n                  0.2, 0.3, 1), 3, 3)\n\nn &lt;- 5000\n\n# Cholesky factor\nL &lt;- chol(Sigma)\n\n# Standard normals\nZ &lt;- matrix(rnorm(3*n), n, 3)\n\n# Construct X = Z L^T + mu\nX &lt;- Z %*% t(L) + matrix(mu, n, 3, byrow = TRUE)\n\n# 3D scatterplot\nlibrary(scatterplot3d)\n\nscatterplot3d(X[,1], X[,2], X[,3],\n              pch = 16, cex.symbols = 0.4,\n              color = \"steelblue\",\n              main = \"3D Scatterplot of Multivariate Normal Sample\",\n              xlab = \"X1\", ylab = \"X2\", zlab = \"X3\")\n\n\n\n\n\n\n\n\n\nA 3D elliptical cloud whose shape reflects the covariance matrix.\nThe cloud is tilted and stretched in directions determined by \\(\\Sigma\\).\nThis visually reinforces the idea that \\(X=LZ+\\mu\\).",
    "crumbs": [
      "Statistical Distributions",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Transformation of Random Variables</span>"
    ]
  },
  {
    "objectID": "02_4_workshop.html",
    "href": "02_4_workshop.html",
    "title": "8  Workshop Activities",
    "section": "",
    "text": "8.1 Part 1: Univariate distributions in simulation\nThis workshop consists of three parts:\nBy the end of this workshop, you should be able to:\nFrom this workshop onward, no R Notebook or Jupyter Notebook templates will be provided.\nYou are expected to practice creating and rendering HTML documents independently, as this is an essential component of your project.\nIf you find that R Notebook or Jupyter Notebook is somewhat limited in producing polished reports, you may consider using Quarto.\nQuarto is a modern and more powerful publishing system (and successor to R Markdown) that supports dynamic content using R and Python. It can be used to create reproducible, production-quality articles, presentations, dashboards, websites, blogs, and books in formats such as HTML, PDF, MS Word, ePub, and more. For instance, this book and all the lecture slides are developed using Quarto.\nYou can install Quarto and use it directly within RStudio or VS Code.",
    "crumbs": [
      "Statistical Distributions",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Workshop Activities</span>"
    ]
  },
  {
    "objectID": "02_4_workshop.html#part-1-univariate-distributions-in-simulation",
    "href": "02_4_workshop.html#part-1-univariate-distributions-in-simulation",
    "title": "8  Workshop Activities",
    "section": "",
    "text": "Exercise 1: Geometric Distribution\nLet \\[X \\sim \\text{Geometric}(p),\\] where \\(p = 0.3\\).\n\nSimulate 10,000 observations.\nPlot the empirical PMF (barplot of relative frequencies).\nOverlay the theoretical PMF.\nCompute:\n\nempirical mean and variance\ntheoretical mean and variance\n\nExplain:\n\nWhy is the geometric distribution memoryless? Verify: \\(P(X&gt;s+t | X&gt;s)=P(X&gt;t)\\)\nHow does changing \\(p\\) affect skewness?\n\nEstimate \\(P(X &gt; 5)\\) analytically and via simulation. Compare results.\n\n\n\nExercise 2: Gamma Distribution\nLet \\[X \\sim \\text{Gamma}(\\alpha = 3, \\beta = 2)\\] (Use shape–rate parameterisation.)\n\nSimulate 10,000 observations.\nPlot histogram with theoretical density overlay.\nCompute empirical vs theoretical mean and variance.\nInvestigate how changing \\(\\alpha\\) affects:\n\nskewness\ntail behaviour\n\n\n\n\nExercise 3: Beta Distribution\nLet \\[X \\sim \\text{Beta}(2,5).\\]\n\nSimulate 10,000 observations.\nPlot histogram and density.\nCompute mean and variance.\nRepeat for:\n\nBeta(0.5, 0.5)\nBeta(5, 5)\n\nHow do the shape parameters affect:\n\nsymmetry?\nconcentration?\nboundary behaviour?\n\n\n\n\nExercise 4: Multinomial Distribution\nLet\n\\[(X_1, X_2, X_3) \\sim \\text{Multinomial}(n=20, p=(0.2,0.5,0.3))\\]\n\nSimulate 5,000 independent multinomial experiments.\nCompute:\n\nsample means of each component\ncovariance matrix\n\nVerify: \\(E[X_i] = np_i\\)\nVerify that components are negatively correlated.\nVisualisation\n\nScatterplot of \\(X_1\\) vs \\(X_2\\)\nComment on dependence structure.\n\nConceptual Question: Why must multinomial components be dependent?",
    "crumbs": [
      "Statistical Distributions",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Workshop Activities</span>"
    ]
  },
  {
    "objectID": "02_4_workshop.html#part-2-joint-distributions-and-dependence",
    "href": "02_4_workshop.html#part-2-joint-distributions-and-dependence",
    "title": "8  Workshop Activities",
    "section": "8.2 Part 2: Joint distributions and dependence",
    "text": "8.2 Part 2: Joint distributions and dependence\n\nExercise 5\nExplain why dependence matters in simulation. Give 3 examples.\n\n\nExercise 6: Visualising Dependence\nSimulate two scenarios:\nCase A: Independent variables\nLet: \\[\nX \\sim \\mathcal{N}(0,1), \\quad Y \\sim \\mathcal{N}(0,1)\n\\]\nGenerate independently.\nCase B: Dependent variables\nDefine:\n\\[\nY = 0.8X + \\sqrt{1-0.8^2} Z\n\\]\nwhere \\(Z \\sim \\mathcal{N}(0,1)\\) independent of \\(X\\).\n\nGenerate 5,000 observations for each case.\nProduce scatterplots.\nCompute correlation.\nCompare visually and numerically.\n\n\n\nExercise 7: Constructing a Joint Distribution (Discrete)\nLet:\n\\[\nP(X=0,Y=0)=0.2, \\quad\nP(X=0,Y=1)=0.3,\n\\] \\[\nP(X=1,Y=0)=0.1, \\quad\nP(X=1,Y=1)=0.4\n\\]\n\nVerify probabilities sum to 1.\nCompute marginal distributions.\nCheck if independent.\nCompute covariance manually.\nSimulate 10,000 draws from this joint distribution.\nCompare empirical and theoretical covariance.\n\n\n\nExercise 8: Zero Correlation ≠ Independence\nLet:\n\\[\nX \\sim \\mathcal{N}(0,1)\n\\]\nDefine:\n\\[\nY = X^2\n\\]\n\nSimulate 10,000 observations.\nCompute correlation.\nPlot scatterplot.\nExplain why they are dependent despite near-zero correlation.\n\n\nExercise 9: Conditional Simulation\nLet:\n\\[\nX \\sim \\text{Gamma}(2,1)\n\\]\nGiven \\(X=x,\\)\n\\[\nY|X=x \\sim \\text{Poisson}(x)\n\\]\n\nSimulate 5,000 pairs.\nPlot scatterplot.\nEstimate:\n\n(E[Y])\nCompare with theoretical value using law of total expectation.\n\n\nHint:\n\\[\nE[Y] = E[E(Y|X)]\n\\]\n\n\n\nExercise 10: Simulating Bivariate Normal\nConstruct a portfolio simulation:\nLet:\n\n\\(X_1, X_2\\) be correlated returns (bivariate normal).\nPortfolio return:\n\n\\[\nR = 0.6X_1 + 0.4X_2\n\\]\nTasks:\n\nSimulate 10,000 returns.\nEstimate variance.\nPlot scatterplot and explain.\nExplain how ignoring dependence affects risk estimation.",
    "crumbs": [
      "Statistical Distributions",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Workshop Activities</span>"
    ]
  },
  {
    "objectID": "02_4_workshop.html#part-3-transformation-of-random-variables",
    "href": "02_4_workshop.html#part-3-transformation-of-random-variables",
    "title": "8  Workshop Activities",
    "section": "8.3 Part 3: Transformation of random variables",
    "text": "8.3 Part 3: Transformation of random variables",
    "crumbs": [
      "Statistical Distributions",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Workshop Activities</span>"
    ]
  },
  {
    "objectID": "02_5_workshop_sol.html",
    "href": "02_5_workshop_sol.html",
    "title": "9  Workshop Solutions",
    "section": "",
    "text": "9.1 Part 1: Univariate distributions in simulation\nThis workshop consists of three parts:",
    "crumbs": [
      "Statistical Distributions",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Workshop Solutions</span>"
    ]
  },
  {
    "objectID": "02_5_workshop_sol.html#part-1-univariate-distributions-in-simulation",
    "href": "02_5_workshop_sol.html#part-1-univariate-distributions-in-simulation",
    "title": "9  Workshop Solutions",
    "section": "",
    "text": "Exercise 1: Geometric Distribution\nLet \\[X \\sim \\text{Geometric}(p),\\] where \\(p = 0.3\\).\n\nSimulate 10,000 observations.\nPlot the empirical PMF (barplot of relative frequencies).\nOverlay the theoretical PMF.\nCompute:\n\nempirical mean and variance\ntheoretical mean and variance\n\nExplain:\n\nWhy is the geometric distribution memoryless? Verify: \\(P(X&gt;s+t | X&gt;s)=P(X&gt;t)\\)\nHow does changing \\(p\\) affect skewness?\n\nEstimate \\(P(X &gt; 5)\\) analytically and via simulation. Compare results.\n\n\nSolution\n\nSimulate 10,000 observations.\n\n\nset.seed(123)\nn &lt;- 10000\np &lt;- 0.3\nx &lt;- rgeom(n, prob = p)\n\n\nPlot the empirical PMF (barplot of relative frequencies).\nOverlay the theoretical PMF.\n\n\n# create frequency table (count how many times each value appears)\ntab &lt;- table(x) \n# tab # printing the object to see what it is\nemp_pmf &lt;- tab / n # devided by n to get relative frequencies\n\n# Plot the empirical PMF (barplot of relative frequencies).\nbarplot(emp_pmf,\n        main = \"Empirical PMF of Geometric(0.3)\",\n        xlab = \"x (number of failures before first success)\",\n        ylab = \"Probability\",\n        col = \"skyblue\")\n\n# theoretical values (for observed range)\nx_vals &lt;- as.numeric(names(emp_pmf))\n# names(emp_pmf) returns the values of the rv (x) in \"string\" format\n# wrap as.numeric() convert \"string\" name to numbers\n# theoretical PMF using the same range of x values\ntheo_pmf &lt;- dgeom(x_vals, prob = p)\n\n# overlay theoretical probabilities\n# have to plot in the same code chunk\npoints(seq_along(x_vals),\n       theo_pmf,\n       col = \"red\",\n       pch = 19)\n\n# legend\nlegend(\"topright\",\n       legend = c(\"Empirical\", \"Theoretical\"),\n       fill = c(\"skyblue\", NA),\n       border = c(\"black\", NA),\n       pch = c(NA, 19),\n       col = c(\"black\", \"red\"))\n\n\n\n\n\n\n\n\nEmpirical PMF closely matches theoretical PMF.\n\nCompute:\n\nempirical mean and variance\ntheoretical mean and variance\n\n\n\n# Empirical mean and variance\nemp_mean &lt;- mean(x)\nemp_var  &lt;- var(x)\n\n# Theoretical mean and variance\ntheo_mean &lt;- (1 - p) / p\ntheo_var  &lt;- (1 - p) / (p^2)\n\ncat(\"Empirical mean:\", emp_mean,\n    \", Empirical variance:\", emp_var)\n\nEmpirical mean: 2.3 , Empirical variance: 7.554555\n\ncat(\"Theoretical mean:\", theo_mean,\n    \", Theoretical variance:\", theo_var)\n\nTheoretical mean: 2.333333 , Theoretical variance: 7.777778\n\n\n\nThe reason empirical mean (and variance) are close to the theoretical values is due to the Law of Large Numbers (LLN).\nVariance closeness is also due to consistency (LLN applied to functions of X).\n\n\nExplain:\n\nWhy is the geometric distribution memoryless? Verify: \\(P(X&gt;s+t | X&gt;s)=P(X&gt;t)\\)\nHow does changing \\(p\\) affect skewness?\n\n\nUsing conditional probability:\n\\[\nP(X&gt;s+t \\mid X&gt;s) = \\frac{P(X&gt;s+t)}{P(X&gt;s)},\n\\]\nand mean(x &gt;= k) to estimate P(X&gt;k)\n\n# Define\ns &lt;- 3\nt &lt;- 4\n\n# using simulated data x and conditional probability\nlhs &lt;- mean(x &gt;= (s + t)) / mean(x &gt;= s)\nrhs &lt;- mean(x &gt;= t)\n# use &gt;= instead of &gt; to compensate `rgeom` for modelling X as number of \"failures\"\n\nlhs\n\n[1] 0.2382501\n\nrhs\n\n[1] 0.2387\n\n\nMathematically, for a geometric distribution:\n\\[\nP(X&gt;k) = (1-p)^{k+1},\n\\]\nSo,\n\\[\n\\frac{P(X&gt;s+t)}{P(X&gt;s)} = \\frac{(1-p)^{s+t+1}}{(1-p)^{s+1}} = (1-p)^t = P(X&gt;t)\n\\]\nThe cancellation is why memorylessness holds.\n\n# Effect of changing p\nset.seed(123)\nn &lt;- 10000\nps &lt;- c(0.3, 0.5, 0.7, 0.9)\ncols &lt;- c(\"blue\", \"red\", \"purple\", \"green\")\n\n# use for loop to plot and overlay\nfor (i in 1:length(ps)) {\n    if (i == 1) { # base plot\n        plot(table(rgeom(n, ps[[i]])) / n, \n        type=\"b\",\n        col = cols[[i]],\n        ylab = \"Relative frequency\",\n        xlab = \"x\")\n    } else { # use lines() to overlay\n    lines(table(rgeom(n, ps[[i]])) / n, \n     type=\"b\",\n     col = cols[[i]])\n    }\n}\n\nlegend(\"topright\",\n       legend = paste(\"p =\", ps),\n       col = c(\"blue\", \"red\", \"purple\", \"green\"),\n       lty = 1,\n       pch = 1)\n\n\n\n\n\n\n\n\nIncreasing \\(p\\) makes the curve shift left and decay faster, concentrating more probability at small values (especially at 0).\n\nEstimate \\(P(X &gt; 5)\\) analytically and via simulation. Compare results.\n\n\nk &lt;- 5\nanalytic_prob &lt;- (1 - p)^(k + 1)\nanalytic_prob\n\n[1] 0.117649\n\nsim_prob &lt;- mean(x &gt; k)\nsim_prob # they should be extremely close\n\n[1] 0.118\n\n\nWe are verifying that the simulated tail probability matches the theoretical geometric survival probability \\(P(X&gt;5) = (1-p)^{5+1} = 0.117649\\).\n\n\n\n\nExercise 2: Gamma Distribution\nLet \\[X \\sim \\text{Gamma}(\\alpha = 3, \\beta = 2)\\] (Use shape–rate parameterisation.)\n\nSimulate 10,000 observations.\nPlot histogram with theoretical density overlay.\nCompute empirical vs theoretical mean and variance.\nInvestigate how changing \\(\\alpha\\) affects:\n\nskewness\ntail behaviour\n\n\n\nSolution\n\nSimulate 10,000 observations.\n\n\nset.seed(123)\nn &lt;- 10000\nalpha &lt;- 3\nbeta &lt;- 2   # rate\nx &lt;- rgamma(n, shape = alpha, rate = beta)\n\n\nPlot histogram with theoretical density overlay.\n\n\n# hist(probability = TRUE) scales histogram to the total area = 1 (make it density)\nhist(x, breaks = 40, probability = TRUE,\n     xlab = \"x\")\n\n# using the same value of x that has been generated\ncurve(dgamma(x, shape = alpha, rate = beta),\n      from = 0, to = max(x),\n      add = TRUE, lwd = 2) # use add = TRUE to overlay\n\n\n\n\n\n\n\n\n\nCompute empirical vs theoretical mean and variance.\n\n\nemp_mean &lt;- mean(x)\nemp_var  &lt;- var(x)\n\ntheo_mean &lt;- alpha / beta\ntheo_var  &lt;- alpha / (beta^2)\n\nemp_mean; theo_mean\n\n[1] 1.482623\n\n\n[1] 1.5\n\nemp_var;  theo_var\n\n[1] 0.7325013\n\n\n[1] 0.75\n\n\nWith large \\(n\\), the empirical values should be very close.\n\nInvestigate how changing \\(\\alpha\\) affects:\n\nskewness\ntail behaviour\n\n\n\nset.seed(123)\nalphas &lt;- c(1, 2, 3, 5, 10)\ncols &lt;- c(\"blue\", \"red\", \"purple\", \"green\")\nbeta &lt;- 2\nn &lt;- 10000\n\n# Create empty list to store results\nsamples &lt;- vector(\"list\", length(alphas))\n\nfor (i in 1:length(alphas)) {\n  a &lt;- alphas[i]                 # current alpha\n  samples[[i]] &lt;- rgamma(n,      # simulate\n                         shape = a,\n                         rate = beta)\n}\n\n# Choose a common x-range so curves are comparable\nxmax &lt;- max(unlist(samples)) # find the largest observed value across all simulated datasets.\nxs &lt;- seq(0, xmax, length.out = 1000) # create a smooth grid for x-values to draw pdf\n\nplot(xs, dgamma(xs, shape = alphas[1], rate = beta), type = \"l\",\n     xlab = \"x\", ylab = \"Density\", lwd = 2,\n     main = \"Gamma densities for different alpha (rate fixed)\")\n\nfor (i in 2:length(alphas)) {\n  lines(xs, dgamma(xs, shape = alphas[i], rate = beta), col = cols[[i-1]],\n  lwd = 2)\n}\n\nlegend(\"topright\",\n       legend = paste(\"alpha =\", alphas),\n       col = c(\"black\", \"blue\", \"red\", \"purple\", \"green\"),\n       lwd=2, lty = 1, bty = \"n\")\n\n\n\n\n\n\n\n\nWith \\(\\beta\\) fixed, increasing \\(\\alpha\\) has the following effects:\n\nSkewness decreases (more “bell-shaped”).\nTail becomes less extreme relative to the center.\nThe distribution concentrates more around its mean (and looks more normal-like), so very large values become less common relative to the bulk.\n\n\n\n\n\nExercise 3: Beta Distribution\nLet \\[X \\sim \\text{Beta}(2,5).\\]\n\nSimulate 10,000 observations.\nPlot histogram and density.\nCompute mean and variance.\nRepeat for:\n\nBeta(0.5, 0.5)\nBeta(5, 5)\n\nHow do the shape parameters affect:\n\nsymmetry?\nconcentration?\nboundary behaviour?\n\n\n\nSolution\n\nSimulate 10,000 observations.\n\n\nset.seed(123)\nn &lt;- 10000\nx &lt;- rbeta(n, shape1=2, shape2=5)\n\n\nPlot histogram and density.\n\n\nhist(x, breaks=30, probability=TRUE, main=\"\")\n\n# theoretical density\ncurve(dbeta(x, shape1=2, shape2=5),\n      from=0, to=1, add=TRUE, lwd=2, col=\"red\")\n\n# empirical density\nlines(density(x), lwd=2, lty=2, col=\"blue\")\n\nlegend(\"topright\", col = c(\"red\", \"blue\"),\n       legend = c(\"Theoretical density\", \"Empirical density\"),\n       lty = c(1, 2), bty = \"n\")\n\n\n\n\n\n\n\n\n\nCompute mean and variance.\n\n\na &lt;- 2\nb &lt;- 5\n\nemp_mean &lt;- mean(x)\nemp_var  &lt;- var(x)\n\ntheo_mean &lt;- a / (a + b)\ntheo_var  &lt;- (a * b) / ((a + b)^2 * (a + b + 1))\n\nemp_mean; theo_mean\n\n[1] 0.2862241\n\n\n[1] 0.2857143\n\nemp_var;  theo_var\n\n[1] 0.02578022\n\n\n[1] 0.0255102\n\n\n\nRepeat for:\n\nBeta(0.5, 0.5)\nBeta(5, 5)\n\n\n\nrun_beta &lt;- function(a, b, n = 10000, seed = 123) {\n  set.seed(seed)\n  x &lt;- rbeta(n, a, b)\n\n  hist(x, breaks = 40, probability = TRUE,\n       main = paste0(\"Beta(\", a, \", \", b, \")\"),\n       xlab = \"x\", xlim = c(0, 1))\n  curve(dbeta(x, a, b), from = 0, to = 1, add = TRUE, lwd = 2)\n\n  emp_mean &lt;- mean(x)\n  emp_var  &lt;- var(x)\n\n  theo_mean &lt;- a / (a + b)\n  theo_var  &lt;- (a * b) / ((a + b)^2 * (a + b + 1))\n\n  cat(\"\\nBeta(\", a, \",\", b, \")\\n\", sep = \"\")\n  cat(\"Empirical mean:\", emp_mean, \" Theoretical mean:\", theo_mean, \"\\n\")\n  cat(\"Empirical var :\", emp_var,  \" Theoretical var :\", theo_var,  \"\\n\")\n}\n\nrun_beta(2, 5)\n\n\n\n\n\n\n\n\n\nBeta(2,5)\nEmpirical mean: 0.2862241  Theoretical mean: 0.2857143 \nEmpirical var : 0.02578022  Theoretical var : 0.0255102 \n\nrun_beta(0.5, 0.5)\n\n\n\n\n\n\n\n\n\nBeta(0.5,0.5)\nEmpirical mean: 0.4992115  Theoretical mean: 0.5 \nEmpirical var : 0.1251524  Theoretical var : 0.125 \n\nrun_beta(5, 5)\n\n\n\n\n\n\n\n\n\nBeta(5,5)\nEmpirical mean: 0.4999191  Theoretical mean: 0.5 \nEmpirical var : 0.0229485  Theoretical var : 0.02272727 \n\n\n\nHow do the shape parameters affect:\n\nsymmetry?\nconcentration?\nboundary behaviour?\n\n\nSymmetry\nThe Beta distribution is symmetric if and only if \\(\\alpha = \\beta.\\)\n\nIf \\(\\alpha = \\beta\\) → symmetric around \\(0.5\\).\nIf \\(\\alpha &lt; \\beta\\) → distribution is skewed toward 0.\nIf \\(\\alpha &gt; \\beta\\) → distribution is skewed toward 1.\n\nFrom 4):\n\n\\(\\text{Beta}(5,5)\\) → symmetric and bell-shaped.\n\\(\\text{Beta}(2,5)\\) → skewed toward 0.\n\\(\\text{Beta}(0.5,0.5)\\) → symmetric but U-shaped.\n\nConcentration (Spread Around the Mean)\nThe variance is:\n\\[\n\\mathrm{Var}(X) =\n\\frac{\\alpha \\beta}{(\\alpha + \\beta)^2 (\\alpha + \\beta + 1)}.\n\\]\nAs \\(\\alpha + \\beta\\) increases:\n\nThe variance decreases.\nThe distribution becomes more concentrated around the mean.\n\nThus:\n\nLarge \\(\\alpha + \\beta\\) → tightly concentrated.\nSmall \\(\\alpha + \\beta\\) → more spread out.\n\nBoundary Behaviour (Near 0 and 1)\nThe density near the boundaries depends on whether parameters are less than 1.\nNear 0:\n\nIf \\(\\alpha &lt; 1\\) → density \\(\\to \\infty\\) at 0.\nIf \\(\\alpha = 1\\) → finite nonzero value at 0.\nIf \\(\\alpha &gt; 1\\) → density \\(\\to 0\\) at 0.\n\nNear 1:\n\nIf \\(\\beta &lt; 1\\) → density \\(\\to \\infty\\) at 1.\nIf \\(\\beta = 1\\) → finite nonzero value at 1.\nIf \\(\\beta &gt; 1\\) → density \\(\\to 0\\) at 1.\n\nExamples:\n\n\\(\\text{Beta}(0.5,0.5)\\) → spikes at both 0 and 1 (U-shaped).\n\\(\\text{Beta}(5,5)\\) → zero at both boundaries, peak at 0.5.\n\\(\\text{Beta}(2,5)\\) → zero at boundaries, skewed toward 0.\n\n\n\n\n\nExercise 4: Multinomial Distribution\nLet\n\\[(X_1, X_2, X_3) \\sim \\text{Multinomial}(n=20, p=(0.2,0.5,0.3))\\]\n\nSimulate 5,000 independent multinomial experiments.\nCompute:\n\nsample means of each component\ncovariance matrix\n\nVerify: \\(E[X_i] = np_i\\)\nVerify that components are negatively correlated.\nVisualisation\n\nScatterplot of \\(X_1\\) vs \\(X_2\\)\nComment on dependence structure.\n\nConceptual Question: Why must multinomial components be dependent?\n\n\nSolution\n\nSimulate 5,000 independent multinomial experiments.\n\n\nset.seed(123)\nB &lt;- 5000\nn &lt;- 20\np &lt;- c(0.2, 0.5, 0.3)\nX &lt;- rmultinom(B, size = n, prob = p)\n# now each row of object X represents X_i\n# use t() to transpose\nX &lt;- t(rmultinom(B, size = n, prob = p))\n# now each \"column\" of object X represents X_i\n# use colnames() to change columns name of object X\ncolnames(X) &lt;- c(\"X1\", \"X2\", \"X3\")\n\n\nCompute:\n\nsample means of each component\ncovariance matrix\n\n\n\nsample_means &lt;- colMeans(X) # return mean of each column\nsample_means\n\n     X1      X2      X3 \n 3.9964 10.0404  5.9632 \n\n\nBy LLN, sample means (empirical averages) are close to:\n\\[\nE[X_i] = np_i\n\\]\n\n\\(20 \\times 0.2 = 4\\)\n\\(20 \\times 0.5 = 10\\)\n\\(20 \\times 0.3 = 6\\)\n\n\nsample_cov   &lt;- cov(X) # return covariance matrix\nsample_cov\n\n          X1        X2        X3\nX1  3.225832 -2.050065 -1.175768\nX2 -2.050065  4.936155 -2.886090\nX3 -1.175768 -2.886090  4.061858\n\n\nThis returns:\n\\[\n\\begin{pmatrix}\n\\mathrm{Var}(X_1) & \\mathrm{Cov}(X_1,X_2) & \\mathrm{Cov}(X_1,X_3) \\\\\n\\mathrm{Cov}(X_2,X_1) & \\mathrm{Var}(X_2) & \\mathrm{Cov}(X_2,X_3) \\\\\n\\mathrm{Cov}(X_3,X_1) & \\mathrm{Cov}(X_3,X_2) & \\mathrm{Var}(X_3)\n\\end{pmatrix}\n\\]\n\nIn theory, \\(Var(X_i)=np_i(1-p_i) &gt; 0\\).\nFor covariance, \\(Cov(X_i, X_j) = -np_ip_j \\quad(i\\neq j)\\).\nCovariances are negative because an increase in \\(X_i\\) must result in a decrease in \\(X_j, (i\\neq j)\\).\nSince the variables move in opposite directions, we have negative covariance.\n\n\nVerify: \\(E[X_i] = np_i\\)\n\n\ntheo_means &lt;- n * p\n# combine objects by columns\ncbind(empirical = sample_means, theoretical = theo_means)\n\n   empirical theoretical\nX1    3.9964           4\nX2   10.0404          10\nX3    5.9632           6\n\n\n\nVerify that components are negatively correlated.\n\n\nsample_cor &lt;- cor(X)\nsample_cor # returns correlation between the columns of X\n\n           X1         X2         X3\nX1  1.0000000 -0.5137505 -0.3248166\nX2 -0.5137505  1.0000000 -0.6445449\nX3 -0.3248166 -0.6445449  1.0000000\n\n\nNote that:\n\\[\n\\mathrm{Corr}(X_i, X_i)=\\frac{\\mathrm{Cov}(X_i, X_i)}\n{\\sqrt{\\mathrm{Var}(X_i)\\mathrm{Var}(X_i)}}\n\\]\n\nThe covariance matrix measures joint variability in original units, while the correlation matrix standardises this to measure pure strength of linear dependence on a -1 to 1 scale.\nIn a correlation matrix, the diagonal entries are always 1, because a variable is perfectly positively linearly related to itself.\n\n\nVisualisation\n\nScatterplot of \\(X_1\\) vs \\(X_2\\)\nComment on dependence structure.\n\n\n\nplot(X[, \"X1\"], X[, \"X2\"],\n     xlab = \"X1\", ylab = \"X2\",\n     main = \"Scatterplot of X1 vs X2 (Multinomial, n=20)\",\n     pch = 16)\n\n\n\n\n\n\n\n\nYou should see a downward trend. When \\(X_1\\) is large, \\(X_2\\) tends to be smaller (and vice versa).\nAlso, the points lie within feasible integer bounds, especially because:\n\\[\nX_1+X_2+X_3=20.\n\\]\nSo not every pair \\((X_1,X_2)\\) is possible; they’re constrained by the remaining count for \\(X_3\\).\n\nConceptual Question: Why must multinomial components be dependent?\n\nBecause the components must sum to a fixed total:\n\\[\nX_1+X_2+X_3=n.\n\\]\nSo if one component increases, at least one of the others must decrease to keep the sum equal to \\(n\\). This fixed-sum constraint forces dependence and typically creates negative correlations between different categories.",
    "crumbs": [
      "Statistical Distributions",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Workshop Solutions</span>"
    ]
  },
  {
    "objectID": "02_5_workshop_sol.html#part-2-joint-distributions-and-dependence",
    "href": "02_5_workshop_sol.html#part-2-joint-distributions-and-dependence",
    "title": "9  Workshop Solutions",
    "section": "9.2 Part 2: Joint distributions and dependence",
    "text": "9.2 Part 2: Joint distributions and dependence\n\nExercise 5\nExplain why dependence matters in simulation. Give 3 examples.\n\n\nExercise 6: Visualising Dependence\nSimulate two scenarios:\nCase A: Independent variables\nLet: \\[\nX \\sim \\mathcal{N}(0,1), \\quad Y \\sim \\mathcal{N}(0,1)\n\\]\nGenerate independently.\nCase B: Dependent variables\nDefine:\n\\[\nY = 0.8X + \\sqrt{1-0.8^2} Z\n\\]\nwhere \\(Z \\sim \\mathcal{N}(0,1)\\) independent of \\(X\\).\n\nGenerate 5,000 observations for each case.\nProduce scatterplots.\nCompute correlation.\nCompare visually and numerically.\n\n\n\nExercise 7: Constructing a Joint Distribution (Discrete)\nLet:\n\\[\nP(X=0,Y=0)=0.2, \\quad\nP(X=0,Y=1)=0.3,\n\\] \\[\nP(X=1,Y=0)=0.1, \\quad\nP(X=1,Y=1)=0.4\n\\]\n\nVerify probabilities sum to 1.\nCompute marginal distributions.\nCheck if independent.\nCompute covariance manually.\nSimulate 10,000 draws from this joint distribution.\nCompare empirical and theoretical covariance.\n\n\n\nExercise 8: Zero Correlation ≠ Independence\nLet:\n\\[\nX \\sim \\mathcal{N}(0,1)\n\\]\nDefine:\n\\[\nY = X^2\n\\]\n\nSimulate 10,000 observations.\nCompute correlation.\nPlot scatterplot.\nExplain why they are dependent despite near-zero correlation.\n\n\nExercise 9: Conditional Simulation\nLet:\n\\[\nX \\sim \\text{Gamma}(2,1)\n\\]\nGiven \\(X=x,\\)\n\\[\nY|X=x \\sim \\text{Poisson}(x)\n\\]\n\nSimulate 5,000 pairs.\nPlot scatterplot.\nEstimate:\n\n(E[Y])\nCompare with theoretical value using law of total expectation.\n\n\nHint:\n\\[\nE[Y] = E[E(Y|X)]\n\\]\n\n\n\nExercise 10: Simulating Bivariate Normal\nConstruct a portfolio simulation:\nLet:\n\n\\(X_1, X_2\\) be correlated returns (bivariate normal).\nPortfolio return:\n\n\\[\nR = 0.6X_1 + 0.4X_2\n\\]\nTasks:\n\nSimulate 10,000 returns.\nEstimate variance.\nPlot scatterplot and explain.\nExplain how ignoring dependence affects risk estimation.",
    "crumbs": [
      "Statistical Distributions",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Workshop Solutions</span>"
    ]
  }
]